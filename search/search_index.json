{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"apm/","text":"Introduction \u00b6 Splunk APM is a NoSample\u2122 Full-fidelity application performance monitoring and troubleshooting solution for cloud-native, microservices-based applications. By collecting all traces, instead of a sampled subset, no anomaly goes undetected. Whether a user experiences an error or longer-than-usual latency, you\u2019ll be able to know and act on it within seconds. Furthermore, not all bad behavior results in errors \u2014 as your developers create new applications they need to know whether their canary releases provide the expected results. Only by collecting all trace data will you ensure that your cloud-native applications behave the way they are supposed to. Infrastructure and application performance are interdependent. To see the full picture, Splunk APM provides seamless correlation between cloud infrastructure and the microservices running on top of it. If your application acts out because of memory leakage, a noisy neighbor container or any other infrastructure-related issue, Splunk will let you know. To complete the picture, in-context access to Splunk logs and events enable deeper troubleshooting and root-cause analysis.","title":"Introduction"},{"location":"apm/#introduction","text":"Splunk APM is a NoSample\u2122 Full-fidelity application performance monitoring and troubleshooting solution for cloud-native, microservices-based applications. By collecting all traces, instead of a sampled subset, no anomaly goes undetected. Whether a user experiences an error or longer-than-usual latency, you\u2019ll be able to know and act on it within seconds. Furthermore, not all bad behavior results in errors \u2014 as your developers create new applications they need to know whether their canary releases provide the expected results. Only by collecting all trace data will you ensure that your cloud-native applications behave the way they are supposed to. Infrastructure and application performance are interdependent. To see the full picture, Splunk APM provides seamless correlation between cloud infrastructure and the microservices running on top of it. If your application acts out because of memory leakage, a noisy neighbor container or any other infrastructure-related issue, Splunk will let you know. To complete the picture, in-context access to Splunk logs and events enable deeper troubleshooting and root-cause analysis.","title":"Introduction"},{"location":"apm/online-boutique/","text":"Deploying the Online Boutique in K3s - Lab Summary \u00b6 Deploy the Online Boutique application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic using Locust See APM metrics in the UI 1. Deploy Online Boutique \u00b6 To deploy the Online Boutique application into K3s apply the deployment: Using OpenTelemetry Collector cd ~/workshop kubectl apply -f apm/microservices-demo/k8s/deployment.yaml Output deployment.apps/checkoutservice created service/checkoutservice created deployment.apps/redis-cart created service/redis-cart created deployment.apps/productcatalogservice created service/productcatalogservice created deployment.apps/loadgenerator created service/loadgenerator created deployment.apps/frontend created service/frontend created service/frontend-external created deployment.apps/paymentservice created service/paymentservice created deployment.apps/emailservice created service/emailservice created deployment.apps/adservice created service/adservice created deployment.apps/cartservice created service/cartservice created deployment.apps/recommendationservice created service/recommendationservice created deployment.apps/shippingservice created service/shippingservice created deployment.apps/currencyservice created service/currencyservice created To ensure the Online Boutique application is running: Get Pods kubectl get pods Output NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-56585564cc-xclzj 1/1 Running 0 84s splunk-otel-collector-agent-hkshj 1/1 Running 0 84s svclb-frontend-external-c74n6 1/1 Running 0 53s currencyservice-747b74467f-xxrl9 1/1 Running 0 52s redis-cart-74594bd569-2jb6c 1/1 Running 0 54s adservice-6fb948b8c6-2xlrc 0/1 Running 0 53s recommendationservice-b5df8776c-sbt4h 1/1 Running 0 53s shippingservice-6d6f7b8d87-5lg9g 1/1 Running 0 53s svclb-loadgenerator-jxwct 1/1 Running 0 53s emailservice-9dd74d87c-wjdqr 1/1 Running 0 53s checkoutservice-8bcd56b46-bfj7d 1/1 Running 0 54s productcatalogservice-796cdcc5f5-vhspz 1/1 Running 0 53s paymentservice-6c875bf647-dklzb 1/1 Running 0 53s frontend-b8f747b87-4tkxn 1/1 Running 0 53s cartservice-59d5979db7-bqf64 1/1 Running 1 53s loadgenerator-57c8b84966-7nr4f 1/1 Running 3 53s Info Usually it should only take around 1min 30secs for the pods to transition into a Running state. 3. Validate in the UI \u00b6 From the top left hamburger menu, click Infrastructure \u2192 Kubernetes . Use the Cluster dropdown so select your cluster, you should see the new pods started and containers deployed. When you click on your cluster in the Splunk UI you should have a view that looks like below: If you select the WORKLOADS tab again you should now see that there are a number of Deployments and ReplicaSets: 4. View Online Boutique \u00b6 The Online Boutique is viewable on port 81 of the EC2 instance's IP address. The IP address is the one you used to SSH into the instance at the beginning of the workshop. Open your web browser and go to http:// EC2-IP :81/ where you will then be able to see the Online Boutique running. 5. Generate traffic \u00b6 The Online Boutique deployment contains a container running Locust that we can use to generate load traffic against the website to generate metrics, traces and spans. Locust is available on port 82 of the EC2 instance's IP address. Open a new tab in your web browser and go to http:// EC2-IP :82/ , you will then be able to see the Locust running. Set the Spawn rate to be 2 and click Start Swarming , this will start a gentle continous load on the application. Now go to Dashboards \u2192 APM Services \u2192 Service . For this we need to know the name of your application environment. In this workshop all the environments use: hostname -apm-env . To find the hostname, on the AWS/EC2 instance run the following command: Shell Command echo $(hostname)-apm-env Output Example bdzx-apm-env Select your environment you found in the previous step then select the frontend service and set time to Past 15 minutes. With this automatically generated dashboard you can keep an eye out for the health of your service(s) using RED (Rate, Error & Duration) metrics. It provides various performance related charts as well as correlated information on the underlying host and Kubernetes pods (if applicable). Take some time to explore the various charts in this dashboard 6. Verify Splunk APM metrics \u00b6 From the top left hamburger menu, click APM, this will bring you to the APM Overview dashboard: Select the Explore on the right hand side and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency/Service Map for the Online Boutique application. It should look similar to the screenshot below: The legend at the bottom of the page explains the different visualizations in the Dependency/Service Map. Service requests, error rate and root error rate. Request rate, latency and error rate Also in this view you can see the overall Error and Latency rates over time charts. 7. OpenTelemetry Dashboard \u00b6 Once the Open Telemetery Collector is deployed the platform will automatically provide a built in dashboard display OpenTelemetry Collector metrics. From the top left hamburger menu, select Dashboards \u2192 OpenTelemetry Collector and validate metrics and spans are being sent: 8. OpenTelemetry zpages \u00b6 To debug the traces being sent you can use the zpages extension. zpages are part of the OpenTelemetry collector and provide live data for troubleshooting and statistics. They are available on port 55679 of the EC2 instance's IP address. Open a new tab in your web browser and enter in http:// EC2-IP :55679/debug/tracez , you will then be able to see the zpages output. Alternatively, from your shell prompt you can run a text based browser: Shell Command lynx http://localhost:55679/debug/tracez","title":"Deploying the Online Boutique in K3s - Lab Summary"},{"location":"apm/online-boutique/#deploying-the-online-boutique-in-k3s-lab-summary","text":"Deploy the Online Boutique application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic using Locust See APM metrics in the UI","title":"Deploying the Online Boutique in K3s - Lab Summary"},{"location":"apm/online-boutique/#1-deploy-online-boutique","text":"To deploy the Online Boutique application into K3s apply the deployment: Using OpenTelemetry Collector cd ~/workshop kubectl apply -f apm/microservices-demo/k8s/deployment.yaml Output deployment.apps/checkoutservice created service/checkoutservice created deployment.apps/redis-cart created service/redis-cart created deployment.apps/productcatalogservice created service/productcatalogservice created deployment.apps/loadgenerator created service/loadgenerator created deployment.apps/frontend created service/frontend created service/frontend-external created deployment.apps/paymentservice created service/paymentservice created deployment.apps/emailservice created service/emailservice created deployment.apps/adservice created service/adservice created deployment.apps/cartservice created service/cartservice created deployment.apps/recommendationservice created service/recommendationservice created deployment.apps/shippingservice created service/shippingservice created deployment.apps/currencyservice created service/currencyservice created To ensure the Online Boutique application is running: Get Pods kubectl get pods Output NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-56585564cc-xclzj 1/1 Running 0 84s splunk-otel-collector-agent-hkshj 1/1 Running 0 84s svclb-frontend-external-c74n6 1/1 Running 0 53s currencyservice-747b74467f-xxrl9 1/1 Running 0 52s redis-cart-74594bd569-2jb6c 1/1 Running 0 54s adservice-6fb948b8c6-2xlrc 0/1 Running 0 53s recommendationservice-b5df8776c-sbt4h 1/1 Running 0 53s shippingservice-6d6f7b8d87-5lg9g 1/1 Running 0 53s svclb-loadgenerator-jxwct 1/1 Running 0 53s emailservice-9dd74d87c-wjdqr 1/1 Running 0 53s checkoutservice-8bcd56b46-bfj7d 1/1 Running 0 54s productcatalogservice-796cdcc5f5-vhspz 1/1 Running 0 53s paymentservice-6c875bf647-dklzb 1/1 Running 0 53s frontend-b8f747b87-4tkxn 1/1 Running 0 53s cartservice-59d5979db7-bqf64 1/1 Running 1 53s loadgenerator-57c8b84966-7nr4f 1/1 Running 3 53s Info Usually it should only take around 1min 30secs for the pods to transition into a Running state.","title":"1. Deploy Online Boutique"},{"location":"apm/online-boutique/#3-validate-in-the-ui","text":"From the top left hamburger menu, click Infrastructure \u2192 Kubernetes . Use the Cluster dropdown so select your cluster, you should see the new pods started and containers deployed. When you click on your cluster in the Splunk UI you should have a view that looks like below: If you select the WORKLOADS tab again you should now see that there are a number of Deployments and ReplicaSets:","title":"3. Validate in the UI"},{"location":"apm/online-boutique/#4-view-online-boutique","text":"The Online Boutique is viewable on port 81 of the EC2 instance's IP address. The IP address is the one you used to SSH into the instance at the beginning of the workshop. Open your web browser and go to http:// EC2-IP :81/ where you will then be able to see the Online Boutique running.","title":"4. View Online Boutique"},{"location":"apm/online-boutique/#5-generate-traffic","text":"The Online Boutique deployment contains a container running Locust that we can use to generate load traffic against the website to generate metrics, traces and spans. Locust is available on port 82 of the EC2 instance's IP address. Open a new tab in your web browser and go to http:// EC2-IP :82/ , you will then be able to see the Locust running. Set the Spawn rate to be 2 and click Start Swarming , this will start a gentle continous load on the application. Now go to Dashboards \u2192 APM Services \u2192 Service . For this we need to know the name of your application environment. In this workshop all the environments use: hostname -apm-env . To find the hostname, on the AWS/EC2 instance run the following command: Shell Command echo $(hostname)-apm-env Output Example bdzx-apm-env Select your environment you found in the previous step then select the frontend service and set time to Past 15 minutes. With this automatically generated dashboard you can keep an eye out for the health of your service(s) using RED (Rate, Error & Duration) metrics. It provides various performance related charts as well as correlated information on the underlying host and Kubernetes pods (if applicable). Take some time to explore the various charts in this dashboard","title":"5. Generate traffic"},{"location":"apm/online-boutique/#6-verify-splunk-apm-metrics","text":"From the top left hamburger menu, click APM, this will bring you to the APM Overview dashboard: Select the Explore on the right hand side and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency/Service Map for the Online Boutique application. It should look similar to the screenshot below: The legend at the bottom of the page explains the different visualizations in the Dependency/Service Map. Service requests, error rate and root error rate. Request rate, latency and error rate Also in this view you can see the overall Error and Latency rates over time charts.","title":"6. Verify Splunk APM metrics"},{"location":"apm/online-boutique/#7-opentelemetry-dashboard","text":"Once the Open Telemetery Collector is deployed the platform will automatically provide a built in dashboard display OpenTelemetry Collector metrics. From the top left hamburger menu, select Dashboards \u2192 OpenTelemetry Collector and validate metrics and spans are being sent:","title":"7. OpenTelemetry Dashboard"},{"location":"apm/online-boutique/#8-opentelemetry-zpages","text":"To debug the traces being sent you can use the zpages extension. zpages are part of the OpenTelemetry collector and provide live data for troubleshooting and statistics. They are available on port 55679 of the EC2 instance's IP address. Open a new tab in your web browser and enter in http:// EC2-IP :55679/debug/tracez , you will then be able to see the zpages output. Alternatively, from your shell prompt you can run a text based browser: Shell Command lynx http://localhost:55679/debug/tracez","title":"8. OpenTelemetry zpages"},{"location":"apm/otel/","text":"Open Telemetry Collector \u00b6 The collector is an optional component between the smart agent and SaaS ingest. In this configuration we are using the sapm endpoint to receive traces. 1. Install OpenTelemetry Collector with helm \u00b6 Add the repository with Shell Command helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts Then Shell Command helm install \\ --set standaloneCollector.configOverride.exporters.signalfx.realm=$REALM \\ --set standaloneCollector.configOverride.exporters.signalfx.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.endpoint=https://ingest.$REALM.signalfx.com/v2/trace \\ opentelemetry-collector open-telemetry/opentelemetry-collector \\ -f ~/workshop/otel/collector.yaml 2. Validate the OpenTelemetry Collector installation \u00b6 Review the OpenTelemetry Collector logs: Shell Command kubectl logs -l app.kubernetes.io/name = opentelemetry-collector Look for a log entry with Example Output ... \"msg\":\"Everything is ready. Begin running and processing data.\"} Validate that the service is running and has a sapm endpoint on port 7276. Shell Command kubectl get svc opentelemetry-collector Example Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE opentelemetry-collector ClusterIP 10.43.119.140 <none> 14250/TCP,14268/TCP,55680/TCP, 7276 /TCP,9411/TCP 13m Use the healthcheck endpoint to confirm: Shell Command OTEL_ENDPOINT=$(kubectl get svc opentelemetry-collector -n default -o jsonpath='{.spec.clusterIP}') curl http://$OTEL_ENDPOINT:13133/; echo Example Output {\"status\":\"Server available\",\"upSince\":\"2020-10-22T08:07:33.656859114Z\",\"uptime\":\"8m33.548333561s\"} 3. Reconfigure the agent to use OpenTelemetry Collector \u00b6 We want to send traces in sapm format and point it to the OpenTelemetry Collector trace endpoint. Uninstall the agent: Shell Command helm uninstall signalfx-agent Then reinstall it with traceEndpointUrl set to point to OpenTelemetry Collector and using sapm as trace format: Shell Command helm install \\ --set writer.traceExportFormat=sapm \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$(hostname)-k3s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=http://opentelemetry-collector:7276/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml Check the OpenTelemetry Collector dashboards and validate metrics and spans are being sent.","title":"Open Telemetry Collector"},{"location":"apm/otel/#open-telemetry-collector","text":"The collector is an optional component between the smart agent and SaaS ingest. In this configuration we are using the sapm endpoint to receive traces.","title":"Open Telemetry Collector"},{"location":"apm/otel/#1-install-opentelemetry-collector-with-helm","text":"Add the repository with Shell Command helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts Then Shell Command helm install \\ --set standaloneCollector.configOverride.exporters.signalfx.realm=$REALM \\ --set standaloneCollector.configOverride.exporters.signalfx.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.endpoint=https://ingest.$REALM.signalfx.com/v2/trace \\ opentelemetry-collector open-telemetry/opentelemetry-collector \\ -f ~/workshop/otel/collector.yaml","title":"1. Install OpenTelemetry Collector with helm"},{"location":"apm/otel/#2-validate-the-opentelemetry-collector-installation","text":"Review the OpenTelemetry Collector logs: Shell Command kubectl logs -l app.kubernetes.io/name = opentelemetry-collector Look for a log entry with Example Output ... \"msg\":\"Everything is ready. Begin running and processing data.\"} Validate that the service is running and has a sapm endpoint on port 7276. Shell Command kubectl get svc opentelemetry-collector Example Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE opentelemetry-collector ClusterIP 10.43.119.140 <none> 14250/TCP,14268/TCP,55680/TCP, 7276 /TCP,9411/TCP 13m Use the healthcheck endpoint to confirm: Shell Command OTEL_ENDPOINT=$(kubectl get svc opentelemetry-collector -n default -o jsonpath='{.spec.clusterIP}') curl http://$OTEL_ENDPOINT:13133/; echo Example Output {\"status\":\"Server available\",\"upSince\":\"2020-10-22T08:07:33.656859114Z\",\"uptime\":\"8m33.548333561s\"}","title":"2. Validate the OpenTelemetry Collector installation"},{"location":"apm/otel/#3-reconfigure-the-agent-to-use-opentelemetry-collector","text":"We want to send traces in sapm format and point it to the OpenTelemetry Collector trace endpoint. Uninstall the agent: Shell Command helm uninstall signalfx-agent Then reinstall it with traceEndpointUrl set to point to OpenTelemetry Collector and using sapm as trace format: Shell Command helm install \\ --set writer.traceExportFormat=sapm \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$(hostname)-k3s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=http://opentelemetry-collector:7276/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml Check the OpenTelemetry Collector dashboards and validate metrics and spans are being sent.","title":"3. Reconfigure the agent to use OpenTelemetry Collector"},{"location":"apm/using-apm/","text":"Using Splunk APM \u00b6 APM Overview - RED metrics Using the Service Map Introduction to Tag Spotlight Example Traces Contextual Links to Infra 1. Traces and Spans explained \u00b6 A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services. Each span has a name, representing the operation captured by this span, and a service name, representing within which service the operation took place. Additionally, spans may reference another span as their parent, defining the relationships between the operations captured in the trace that were performed to process that transaction. Each span contains a lot of information about the method, operation, or block of code that it captures, including: the operation name the start time of the operation with microsecond precision how long the operation took to execute, also with microsecond precision the logical name of the service on which the operation took place the IP address of the service instance on which the operation took place 2. Service Map \u00b6 Click on paymentservice in the service map and select version from the breakdown drop down filter underneath paymentservice . This will filter our service map by the custom span tag version . You will now see the service map has been updated like the below screenshot to show the different versions of the paymentservice . 3. Tag Spotlight \u00b6 On the right hand side of the screen scroll down on Tag Spotlight and select Top Across All Indexed Tags from the dropdown. Once this has been selected click the arrows as indicated in the screenshot below. The Tag Spotlight Page will be displayed. From this page you can view the top tags in your application and their corresponding error rates and request rates. Note that for the version span tag it appears that version 350.10 has a 100% error rate and for our tenant.level span tag it shows that all three tenants (Gold, Silver & Bronze) have errors present. The Tag Spotlight page is interactive and allows you to add a tag as a filter by simply clicking on your desired tag. Click on gold under tenant.level to add it as a filter. Once this is done the page will now only display data with gold as it\u2019s tenant.level . Tag Spotlight is very useful for analysing your data and spotting trends. We can see that for the Gold Tenant that out of the total number of requests, 55 of them are in error. If we correlate this to the version tag, we can see that version 350.10 served 55 requests and version 350.9 served 17 requests. This means that all of the requests that went through version 350.10 ended up in an error state. In order to test this theory further that all of the requests from paymentservice version 350.10 result in an error, we can change our filter to another tenant by using the tag selector. Change your filter from gold tenant to silver tenant. Now we can perform a similar analysis by looking at the number of requests in error for the silver tenant and correlating that with the version number. Note the amount of errors for the silver tenant match the amount of requests for version 350.10 . Tag Spotlight not only allows you to look at request and error rates but also at the latency per service. In order to do this just select the latency button and remove your Silver Tenant Tag so that you can see the latency for all of the Payment Service. Go back to your service map by pressing the X button on the far right underneath Clear All . Click anywhere on the pink line in the \u2018Service Requests & Errors\u2019 graph. Once selected you should see a list of example traces. Click on one of the example traces. 4. Example Trace \u00b6 You should now see the entire trace along with the spans for the example trace that was selected. Spans which have errors are indicated by a red exclamation mark beside it. Click one of these to expand the span and see the associated metadata and some error details. Note that we are able to see that this error is caused by a 401 error and other useful information such as \u2018tenant\u2019 and \u2018version\u2019 is also displayed.","title":"Using Splunk APM"},{"location":"apm/using-apm/#using-splunk-apm","text":"APM Overview - RED metrics Using the Service Map Introduction to Tag Spotlight Example Traces Contextual Links to Infra","title":"Using Splunk APM"},{"location":"apm/using-apm/#1-traces-and-spans-explained","text":"A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services. Each span has a name, representing the operation captured by this span, and a service name, representing within which service the operation took place. Additionally, spans may reference another span as their parent, defining the relationships between the operations captured in the trace that were performed to process that transaction. Each span contains a lot of information about the method, operation, or block of code that it captures, including: the operation name the start time of the operation with microsecond precision how long the operation took to execute, also with microsecond precision the logical name of the service on which the operation took place the IP address of the service instance on which the operation took place","title":"1. Traces and Spans explained"},{"location":"apm/using-apm/#2-service-map","text":"Click on paymentservice in the service map and select version from the breakdown drop down filter underneath paymentservice . This will filter our service map by the custom span tag version . You will now see the service map has been updated like the below screenshot to show the different versions of the paymentservice .","title":"2. Service Map"},{"location":"apm/using-apm/#3-tag-spotlight","text":"On the right hand side of the screen scroll down on Tag Spotlight and select Top Across All Indexed Tags from the dropdown. Once this has been selected click the arrows as indicated in the screenshot below. The Tag Spotlight Page will be displayed. From this page you can view the top tags in your application and their corresponding error rates and request rates. Note that for the version span tag it appears that version 350.10 has a 100% error rate and for our tenant.level span tag it shows that all three tenants (Gold, Silver & Bronze) have errors present. The Tag Spotlight page is interactive and allows you to add a tag as a filter by simply clicking on your desired tag. Click on gold under tenant.level to add it as a filter. Once this is done the page will now only display data with gold as it\u2019s tenant.level . Tag Spotlight is very useful for analysing your data and spotting trends. We can see that for the Gold Tenant that out of the total number of requests, 55 of them are in error. If we correlate this to the version tag, we can see that version 350.10 served 55 requests and version 350.9 served 17 requests. This means that all of the requests that went through version 350.10 ended up in an error state. In order to test this theory further that all of the requests from paymentservice version 350.10 result in an error, we can change our filter to another tenant by using the tag selector. Change your filter from gold tenant to silver tenant. Now we can perform a similar analysis by looking at the number of requests in error for the silver tenant and correlating that with the version number. Note the amount of errors for the silver tenant match the amount of requests for version 350.10 . Tag Spotlight not only allows you to look at request and error rates but also at the latency per service. In order to do this just select the latency button and remove your Silver Tenant Tag so that you can see the latency for all of the Payment Service. Go back to your service map by pressing the X button on the far right underneath Clear All . Click anywhere on the pink line in the \u2018Service Requests & Errors\u2019 graph. Once selected you should see a list of example traces. Click on one of the example traces.","title":"3. Tag Spotlight"},{"location":"apm/using-apm/#4-example-trace","text":"You should now see the entire trace along with the spans for the example trace that was selected. Spans which have errors are indicated by a red exclamation mark beside it. Click one of these to expand the span and see the associated metadata and some error details. Note that we are able to see that this error is caused by a 401 error and other useful information such as \u2018tenant\u2019 and \u2018version\u2019 is also displayed.","title":"4. Example Trace"},{"location":"dashboards/","text":"Working with Dashboards, Charts and Metrics \u00b6 Introduction to the Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Saving charts in a dashboard Introduction to SignalFlow 1. Dashboards \u00b6 Dashboards are groupings of charts and visualizations of metrics. Well-designed dashboards can provide useful and actionable insight into your system at a glance. Dashboards can be complex or contain just a few charts that drill down only into the data you want to see. During this module we are going to create the following charts and dashboard and connect it to your Team page. 2. Your Teams' Page \u00b6 From the top left hamburger menu select Dashboards from the side menu. As you have already been assigned to a team, you will land on the team dashboard. We use the team Observability as an example here. The one in your workshop will be different! This page shows the total number of team members, how many active alerts for your team and all dashboards that are assigned to your team. Right now they are no dashboards assigned but as stated before, we will add the new dashboard that you will create to your Teams page later. 3. Sample Charts \u00b6 To continue, click on All Dashboards on the top right corner of the screen. This brings you to the view that shows all the available dashboards, included the pre-built ones. If you are already receiving metrics from a Cloud API integration or another service through the Splunk Agent you will see relevant dashboards for these services. 4. Inspecting the Sample Data \u00b6 Among the dashboards you will see a Dashboard group called Sample Data . Expand the Sample Data dashboard group by clicking on it, and then click on the Sample Charts dashboard. In the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards. Have a look through all the dashboards in this dashboard group ( PART 1 , PART 2 , PART 3 and INTRO TO SPLUNK OBSERVABILITY CLOUD )","title":"Introduction"},{"location":"dashboards/#working-with-dashboards-charts-and-metrics","text":"Introduction to the Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Saving charts in a dashboard Introduction to SignalFlow","title":"Working with Dashboards, Charts and Metrics"},{"location":"dashboards/#1-dashboards","text":"Dashboards are groupings of charts and visualizations of metrics. Well-designed dashboards can provide useful and actionable insight into your system at a glance. Dashboards can be complex or contain just a few charts that drill down only into the data you want to see. During this module we are going to create the following charts and dashboard and connect it to your Team page.","title":"1. Dashboards"},{"location":"dashboards/#2-your-teams-page","text":"From the top left hamburger menu select Dashboards from the side menu. As you have already been assigned to a team, you will land on the team dashboard. We use the team Observability as an example here. The one in your workshop will be different! This page shows the total number of team members, how many active alerts for your team and all dashboards that are assigned to your team. Right now they are no dashboards assigned but as stated before, we will add the new dashboard that you will create to your Teams page later.","title":"2. Your Teams' Page"},{"location":"dashboards/#3-sample-charts","text":"To continue, click on All Dashboards on the top right corner of the screen. This brings you to the view that shows all the available dashboards, included the pre-built ones. If you are already receiving metrics from a Cloud API integration or another service through the Splunk Agent you will see relevant dashboards for these services.","title":"3. Sample Charts"},{"location":"dashboards/#4-inspecting-the-sample-data","text":"Among the dashboards you will see a Dashboard group called Sample Data . Expand the Sample Data dashboard group by clicking on it, and then click on the Sample Charts dashboard. In the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards. Have a look through all the dashboards in this dashboard group ( PART 1 , PART 2 , PART 3 and INTRO TO SPLUNK OBSERVABILITY CLOUD )","title":"4. Inspecting the Sample Data"},{"location":"dashboards/adding-charts/","text":"Adding charts to dashboards \u00b6 Let's now save our chart. 1. Save to existing dashboard \u00b6 Check that you have YOUR_NAME-Dashboard: YOUR_NAME-Dashboard in the top right corner. This means you chart will be saved in this Dashboard. Name the Chart Latency History and add a Chart Description if you wish. Click on Save And Close . This returns you to your dashboard that now has two charts! Now let's quickly add another Chart based on the previous one. 2. Copy and Paste a chart \u00b6 Click on the three dots ... on the Latency History chart in your dashboard and then on Copy . You see the chart being copied, and you should now have a red circle with a white 1 next to the + on the top left of the page. Click on the at the top of the page, and then in the menu on Paste Charts (There should also be a red dot with a 1 visible at the end of the line). This will place a copy of the previous chart in your dashboard. 3. Edit the pasted chart \u00b6 Click on the three dots ... on on of the Latency History charts in your dashboard and then on Open (or you can click on the name of the chart which here is Latency History ). This will bring you to the editor environment again. First set the time for the chart to -1 hour in the Time box at the top right of the chart. Then to make this a different chart, click on the eye icon in front of signal \" A \" to make it visible again, and then hide signal \" C \" via the eye icon and change the name for Latency history to Latency vs Load . Click on the Add Metric Or Event button. This will bring up the box for a new signal. Type and select demo.trans.count for Signal D . This will add a new Signal D to your chart, It shows the number of active request active. Add the filter for the demo_datacenter:Paris then change the Rollup type by clicking on Delta Rollup (or clicking on the cog icon). When the visualisation pane opens, change the Rollup dropdown to Rollup:Rate/sec , then click into the name field in the top left hand corner and it to Latency vs load and press the Save And Close button. This returns you to your dashboard that now has three different charts! Let's add an \"instruction\" note and arrange the charts!","title":"Adding more charts"},{"location":"dashboards/adding-charts/#adding-charts-to-dashboards","text":"Let's now save our chart.","title":"Adding charts to dashboards"},{"location":"dashboards/adding-charts/#1-save-to-existing-dashboard","text":"Check that you have YOUR_NAME-Dashboard: YOUR_NAME-Dashboard in the top right corner. This means you chart will be saved in this Dashboard. Name the Chart Latency History and add a Chart Description if you wish. Click on Save And Close . This returns you to your dashboard that now has two charts! Now let's quickly add another Chart based on the previous one.","title":"1. Save to existing dashboard"},{"location":"dashboards/adding-charts/#2-copy-and-paste-a-chart","text":"Click on the three dots ... on the Latency History chart in your dashboard and then on Copy . You see the chart being copied, and you should now have a red circle with a white 1 next to the + on the top left of the page. Click on the at the top of the page, and then in the menu on Paste Charts (There should also be a red dot with a 1 visible at the end of the line). This will place a copy of the previous chart in your dashboard.","title":"2. Copy and Paste a chart"},{"location":"dashboards/adding-charts/#3-edit-the-pasted-chart","text":"Click on the three dots ... on on of the Latency History charts in your dashboard and then on Open (or you can click on the name of the chart which here is Latency History ). This will bring you to the editor environment again. First set the time for the chart to -1 hour in the Time box at the top right of the chart. Then to make this a different chart, click on the eye icon in front of signal \" A \" to make it visible again, and then hide signal \" C \" via the eye icon and change the name for Latency history to Latency vs Load . Click on the Add Metric Or Event button. This will bring up the box for a new signal. Type and select demo.trans.count for Signal D . This will add a new Signal D to your chart, It shows the number of active request active. Add the filter for the demo_datacenter:Paris then change the Rollup type by clicking on Delta Rollup (or clicking on the cog icon). When the visualisation pane opens, change the Rollup dropdown to Rollup:Rate/sec , then click into the name field in the top left hand corner and it to Latency vs load and press the Save And Close button. This returns you to your dashboard that now has three different charts! Let's add an \"instruction\" note and arrange the charts!","title":"3. Edit the pasted chart"},{"location":"dashboards/dashboarding/","text":"Adding Notes and Dashboard Layout \u00b6 1. Adding Notes \u00b6 Often on dashboards it makes sense to place a short \"instruction\" pane that helps users of a dashboard. Lets add one now by clicking on the New Text Note Button. This will open the notes editor. To allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages. This includes (but not limited to): Headers. (in various sizes) Emphasis styles. Lists and Tables. Links. These can be external webpages (for documentation for example) or directly to other Splunk IMT Dashboards Below is an example of above Markdown options you can use in your note. Sample Markdown text # h1 Big headings ###### h6 To small headings ##### Emphasis **This is bold text**, *This is italic text* , ~~Strikethrough~~ ##### Lists Unordered + Create a list by starting a line with `+`, `-`, or `*` - Sub-lists are made by indenting 2 spaces: - Marker character change forces new list start: * Ac tristique libero volutpat at + Facilisis in pretium nisl aliquet * Very easy! Ordered 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa ##### Tables | Option | Description | | ------ | ----------- | | chart | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | #### Links [link to webpage](https://www.splunk.com) [link to dashboard with title](https://app.eu0.signalfx.com/#/dashboard/EaJHrbPAEAA?groupId=EaJHgrsAIAA&configId=EaJHsHzAEAA \"Link to the Sample chart Dashboard!\") Copy the above by using the copy button and paste it in the Edit box. the preview will show you how it will look. 2. Saving our chart \u00b6 Give the Note chart a name, in our example we used Example text chart , then press the Save And Close Button. This will bring you back to you Dashboard, that now includes the note 3. Ordering & sizing of charts \u00b6 If you do not like the default order and sizes of your charts you can simply use window dragging technique to move and size them to the desired location Grab the top border of a chart and you should see the mouse pointer change to a drag icon. You can now simply drag you chart to the desired location. Now drag the Latency History Chart to site below the Latency vs Load Chart. You can also resize a chart, simply by dragging the side or the bottom. As a last exercise reduce the width of the note chart to about a third of the other charts. The chart will automatically snap to one of the sizes it supports. Widen the 3 other charts to about a third of the Dashboard. Drag the notes to the left of the others and resize it to match it to the 23 others. Set the time to -1 h hour and you should have the following dashboard! On to Detectors!","title":"Notes and layout"},{"location":"dashboards/dashboarding/#adding-notes-and-dashboard-layout","text":"","title":"Adding Notes and Dashboard Layout"},{"location":"dashboards/dashboarding/#1-adding-notes","text":"Often on dashboards it makes sense to place a short \"instruction\" pane that helps users of a dashboard. Lets add one now by clicking on the New Text Note Button. This will open the notes editor. To allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages. This includes (but not limited to): Headers. (in various sizes) Emphasis styles. Lists and Tables. Links. These can be external webpages (for documentation for example) or directly to other Splunk IMT Dashboards Below is an example of above Markdown options you can use in your note. Sample Markdown text # h1 Big headings ###### h6 To small headings ##### Emphasis **This is bold text**, *This is italic text* , ~~Strikethrough~~ ##### Lists Unordered + Create a list by starting a line with `+`, `-`, or `*` - Sub-lists are made by indenting 2 spaces: - Marker character change forces new list start: * Ac tristique libero volutpat at + Facilisis in pretium nisl aliquet * Very easy! Ordered 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa ##### Tables | Option | Description | | ------ | ----------- | | chart | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | #### Links [link to webpage](https://www.splunk.com) [link to dashboard with title](https://app.eu0.signalfx.com/#/dashboard/EaJHrbPAEAA?groupId=EaJHgrsAIAA&configId=EaJHsHzAEAA \"Link to the Sample chart Dashboard!\") Copy the above by using the copy button and paste it in the Edit box. the preview will show you how it will look.","title":"1. Adding Notes"},{"location":"dashboards/dashboarding/#2-saving-our-chart","text":"Give the Note chart a name, in our example we used Example text chart , then press the Save And Close Button. This will bring you back to you Dashboard, that now includes the note","title":"2. Saving our chart"},{"location":"dashboards/dashboarding/#3-ordering-sizing-of-charts","text":"If you do not like the default order and sizes of your charts you can simply use window dragging technique to move and size them to the desired location Grab the top border of a chart and you should see the mouse pointer change to a drag icon. You can now simply drag you chart to the desired location. Now drag the Latency History Chart to site below the Latency vs Load Chart. You can also resize a chart, simply by dragging the side or the bottom. As a last exercise reduce the width of the note chart to about a third of the other charts. The chart will automatically snap to one of the sizes it supports. Widen the 3 other charts to about a third of the Dashboard. Drag the notes to the left of the others and resize it to match it to the 23 others. Set the time to -1 h hour and you should have the following dashboard! On to Detectors!","title":"3. Ordering &amp; sizing of charts"},{"location":"dashboards/editing/","text":"Editing charts \u00b6 1. Editing a chart \u00b6 Click on the three dots ... on the Latency histogram chart in the Sample Data dashboard and then on Open (or you can click on the name of the chart which here is Latency histogram ). You will see the plot options, current plot and signal (metric) for the Latency histogram chart in the chart editor UI. In the Plot Editor tab under Signal you see the metric demo.trans.latency we are currently plotting. You will see a number of Line plots. The number 18 ts indicates that we are plotting 18 metric time series in the chart. Click on the different chart type icons to explore each of the visualizations. Notice their name while you swipe over them. For example, click on the Heat Map icon: See how the chart changes to a heat map. Note You can use different charts to visualize your metrics - you choose which chart type fits best for the visualization you want to have. For more info on the different chart types see Choosing a chart type . Click on the Line chart type and you will see the line plot. 2. Changing the time window \u00b6 You can also increase the time window of the chart by changing the time to Past 15 minutes by selecting from the Time dropdown. 3. Viewing the Data Table \u00b6 Click on the Data Table tab. You now see 18 rows, each representing a metric time series with a number of columns. These columns represent the dimensions of the metric. The dimensions for demo.trans.latency are: demo_datacenter demo_customer demo_host In the demo_datacenter column you see that there are two data centers, Paris and Tokyo , for which we are getting metrics. If you move your cursor over the lines in the chart horizontally you will see the data table update accordingly. If you click on one of the lines in the chart you will see a pinned value appear in the data table. Now click on Plot editor again to close the Data Table and let's save this chart into a dashboard for later use!","title":"Editing charts"},{"location":"dashboards/editing/#editing-charts","text":"","title":"Editing charts"},{"location":"dashboards/editing/#1-editing-a-chart","text":"Click on the three dots ... on the Latency histogram chart in the Sample Data dashboard and then on Open (or you can click on the name of the chart which here is Latency histogram ). You will see the plot options, current plot and signal (metric) for the Latency histogram chart in the chart editor UI. In the Plot Editor tab under Signal you see the metric demo.trans.latency we are currently plotting. You will see a number of Line plots. The number 18 ts indicates that we are plotting 18 metric time series in the chart. Click on the different chart type icons to explore each of the visualizations. Notice their name while you swipe over them. For example, click on the Heat Map icon: See how the chart changes to a heat map. Note You can use different charts to visualize your metrics - you choose which chart type fits best for the visualization you want to have. For more info on the different chart types see Choosing a chart type . Click on the Line chart type and you will see the line plot.","title":"1. Editing a chart"},{"location":"dashboards/editing/#2-changing-the-time-window","text":"You can also increase the time window of the chart by changing the time to Past 15 minutes by selecting from the Time dropdown.","title":"2. Changing the time window"},{"location":"dashboards/editing/#3-viewing-the-data-table","text":"Click on the Data Table tab. You now see 18 rows, each representing a metric time series with a number of columns. These columns represent the dimensions of the metric. The dimensions for demo.trans.latency are: demo_datacenter demo_customer demo_host In the demo_datacenter column you see that there are two data centers, Paris and Tokyo , for which we are getting metrics. If you move your cursor over the lines in the chart horizontally you will see the data table update accordingly. If you click on one of the lines in the chart you will see a pinned value appear in the data table. Now click on Plot editor again to close the Data Table and let's save this chart into a dashboard for later use!","title":"3. Viewing the Data Table"},{"location":"dashboards/filtering/","text":"Using Filters & Formula's \u00b6 1 Creating a new chart \u00b6 Let's now create a new chart and save it in our dashboard! Select the plus icon (top right of the UI) and from the drop down, choose the option Chart . Or click on the + New Chart Button to create a new chart. You will now see a chart template like the following. Let's enter a metric to plot. We are still going to use the metric demo.trans.latency . In the Plot Editor tab under Signal enter demo.trans.latency . You should now have a familiar line chart. Please switch the time to 15 mins. 2. Filtering and Analytics \u00b6 Let's now select the Paris datacenter to do some analytics - for that we will use a filter. Let's go back to the Plot Editor tab and click on Add Filter , wait until it automatically populates, choose demo_datacenter , and then Paris . In the F(x) column, add the analytic function Percentile:Aggregation , and leave the value to 95 (click outside to confirm). For info on the Percentile function and the other functions see Analytics reference . 3. Using Timeshift analytical function \u00b6 Let's now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A . You will see a new row identical to A , called B , both visible and plotted. For Signal B , in the F(x) column add the analytic function Timeshift and enter 1w (or 7d for 7 days), and click outside to confirm. Click on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B . Click on Close . We now see plots for Signal A (the past 15 minutes) as a blue plot, and the plots from a week ago in pink. In order to make this clearer we can click on the Area chart icon to change the visualization. We now can see when last weeks latency was higher! Next, click into the field next to Time on the Override bar and choose Past Hour from the dropdown. 4. Using Formulas \u00b6 Let's now plot the difference of all metric values for a day with 7 days in between. Click on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C . We now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time. Lets look at the Signalflow that drives our Charts and Detectors!","title":"Using filters & formulas"},{"location":"dashboards/filtering/#using-filters-formulas","text":"","title":"Using Filters &amp; Formula's"},{"location":"dashboards/filtering/#1-creating-a-new-chart","text":"Let's now create a new chart and save it in our dashboard! Select the plus icon (top right of the UI) and from the drop down, choose the option Chart . Or click on the + New Chart Button to create a new chart. You will now see a chart template like the following. Let's enter a metric to plot. We are still going to use the metric demo.trans.latency . In the Plot Editor tab under Signal enter demo.trans.latency . You should now have a familiar line chart. Please switch the time to 15 mins.","title":"1 Creating a new chart"},{"location":"dashboards/filtering/#2-filtering-and-analytics","text":"Let's now select the Paris datacenter to do some analytics - for that we will use a filter. Let's go back to the Plot Editor tab and click on Add Filter , wait until it automatically populates, choose demo_datacenter , and then Paris . In the F(x) column, add the analytic function Percentile:Aggregation , and leave the value to 95 (click outside to confirm). For info on the Percentile function and the other functions see Analytics reference .","title":"2. Filtering and Analytics"},{"location":"dashboards/filtering/#3-using-timeshift-analytical-function","text":"Let's now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A . You will see a new row identical to A , called B , both visible and plotted. For Signal B , in the F(x) column add the analytic function Timeshift and enter 1w (or 7d for 7 days), and click outside to confirm. Click on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B . Click on Close . We now see plots for Signal A (the past 15 minutes) as a blue plot, and the plots from a week ago in pink. In order to make this clearer we can click on the Area chart icon to change the visualization. We now can see when last weeks latency was higher! Next, click into the field next to Time on the Override bar and choose Past Hour from the dropdown.","title":"3. Using Timeshift analytical function"},{"location":"dashboards/filtering/#4-using-formulas","text":"Let's now plot the difference of all metric values for a day with 7 days in between. Click on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C . We now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time. Lets look at the Signalflow that drives our Charts and Detectors!","title":"4. Using Formulas"},{"location":"dashboards/savingcharts/","text":"Saving charts \u00b6 1. Saving a chart \u00b6 To start saving your chart, lets give it a name and description. Click the name of the chart Copy of Latency Histogram and rename it to \"Active Latency\" . To change the description click on Spread of latency values across time. and change this to Overview of latency values in real-time. Click the Save As button. Make sure your chart has a name, it will use the name Active Latency the you defined in the previous step, but you can edit it here if needed. Press the Ok button to continue. 2. Creating a dashboard \u00b6 In the Choose dashboard dialog, we need to create a new dashboard, click on the New Dashboard button. You will now see the New Dashboard Dialog. In here you can give you dashboard a name and description, and set Write Permissions . Please use your own name in the following format to give your dashboard a name e.g. YOUR_NAME-Dashboard . Please replace YOUR_NAME with your own name and then remove the tick from the Anyone in this organization can edit tick box to set up edit permissions. You should see you own login information displayed, meaning you are now the only one who can edit this dashboard. Of course you have the option to add other users or teams from the drop box below that may edit your dashboard and charts, but for now make sure you re-tick the Anyone in this organization can edit box to remove any restrictions and press the Create Button to continue. Your new dashboard is now available and selected so you can save your chart in your new dashboard. Make sure you have your dashboard selected and press the Ok button. You will now be taken to your dashboard like below. You can see at the top left that your YOUR_NAME-DASHBOARD is part of a Dashboard Group YOUR_NAME-Dashboard . You can add other dashboards to this dashboard group. 3. Add to Team page \u00b6 It is common practice to link dashboards that are relevant to a Team to a teams page. So let's add your dashboard to the team page for easy access later. Use the top left hamburger menu select Dashboards from the side menu again. This will bring you to your teams dashboard, We use the team Observability as an example here, the workshop one will be different. Press the + Add Dashboard Group button to add you dashboard to the team page. This will bring you to the Select a dashboard group to link to this team dialog. Type your name (that you used above) in the search box to find your Dashboard. Select it so its highlighted and click the Ok button to add your dashboard. Your dashboard group will appear as part of the team page. Pleasu note during the course of the workshop many more will appear here. Now click on the link for your Dashboard to add more charts!","title":"Saving charts"},{"location":"dashboards/savingcharts/#saving-charts","text":"","title":"Saving charts"},{"location":"dashboards/savingcharts/#1-saving-a-chart","text":"To start saving your chart, lets give it a name and description. Click the name of the chart Copy of Latency Histogram and rename it to \"Active Latency\" . To change the description click on Spread of latency values across time. and change this to Overview of latency values in real-time. Click the Save As button. Make sure your chart has a name, it will use the name Active Latency the you defined in the previous step, but you can edit it here if needed. Press the Ok button to continue.","title":"1. Saving a chart"},{"location":"dashboards/savingcharts/#2-creating-a-dashboard","text":"In the Choose dashboard dialog, we need to create a new dashboard, click on the New Dashboard button. You will now see the New Dashboard Dialog. In here you can give you dashboard a name and description, and set Write Permissions . Please use your own name in the following format to give your dashboard a name e.g. YOUR_NAME-Dashboard . Please replace YOUR_NAME with your own name and then remove the tick from the Anyone in this organization can edit tick box to set up edit permissions. You should see you own login information displayed, meaning you are now the only one who can edit this dashboard. Of course you have the option to add other users or teams from the drop box below that may edit your dashboard and charts, but for now make sure you re-tick the Anyone in this organization can edit box to remove any restrictions and press the Create Button to continue. Your new dashboard is now available and selected so you can save your chart in your new dashboard. Make sure you have your dashboard selected and press the Ok button. You will now be taken to your dashboard like below. You can see at the top left that your YOUR_NAME-DASHBOARD is part of a Dashboard Group YOUR_NAME-Dashboard . You can add other dashboards to this dashboard group.","title":"2. Creating a dashboard"},{"location":"dashboards/savingcharts/#3-add-to-team-page","text":"It is common practice to link dashboards that are relevant to a Team to a teams page. So let's add your dashboard to the team page for easy access later. Use the top left hamburger menu select Dashboards from the side menu again. This will bring you to your teams dashboard, We use the team Observability as an example here, the workshop one will be different. Press the + Add Dashboard Group button to add you dashboard to the team page. This will bring you to the Select a dashboard group to link to this team dialog. Type your name (that you used above) in the search box to find your Dashboard. Select it so its highlighted and click the Ok button to add your dashboard. Your dashboard group will appear as part of the team page. Pleasu note during the course of the workshop many more will appear here. Now click on the link for your Dashboard to add more charts!","title":"3. Add to Team page"},{"location":"dashboards/signalflow/","text":"SignalFlow \u00b6 1. Introduction \u00b6 Let's take a look at SignalFlow - the analytics language of Observability Cloud that can be used to setup monitoring as code. The heart of Splunk Infrastructure Monitoring is the SignalFlow analytics engine that runs computations written in a Python-like language. SignalFlow programs accept streaming input and produce output in real time. SignalFlow provides built-in analytical functions that take metric time series (MTS) as input, perform computations, and output a result MTS. Comparisons with historical norms, e.g. on a week-over-week basis Population overviews using a distributed percentile chart Detecting if the rate of change (or other metric expressed as a ratio, such as a service level objective) has exceeded a critical threshold Finding correlated dimensions, e.g. to determine which service is most correlated with alerts for low disk space Infrastructure Monitoring creates these computations in the Chart Builder user interface, which lets you specify the input MTS to use and the analytical functions you want to apply to them. You can also run SignalFlow programs directly by using the SignalFlow API . SignalFlow includes a large library of built-in analytical functions that take a metrics time series as an input, perform computations on its datapoints, and output time series that are the result of the computation. Info For more information on SignalFlow see Getting started with SignalFlow . 2. View SignalFlow \u00b6 In the chart builder, click on View SignalFlow . You will see the SignalFlow code that composes the chart we were working on. You can now edit the SignalFlow directly within the UI. Our documentation has the full list of SignalFlow functions and methods. Also, you can copy the SignalFlow and use it when interacting with the API or with Terraform to enable Monitoring as Code SignalFlow A = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . publish ( label = 'A' , enable = False ) B = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . timeshift ( '1w' ) . publish ( label = 'B' , enable = False ) C = ( A - B ) . publish ( label = 'C' ) Click on View Builder to go back to the Chart Builder UI. Let's save this new chart to our Dashboard!","title":"SignalFlow"},{"location":"dashboards/signalflow/#signalflow","text":"","title":"SignalFlow"},{"location":"dashboards/signalflow/#1-introduction","text":"Let's take a look at SignalFlow - the analytics language of Observability Cloud that can be used to setup monitoring as code. The heart of Splunk Infrastructure Monitoring is the SignalFlow analytics engine that runs computations written in a Python-like language. SignalFlow programs accept streaming input and produce output in real time. SignalFlow provides built-in analytical functions that take metric time series (MTS) as input, perform computations, and output a result MTS. Comparisons with historical norms, e.g. on a week-over-week basis Population overviews using a distributed percentile chart Detecting if the rate of change (or other metric expressed as a ratio, such as a service level objective) has exceeded a critical threshold Finding correlated dimensions, e.g. to determine which service is most correlated with alerts for low disk space Infrastructure Monitoring creates these computations in the Chart Builder user interface, which lets you specify the input MTS to use and the analytical functions you want to apply to them. You can also run SignalFlow programs directly by using the SignalFlow API . SignalFlow includes a large library of built-in analytical functions that take a metrics time series as an input, perform computations on its datapoints, and output time series that are the result of the computation. Info For more information on SignalFlow see Getting started with SignalFlow .","title":"1. Introduction"},{"location":"dashboards/signalflow/#2-view-signalflow","text":"In the chart builder, click on View SignalFlow . You will see the SignalFlow code that composes the chart we were working on. You can now edit the SignalFlow directly within the UI. Our documentation has the full list of SignalFlow functions and methods. Also, you can copy the SignalFlow and use it when interacting with the API or with Terraform to enable Monitoring as Code SignalFlow A = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . publish ( label = 'A' , enable = False ) B = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . timeshift ( '1w' ) . publish ( label = 'B' , enable = False ) C = ( A - B ) . publish ( label = 'C' ) Click on View Builder to go back to the Chart Builder UI. Let's save this new chart to our Dashboard!","title":"2. View SignalFlow"},{"location":"detectors/","text":"Working with Detectors - Lab Summary \u00b6 Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction \u00b6 Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or to an email address for the Ops team when CPU Utilization has reached the 95th percentile, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance. These conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical. 2. Creating a Detector \u00b6 In Dashboards click on your Custom Dashboard Group (that you created in the previous module) and then click on the dashboard name. We are now going to create a new detector from this chart. Once you see the chart, click on the bell icon on your chart and then on New Detector From Chart . In the text field next to Detector Name , ADD YOUR INITIALS before the proposed detector name. Naming the detector It's important that you add your initials in front of the proposed detector name. It should be something like this: XYZ's Latency Chart Detector . Click on Create Alert Rule In the Detector window, inside Alert signal , the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert. Click on Proceed to Alert Condition 3. Setting Alert condition \u00b6 In Alert condition , click on Static Threshold and then on Proceed to Alert Settings In Alert Settings , enter the value 290 in the Threshold field. In the same window change Time on top right to past day ( -1d ). 4. Alert pre-flight check \u00b6 A pre-flight check will take place after 5 seconds. See the Estimated alert count . Based on the current alert settings, the amount of alerts we would have received in 1 day would have been approx. 18 . About pre-flight checks Once you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day. Immediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Cloud. To read more about detector previewing, please visit this link Setting up detectors . Click on Proceed to Alert Message 5. Alert message \u00b6 In Alert message , under Severity choose Major . Click on Proceed to Alert Recipients Click on Add Recipient and then on your email address displayed as the first option. Notification Services That's the same as entering that email address OR you can enter another email address by clicking on E-mail... . This is just one example of the many Notification Services the suite has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services . 6. Alert Activation \u00b6 Click on Proceed to Alert Activation In Activate... click on Activate Alert Rule If you want to get alerts quicker you can click back on Alert Settings and lower the value from 290 to say 280 . If you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metrics from the last 1 hour. Hover over Alerts in the top menu and then click on Detectors . You will see you detector listed here. If you don't then please refresh your browser. Congratulations ! You have created your first detector and activated it!","title":"Creating a Detector"},{"location":"detectors/#working-with-detectors-lab-summary","text":"Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules","title":"Working with Detectors - Lab Summary"},{"location":"detectors/#1-introduction","text":"Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or to an email address for the Ops team when CPU Utilization has reached the 95th percentile, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance. These conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical.","title":"1. Introduction"},{"location":"detectors/#2-creating-a-detector","text":"In Dashboards click on your Custom Dashboard Group (that you created in the previous module) and then click on the dashboard name. We are now going to create a new detector from this chart. Once you see the chart, click on the bell icon on your chart and then on New Detector From Chart . In the text field next to Detector Name , ADD YOUR INITIALS before the proposed detector name. Naming the detector It's important that you add your initials in front of the proposed detector name. It should be something like this: XYZ's Latency Chart Detector . Click on Create Alert Rule In the Detector window, inside Alert signal , the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert. Click on Proceed to Alert Condition","title":"2. Creating a Detector"},{"location":"detectors/#3-setting-alert-condition","text":"In Alert condition , click on Static Threshold and then on Proceed to Alert Settings In Alert Settings , enter the value 290 in the Threshold field. In the same window change Time on top right to past day ( -1d ).","title":"3. Setting Alert condition"},{"location":"detectors/#4-alert-pre-flight-check","text":"A pre-flight check will take place after 5 seconds. See the Estimated alert count . Based on the current alert settings, the amount of alerts we would have received in 1 day would have been approx. 18 . About pre-flight checks Once you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day. Immediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Cloud. To read more about detector previewing, please visit this link Setting up detectors . Click on Proceed to Alert Message","title":"4. Alert pre-flight check"},{"location":"detectors/#5-alert-message","text":"In Alert message , under Severity choose Major . Click on Proceed to Alert Recipients Click on Add Recipient and then on your email address displayed as the first option. Notification Services That's the same as entering that email address OR you can enter another email address by clicking on E-mail... . This is just one example of the many Notification Services the suite has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services .","title":"5. Alert message"},{"location":"detectors/#6-alert-activation","text":"Click on Proceed to Alert Activation In Activate... click on Activate Alert Rule If you want to get alerts quicker you can click back on Alert Settings and lower the value from 290 to say 280 . If you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metrics from the last 1 hour. Hover over Alerts in the top menu and then click on Detectors . You will see you detector listed here. If you don't then please refresh your browser. Congratulations ! You have created your first detector and activated it!","title":"6. Alert Activation"},{"location":"detectors/muting/","text":"Working with Muting Rules - Lab Summary \u00b6 Learn how to configure Muting Rules Learn how to resume notifications 1. Configuring Muting Rules \u00b6 There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let's create one! From the top left hamburger menu icon click Alerts in the menu and then select Detectors . You will see a list of active detectors. If you created an detector in Creating a Detector you can click on the three dots ... on the far right for that detector; if not, do that for another detector. From the drop-down click on Create Muting Rule... In the Muting Rule window check Mute Indefinitely and enter a reason. Important This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector. Click Next and in the new modal window confirm the muting rule setup. Click on Mute Indefinitely to confirm. You won't be receiving any email notifications from your detector until you resume notifications again. Let's now see how to do that! 2. Resuming notifications \u00b6 To Resume notifications, click on Muting Rules , you will see the name of the detector you muted notifications for under Detector heading. Click on the thee dots ... on the far right. Click on Resume Notifications . Click on Resume to confirm and resume notifications for this detector. Congratulations! You have now resumed your alert notifications!","title":"Creating a Muting Rule"},{"location":"detectors/muting/#working-with-muting-rules-lab-summary","text":"Learn how to configure Muting Rules Learn how to resume notifications","title":"Working with Muting Rules - Lab Summary"},{"location":"detectors/muting/#1-configuring-muting-rules","text":"There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let's create one! From the top left hamburger menu icon click Alerts in the menu and then select Detectors . You will see a list of active detectors. If you created an detector in Creating a Detector you can click on the three dots ... on the far right for that detector; if not, do that for another detector. From the drop-down click on Create Muting Rule... In the Muting Rule window check Mute Indefinitely and enter a reason. Important This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector. Click Next and in the new modal window confirm the muting rule setup. Click on Mute Indefinitely to confirm. You won't be receiving any email notifications from your detector until you resume notifications again. Let's now see how to do that!","title":"1. Configuring Muting Rules"},{"location":"detectors/muting/#2-resuming-notifications","text":"To Resume notifications, click on Muting Rules , you will see the name of the detector you muted notifications for under Detector heading. Click on the thee dots ... on the far right. Click on Resume Notifications . Click on Resume to confirm and resume notifications for this detector. Congratulations! You have now resumed your alert notifications!","title":"2. Resuming notifications"},{"location":"eks/readme-b/","text":"Steps to access EKS cluster \u00b6 Login into AWS console from the Event Engine dashboard \u00b6 Remeber to only use \"ap-southeast-1\" as your region Select EC2 from Management Console and select Instances (running). You should see a EC2 instance called SplunkWorkshop-box Now select SplunkWorkshop instance and select connect Verify EKS cluster \u00b6 Select EC2 Instance Connect tab and click connect. This will open an SSH session to the EC2 instance. Once the instance is opened check the running EKS cluster. You should see a cluster named eksctl get cluster --region ap-southeast-1 Update Kubectl \u00b6 aws eks --region ap-southeast-1 update-kubeconfig --name eksworkshop-eksctl Test Kubectl \u00b6 kubectl get nodes If we see our 2 nodes, we know we have authenticated correctly Install Helm 3 \u00b6 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Get Workshop content \u00b6 wget https://github.com/splunk/imt-workshop/archive/refs/heads/master.zip unzip master.zip mv imt-workshop-master workshop","title":"Get EKS Cluster"},{"location":"eks/readme-b/#steps-to-access-eks-cluster","text":"","title":"Steps to access EKS cluster"},{"location":"eks/readme-b/#login-into-aws-console-from-the-event-engine-dashboard","text":"Remeber to only use \"ap-southeast-1\" as your region Select EC2 from Management Console and select Instances (running). You should see a EC2 instance called SplunkWorkshop-box Now select SplunkWorkshop instance and select connect","title":"Login into AWS console from the Event Engine dashboard"},{"location":"eks/readme-b/#verify-eks-cluster","text":"Select EC2 Instance Connect tab and click connect. This will open an SSH session to the EC2 instance. Once the instance is opened check the running EKS cluster. You should see a cluster named eksctl get cluster --region ap-southeast-1","title":"Verify EKS cluster"},{"location":"eks/readme-b/#update-kubectl","text":"aws eks --region ap-southeast-1 update-kubeconfig --name eksworkshop-eksctl","title":"Update Kubectl"},{"location":"eks/readme-b/#test-kubectl","text":"kubectl get nodes If we see our 2 nodes, we know we have authenticated correctly","title":"Test Kubectl"},{"location":"eks/readme-b/#install-helm-3","text":"curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install Helm 3"},{"location":"eks/readme-b/#get-workshop-content","text":"wget https://github.com/splunk/imt-workshop/archive/refs/heads/master.zip unzip master.zip mv imt-workshop-master workshop","title":"Get Workshop content"},{"location":"lambda/","text":"APM with AWS Lambda (Developer Focused) \u00b6 Enabling APM If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed! An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from Splunk's Observability team to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to Splunk's observability suite and check that you have the APM tab on the top navbar next to Dashboards. 1. AWS Lambda exercise & APM overview \u00b6 This workshop section is focused on developer's of serverless/Lambda application/functions. This workshop is going to guide them through the steps to add Tracing to Python and Node-Js Lambda Functions, and see traces flow from an on-prem Java application though the various Python and Node-Js Lambda Functions in Splunk APM. Splunk APM captures end-to-end distributed transactions from your applications, including serverless apps (Lambda's) with trace spans sent directly to Splunk or via an optional OpenTelemetry Collector that act as a central aggregation point prior to sending trace spans to Splunk. (recommended, and show in the workshop). In addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment. The following illustration shows the recommended deployment model: Splunk's auto-instrumentation libraries send spans to the OpenTelemetry Collector; the OpenTelemetry Collector forwards the spans to Observability Cloud. 2. AWS Lambda exercise requirements flow \u00b6 During this workshop you will perform the following activities: Perform a test run of the Splunk Mobile Phone Web shop and its services Enable Tracing on local SpringBoot App Enable Tracing on First Python & Node-js Lambda Functions Enable Tracing on the other functions Enrich the spans Look at configuring options for the OpenTelemetry Collector 3. AWS Lambda exercise requirements \u00b6 This workshop section assumes that you have access to the following features as they are required for the workshop: AWS account: Access to EC2 Instances Ability to create/run AWS Lambda's","title":"APM with AWS Lambda (Developer Focused)"},{"location":"lambda/#apm-with-aws-lambda-developer-focused","text":"Enabling APM If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed! An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from Splunk's Observability team to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to Splunk's observability suite and check that you have the APM tab on the top navbar next to Dashboards.","title":"APM with AWS Lambda (Developer Focused)"},{"location":"lambda/#1-aws-lambda-exercise-apm-overview","text":"This workshop section is focused on developer's of serverless/Lambda application/functions. This workshop is going to guide them through the steps to add Tracing to Python and Node-Js Lambda Functions, and see traces flow from an on-prem Java application though the various Python and Node-Js Lambda Functions in Splunk APM. Splunk APM captures end-to-end distributed transactions from your applications, including serverless apps (Lambda's) with trace spans sent directly to Splunk or via an optional OpenTelemetry Collector that act as a central aggregation point prior to sending trace spans to Splunk. (recommended, and show in the workshop). In addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment. The following illustration shows the recommended deployment model: Splunk's auto-instrumentation libraries send spans to the OpenTelemetry Collector; the OpenTelemetry Collector forwards the spans to Observability Cloud.","title":"1. AWS Lambda exercise &amp; APM overview"},{"location":"lambda/#2-aws-lambda-exercise-requirements-flow","text":"During this workshop you will perform the following activities: Perform a test run of the Splunk Mobile Phone Web shop and its services Enable Tracing on local SpringBoot App Enable Tracing on First Python & Node-js Lambda Functions Enable Tracing on the other functions Enrich the spans Look at configuring options for the OpenTelemetry Collector","title":"2. AWS Lambda exercise requirements flow"},{"location":"lambda/#3-aws-lambda-exercise-requirements","text":"This workshop section assumes that you have access to the following features as they are required for the workshop: AWS account: Access to EC2 Instances Ability to create/run AWS Lambda's","title":"3. AWS Lambda exercise requirements"},{"location":"lambda/initial_run_env/","text":"Initial run of Splunk Mobile Shop Application \u00b6 The goal of this session is to make you familiar with the various components that are used in the workshop. Required information You should have the following information at hand as you will need this in various places throughout the workshop Access to the AWS account that is used to setup the workshop The region the Workshop is using to run the workshop (i.e. Ireland, Frankfurt, Ohio, Tokio ..) A unique UID string allowing you to identify your services IP address & password of the EC2 instance assigned to you 1. Validate availability of Lambda Functions \u00b6 Please log into the AWS account that has been used to create the workshop and select the region that is used by the workshop - the Splunk SE or your organisations lead will confirm this at the start of the Workshop. In the example below the Region is Frankfurt, but we may be using a different one for this Workshop. Once the Lambda service have been selected you should see a list of available Lambda Functions. To find the ones that are assigned to you, use the filter option with the the Unique ID you have been allocated, you should have something similar to: UID _RetailOrder UID _RetailOrderLine UID _RetailOrderPrice UID _RetailOrderDiscount (Where UID represents the Unique ID you have been provided - ACME in our example) 2. Verify CloudWatch Logs Location \u00b6 To investigate issues that may occur during the run, we need to check the CloudWatch logs. Open CloudWatch Log Groups in a new browser tab so you can easily switch between the Lambda Functions and the Logs. The activity to pick in CloudWatch is the Log Group section. If there are logs present, you can filter on your preset like before. The result should be that there are no logs visible (see below). If there are logs, even after filtering on your preset, make sure they are not related to the 4 service above, or delete them if possible). Once you have deleted the logs, the filter option will be disabled until there are new logs. 3. Connect to your EC2 instance \u00b6 Next open a Terminal window and log into the EC2 instance you have been assigned. Note If you need help with this, here are the instructions how to access you pre-configured AWS/EC2 instance . If you need help with this, here are the instructions on how to access you pre-configured AWS/EC2 instance . (If you can, open a second ssh window to you EC2 instance, as this will be useful later in the workshop) Please return here after you have successfully connected to your instance. Once connected move into the correct directory to run the Java SpringBoot application by running the following command within your instances shell session: Shell Command cd ~/SplunkLambdaAPM/MobileShop/Base From here we will start the Java SpringBoot application that contains our simple web shop application. Run the application by issuing the following command: Shell Command mvn spring-boot:run On the first run SpringBoot will download a lot of packages like in the image below, be patience! The next runs will be much faster. but as soon as everything is loaded, you should see the SpringBoot logo. We are now ready to test the app. 4. Test the Splunk Mobile Phone Shop App \u00b6 Open another new browser tab and navigate to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) URL http://[ec2_ip]:8080/order To test your environment enter the following information: Name of a phone: for example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver And hit submit to run a test though your system. The result should be similar to this : You can use The Submit another order to go back and reorder a new phone! 5. Verify Logs in CloudWatch \u00b6 Even if there are no errors and you have the above result, check the Cloudwatch logs to verify they have been created. Go to the CloudWatch tab you opened in step 2 and refresh, confirm that all four logs have been generated. Assuming you have all four logs groups listed, you are now ready to add Traces and Spans, however if there are not four logs listed then we have a problem, so please bring this to the attention of the Splunk SE running the Workshop.","title":"Initial run of Splunk Mobile Shop Application"},{"location":"lambda/initial_run_env/#initial-run-of-splunk-mobile-shop-application","text":"The goal of this session is to make you familiar with the various components that are used in the workshop. Required information You should have the following information at hand as you will need this in various places throughout the workshop Access to the AWS account that is used to setup the workshop The region the Workshop is using to run the workshop (i.e. Ireland, Frankfurt, Ohio, Tokio ..) A unique UID string allowing you to identify your services IP address & password of the EC2 instance assigned to you","title":"Initial run of Splunk Mobile Shop Application"},{"location":"lambda/initial_run_env/#1-validate-availability-of-lambda-functions","text":"Please log into the AWS account that has been used to create the workshop and select the region that is used by the workshop - the Splunk SE or your organisations lead will confirm this at the start of the Workshop. In the example below the Region is Frankfurt, but we may be using a different one for this Workshop. Once the Lambda service have been selected you should see a list of available Lambda Functions. To find the ones that are assigned to you, use the filter option with the the Unique ID you have been allocated, you should have something similar to: UID _RetailOrder UID _RetailOrderLine UID _RetailOrderPrice UID _RetailOrderDiscount (Where UID represents the Unique ID you have been provided - ACME in our example)","title":"1. Validate availability of Lambda Functions"},{"location":"lambda/initial_run_env/#2-verify-cloudwatch-logs-location","text":"To investigate issues that may occur during the run, we need to check the CloudWatch logs. Open CloudWatch Log Groups in a new browser tab so you can easily switch between the Lambda Functions and the Logs. The activity to pick in CloudWatch is the Log Group section. If there are logs present, you can filter on your preset like before. The result should be that there are no logs visible (see below). If there are logs, even after filtering on your preset, make sure they are not related to the 4 service above, or delete them if possible). Once you have deleted the logs, the filter option will be disabled until there are new logs.","title":"2. Verify CloudWatch Logs Location"},{"location":"lambda/initial_run_env/#3-connect-to-your-ec2-instance","text":"Next open a Terminal window and log into the EC2 instance you have been assigned. Note If you need help with this, here are the instructions how to access you pre-configured AWS/EC2 instance . If you need help with this, here are the instructions on how to access you pre-configured AWS/EC2 instance . (If you can, open a second ssh window to you EC2 instance, as this will be useful later in the workshop) Please return here after you have successfully connected to your instance. Once connected move into the correct directory to run the Java SpringBoot application by running the following command within your instances shell session: Shell Command cd ~/SplunkLambdaAPM/MobileShop/Base From here we will start the Java SpringBoot application that contains our simple web shop application. Run the application by issuing the following command: Shell Command mvn spring-boot:run On the first run SpringBoot will download a lot of packages like in the image below, be patience! The next runs will be much faster. but as soon as everything is loaded, you should see the SpringBoot logo. We are now ready to test the app.","title":"3. Connect to your EC2 instance"},{"location":"lambda/initial_run_env/#4-test-the-splunk-mobile-phone-shop-app","text":"Open another new browser tab and navigate to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) URL http://[ec2_ip]:8080/order To test your environment enter the following information: Name of a phone: for example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver And hit submit to run a test though your system. The result should be similar to this : You can use The Submit another order to go back and reorder a new phone!","title":"4. Test the Splunk Mobile Phone Shop App"},{"location":"lambda/initial_run_env/#5-verify-logs-in-cloudwatch","text":"Even if there are no errors and you have the above result, check the Cloudwatch logs to verify they have been created. Go to the CloudWatch tab you opened in step 2 and refresh, confirm that all four logs have been generated. Assuming you have all four logs groups listed, you are now ready to add Traces and Spans, however if there are not four logs listed then we have a problem, so please bring this to the attention of the Splunk SE running the Workshop.","title":"5. Verify Logs in CloudWatch"},{"location":"lambda/retail-order-I/","text":"Enable APM in the RetailOrder Function \u00b6 1. Edit UID_RetailOrder Lambda in your AWS environment \u00b6 To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: Now select the Lambda Function UID _RetailOrder to open the browser based editor environment for Lambda functions. Now scroll down so you have the full editor visible. To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow two lines at the top of the file and add an empty line. RetailOrder Updates import signalfx_lambda import opentracing Add the following two lines above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrder Updates @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() Verify that the top of the file now looks like this: RetailOrder Updates import signalfx_lambda import opentracing import os import json import boto3 import requests # The Environment Tag is used by Splunk APM to filter Environments in UI APM_ENVIRONMENT = os.environ['SIGNALFX_APM_ENVIRONMENT'] PRICE_URL = os.environ['PRICE_URL'] ORDER_LINE = os.environ['ORDER_LINE'] # Define the client to interact with AWS Lambda client = boto3.client('lambda') @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() def lambda_handler(event,context): print(\"event received :\", event) To save your work, you must press on the Deploy button above the editor as show here. 3. Run a case and find both the Service Dashboard for your lambda function & your trace \u00b6 Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this: 3.1 Find your Service Dashboard for the RetailOrder Lambda Function in Splunk APM \u00b6 Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Lambda Function will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes. 3.1 Look at trace info in splunk APM \u00b6 Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application.","title":"Enable APM in the RetailOrder Function"},{"location":"lambda/retail-order-I/#enable-apm-in-the-retailorder-function","text":"","title":"Enable APM in the RetailOrder Function"},{"location":"lambda/retail-order-I/#1-edit-uid_retailorder-lambda-in-your-aws-environment","text":"To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: Now select the Lambda Function UID _RetailOrder to open the browser based editor environment for Lambda functions. Now scroll down so you have the full editor visible. To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow two lines at the top of the file and add an empty line. RetailOrder Updates import signalfx_lambda import opentracing Add the following two lines above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrder Updates @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() Verify that the top of the file now looks like this: RetailOrder Updates import signalfx_lambda import opentracing import os import json import boto3 import requests # The Environment Tag is used by Splunk APM to filter Environments in UI APM_ENVIRONMENT = os.environ['SIGNALFX_APM_ENVIRONMENT'] PRICE_URL = os.environ['PRICE_URL'] ORDER_LINE = os.environ['ORDER_LINE'] # Define the client to interact with AWS Lambda client = boto3.client('lambda') @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() def lambda_handler(event,context): print(\"event received :\", event) To save your work, you must press on the Deploy button above the editor as show here.","title":"1. Edit UID_RetailOrder Lambda in your AWS environment"},{"location":"lambda/retail-order-I/#3-run-a-case-and-find-both-the-service-dashboard-for-your-lambda-function-your-trace","text":"Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this:","title":"3. Run a case and find both the Service Dashboard for your lambda function &amp; your trace"},{"location":"lambda/retail-order-I/#31-find-your-service-dashboard-for-the-retailorder-lambda-function-in-splunk-apm","text":"Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Lambda Function will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes.","title":"3.1 Find your Service Dashboard for the RetailOrder Lambda Function in Splunk APM"},{"location":"lambda/retail-order-I/#31-look-at-trace-info-in-splunk-apm","text":"Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application.","title":"3.1 Look at trace info in splunk APM"},{"location":"lambda/retail-orderline-I/","text":"Enable APM in the RetailOrderLine Function \u00b6 1. Edit UID_RetailOderLine Lambda in your AWS Environment \u00b6 To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: INSERT SCREENSHOT Now select the Lambda Function UID _RetailOrderLine to open the browser based editor environment for Lambda functions. INSERT SCREENSHOT Enable the Full Screen Editor and then close the 'Execution Result' tab to give you a clean working area INSERT SCREENSHOT To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow lines at the top of the file and add an empty line. RetailOrderLine Updates import signalfx_lambda import opentracing from opentracing.ext import tags from opentracing.propagation import Format Add the following line above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrderLine Updates @signalfx_lambda.is_traced(with_span=False) Verify that the file now looks like this: INSERT SCREENSHOT","title":"Enable APM in the RetailOrderLine Function"},{"location":"lambda/retail-orderline-I/#enable-apm-in-the-retailorderline-function","text":"","title":"Enable APM in the RetailOrderLine Function"},{"location":"lambda/retail-orderline-I/#1-edit-uid_retailoderline-lambda-in-your-aws-environment","text":"To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: INSERT SCREENSHOT Now select the Lambda Function UID _RetailOrderLine to open the browser based editor environment for Lambda functions. INSERT SCREENSHOT Enable the Full Screen Editor and then close the 'Execution Result' tab to give you a clean working area INSERT SCREENSHOT To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow lines at the top of the file and add an empty line. RetailOrderLine Updates import signalfx_lambda import opentracing from opentracing.ext import tags from opentracing.propagation import Format Add the following line above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrderLine Updates @signalfx_lambda.is_traced(with_span=False) Verify that the file now looks like this: INSERT SCREENSHOT","title":"1. Edit UID_RetailOderLine Lambda in your AWS Environment"},{"location":"lambda/setup/","text":"Workshop Setup \u00b6 This setup module is used to prepare a set of AWS Lambda Functions and EC2 Instances to be used for the workshop, these will need to be created by an AWS Admin who has access to a suitable account that can be used for the workshop. There will be 4 Lambda Functions and 1 EC2 instance deployed for each participant. Every resource will be prefixed with a Unique ID (UID) to identify which attendee they belong to. Terraform is used to deploy all of the resources and this module details the steps required to install and configure terraform, configure it to authenticate with AWS, and then deploy the resources. 1. Install Terraform \u00b6 Terraform is used to deploy all of the AWS infrastructure for the workshop, so needs to be installed on your machine. Instructions on how to install Terraform can be found here . Terraform AWS Authentication \u00b6 Once Terraform is installed, you need to configure it to authenticate with your AWS Account. Details on AWS Authentication can be found here . The AWS Authentication consists of two files, config and credentials which are typically located in the ~/.ssh folder. The config file details different profiles, and works in conjunction with the credentials file which contains your access and secret keys. config [default] region = us-east-1 output = json [profile splunk] region = us-east-1 output = json authentication [default] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key} [splunk] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key} 2. Clone Workshop Content - WILL NEED TO UPDATE URL ONCE PUBLISHED TO SPLUNK REPO \u00b6 The Workshop Content needs to be pulled down from Github to your local machine, then updated with your specific settings for AWS Authentication. 2.1 Download the Workshop Repo \u00b6 Git Clone git clone git clone https://github.com/geoffhigginbottom/tflambdatestv2.git example result Workshop git clone https://github.com/geoffhigginbottom/tflambdatestv2.git cloning into 'tflambdatestv2'... remote: Enumerating objects: 276, done. remote: Counting objects: 100% (276/276), done. remote: Compressing objects: 100% (186/186), done. remote: Total 276 (delta 159), reused 206 (delta 89), pack-reused 0 Receiving objects: 100% (276/276), 269.48 KiB | 1.13 MiB/s, done. Resolving deltas: 100% (159/159), done. 2.2 Create terraform.tfvars \u00b6 A file called terraform.tfvars needs to be created and populated with your specific settings. This file contains all of the settings required to enable Terraform to connect to both your AWS and Splunk Environments. An example version of the file is included in the repo named terraform.tfvars.example , which you should copy and rename to terraform.tfvars . Run the following command from within the directory where the workshop content was download. crete terraform.tfvars cp terraform.tfvars.example terraform.tfvars Then update the newly created terraform.tfvars starting with the AWS Variables Section. 2.2.1 AWS Variables Section \u00b6 profile should match the profile name used in your aws authentication file which Terraform will use to authenticate with AWS key_name is the name of the ssh_key you wish to use to access the EC2 instances (note password login is also enabled on the Instances) private_key_path is the path to your private ssh key, such as ~/.ssh/xxx.pem or ~/.ssh/id_rsa instance_type is the AWS Instance Type used for the EC2 Instances - this defaults to the free tier \"t2.micro\" region is an optional parameter, normally left commented out. It enables you to override the region prompt during the Terraform deployment terraform.tfvars - AWS Variables ### AWS Variables ### profile = \"xxx\" key_name = \"yyy\" private_key_path = \"~/.ssh/xxx.pem or ~/.ssh/id_rsa etc\" instance_type = \"t2.micro\" ## Terraform will prompt you to select an AWS Region to deploy the resources into ## Setting a region here removes the region prompt (default is to have it prompt you) ## List of supported regions can be found in the variables.tf file #region = \"2\" 2.2.2 Splunk Variables Section \u00b6 function_version is an optional parameter, normally left commented out. It enables you to override the version prompt during the Terraform deployment access_token is the token you wish to use to authenticate with the Splunk Monitoring backend realm specifies which Realm your Splunk Monitoring backend is deployed in collector_image specifies the contributor version of the otel collector, and the latest version can be found here and will need updating as new versions are released terraform.tfvars - Splunk Variables ### Splunk Variables ### ## Terraform will prompt you to select a version to be deployed, ## \"a\" = apm version, \"b\" = base version. ## Base version is typically deployed, but the apm version can be deployed ## for testing and comparison purposes #function_version = \"b\" access_token = \"xxxxxxxxxx\" realm = \"xxx\" # smart_agent_version = \"5.6.0-1\" # Optional - If left blank, latest will be installed smart_agent_version = \"\" # Optional - If left blank, latest will be installed ## Latest otel collector releases can be found at ## https://github.com/open-telemetry/opentelemetry-collector-contrib/releases collector_image = \"otel/opentelemetry-collector-contrib:0.15.0\" 2.3 Generate the Unique IDs \u00b6 A file named quantity.auto.tfvars needs to be created and populated with your specific settings. This file contains the Unique IDs which will be appended to each AWS Resource to identify which workshop participant they are allocated to. There is an example file in the repo called quantity.auto.tfvars.example which needs to be copied and renamed to quantity.auto.tfvars . Run the following command from within the directory where the workshop content was download. crete quantity.auto.tfvars cp quantity.auto.tfvars.example quantity.auto.tfvars Edit quantity.auto.tfvars and populate the list of participants, ensuring each value is unique and has no spaces. Ensure the function_count value equals the total number of names, and that each entry ends with a comma, apart from the last one, as per the example below. quantity.auto.tfvars function_count = \"3\" function_ids = [ \"John\", \"Sarah\", \"Amir\" ] 3. Initialize Terraform \u00b6 Once you have finished creating and updating xxx and yyy you now need to initialize terraform, so run the following command: Terraform init teraform init Example output Initializing the backend... Initializing provider plugins... - Finding latest version of terraform-providers/docker... - Finding latest version of hashicorp/null... - Finding latest version of hashicorp/archive... - Finding latest version of hashicorp/aws... - Installing hashicorp/archive v2.0.0... - Installed hashicorp/archive v2.0.0 (signed by HashiCorp) - Installing hashicorp/aws v3.18.0... - Installed hashicorp/aws v3.18.0 (signed by HashiCorp) - Installing terraform-providers/docker v2.7.2... - Installed terraform-providers/docker v2.7.2 (signed by HashiCorp) - Installing hashicorp/null v3.0.0... - Installed hashicorp/null v3.0.0 (signed by HashiCorp) The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, we recommend adding version constraints in a required_providers block in your configuration, with the constraint strings suggested below. * hashicorp/archive: version = \"~> 2.0.0\" * hashicorp/aws: version = \"~> 3.18.0\" * hashicorp/null: version = \"~> 3.0.0\" * terraform-providers/docker: version = \"~> 2.7.2\" Warning: Additional provider information from registry The remote registry returned warnings for registry.terraform.io/terraform-providers/docker: - For users on Terraform 0.13 or greater, this provider has moved to kreuzwerker/docker. Please update your source in required_providers. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. 4. Deploy the Workshop \u00b6 4.1 Terraform Plan \u00b6 You can now deploy the workshop using Terraform. It is always best practice to run a plan so you can check what changes Terraform is going to make. When executing Terraform will prompt you to select a version of the workshop (select b for base), and also an AWS Region (choose an appropriate one from the list) Terraform plan terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Example output terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.aws_ami.latest-ubuntu: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create <= read (data resources) Terraform will perform the following actions: # data.archive_file.retailorder_lambda_zip will be read during apply # (config refers to values not yet known) <= data \"archive_file\" \"retailorder_lambda_zip\" { + id = (known after apply) + output_base64sha256 = (known after apply) + output_md5 = (known after apply) + output_path = \"retailorder_lambda.zip\" + output_sha = (known after apply) + output_size = (known after apply) + source_file = \"retailorder_lambda_function.py\" + type = \"zip\" } ........ EXTRA LINES REMOVED ........ Plan: 106 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. 4.3 Terraform Apply \u00b6 After checking the plan output looks OK, you can now apply the deployment, using the same options as when you ran the plan , and entering yes when prompted: Terraform apply terraform apply var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Plan: 106 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Example output Apply complete! Resources: 106 added, 0 changed, 0 destroyed. Outputs: OTC_Instances = [ \"john_otc, 52.47.138.166\", \"sarah_otc, 15.188.51.119\", \"amir_otc, 35.180.230.215\", ] retaildiscount_invoke_url = [ \"John_RetailOrderDiscount_api_gateway, o91dr4o3c6.execute-api.eu-west-3.amazonaws.com\", \"Sarah_RetailOrderDiscount_api_gateway, leelg1e0y9.execute-api.eu-west-3.amazonaws.com\", \"Amir_RetailOrderDiscount_api_gateway, 0gdlkj8ceb.execute-api.eu-west-3.amazonaws.com\", ] retailorder_arns = [ \"John_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrder\", \"Sarah_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrder\", \"Amir_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrder\", ] retailorder_invoke_url = [ \"John_RetailOrder_api_gateway, https://286kkhj9k1.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrder_api_gateway, https://rgv6lwdf81.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrder_api_gateway, https://5lnuuesqz3.execute-api.eu-west-3.amazonaws.com/default\", ] retailorderdiscount_arns = [ \"John_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderDiscount\", ] retailorderdiscount_path = [ \"John_RetailOrderDiscount, /default/John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, /default/Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, /default/Amir_RetailOrderDiscount\", ] retailorderline_arns = [ \"John_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderLine\", \"Sarah_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderLine\", \"Amir_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderLine\", ] retailorderprice_arns = [ \"John_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderPrice\", \"Sarah_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderPrice\", \"Amir_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderPrice\", ] retailorderprice_invoke_url = [ \"John_RetailOrderPrice_api_gateway, https://gohem8pwib.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrderPrice_api_gateway, https://dv23py0xo0.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrderPrice_api_gateway, https://dv1rn91g83.execute-api.eu-west-3.amazonaws.com/default\", ] 4.3 What Did We Deploy? \u00b6 Assuming a successful deployment, if you check in your AWS Console you should find the following (with multiple versions where UID matches the user names from quantity.auto.tfvars ): Lambda Functions UID_RetailOrder UID_RetailOrderDiscount UID_RetailOrderLine UID_RetailOrderPrice Lambda Layers request-opentracing_2_0 Instances UID_otc API Gateways UID_RetailOrder_api_gateway UID_RetailOrderDiscount_api_gateway UID_RetailOrderPrice_api_gateway IAM Roles splunk_lambda_role_xxxxxxxxxx IAM Policies lambda_initiate_lambda_policy_xxxxxxxxxxx","title":"Workshop Setup"},{"location":"lambda/setup/#workshop-setup","text":"This setup module is used to prepare a set of AWS Lambda Functions and EC2 Instances to be used for the workshop, these will need to be created by an AWS Admin who has access to a suitable account that can be used for the workshop. There will be 4 Lambda Functions and 1 EC2 instance deployed for each participant. Every resource will be prefixed with a Unique ID (UID) to identify which attendee they belong to. Terraform is used to deploy all of the resources and this module details the steps required to install and configure terraform, configure it to authenticate with AWS, and then deploy the resources.","title":"Workshop Setup"},{"location":"lambda/setup/#1-install-terraform","text":"Terraform is used to deploy all of the AWS infrastructure for the workshop, so needs to be installed on your machine. Instructions on how to install Terraform can be found here .","title":"1. Install Terraform"},{"location":"lambda/setup/#terraform-aws-authentication","text":"Once Terraform is installed, you need to configure it to authenticate with your AWS Account. Details on AWS Authentication can be found here . The AWS Authentication consists of two files, config and credentials which are typically located in the ~/.ssh folder. The config file details different profiles, and works in conjunction with the credentials file which contains your access and secret keys. config [default] region = us-east-1 output = json [profile splunk] region = us-east-1 output = json authentication [default] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key} [splunk] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key}","title":"Terraform AWS Authentication"},{"location":"lambda/setup/#2-clone-workshop-content-will-need-to-update-url-once-published-to-splunk-repo","text":"The Workshop Content needs to be pulled down from Github to your local machine, then updated with your specific settings for AWS Authentication.","title":"2. Clone Workshop Content - WILL NEED TO UPDATE URL ONCE PUBLISHED TO SPLUNK REPO"},{"location":"lambda/setup/#21-download-the-workshop-repo","text":"Git Clone git clone git clone https://github.com/geoffhigginbottom/tflambdatestv2.git example result Workshop git clone https://github.com/geoffhigginbottom/tflambdatestv2.git cloning into 'tflambdatestv2'... remote: Enumerating objects: 276, done. remote: Counting objects: 100% (276/276), done. remote: Compressing objects: 100% (186/186), done. remote: Total 276 (delta 159), reused 206 (delta 89), pack-reused 0 Receiving objects: 100% (276/276), 269.48 KiB | 1.13 MiB/s, done. Resolving deltas: 100% (159/159), done.","title":"2.1 Download the Workshop Repo"},{"location":"lambda/setup/#22-create-terraformtfvars","text":"A file called terraform.tfvars needs to be created and populated with your specific settings. This file contains all of the settings required to enable Terraform to connect to both your AWS and Splunk Environments. An example version of the file is included in the repo named terraform.tfvars.example , which you should copy and rename to terraform.tfvars . Run the following command from within the directory where the workshop content was download. crete terraform.tfvars cp terraform.tfvars.example terraform.tfvars Then update the newly created terraform.tfvars starting with the AWS Variables Section.","title":"2.2 Create terraform.tfvars"},{"location":"lambda/setup/#221-aws-variables-section","text":"profile should match the profile name used in your aws authentication file which Terraform will use to authenticate with AWS key_name is the name of the ssh_key you wish to use to access the EC2 instances (note password login is also enabled on the Instances) private_key_path is the path to your private ssh key, such as ~/.ssh/xxx.pem or ~/.ssh/id_rsa instance_type is the AWS Instance Type used for the EC2 Instances - this defaults to the free tier \"t2.micro\" region is an optional parameter, normally left commented out. It enables you to override the region prompt during the Terraform deployment terraform.tfvars - AWS Variables ### AWS Variables ### profile = \"xxx\" key_name = \"yyy\" private_key_path = \"~/.ssh/xxx.pem or ~/.ssh/id_rsa etc\" instance_type = \"t2.micro\" ## Terraform will prompt you to select an AWS Region to deploy the resources into ## Setting a region here removes the region prompt (default is to have it prompt you) ## List of supported regions can be found in the variables.tf file #region = \"2\"","title":"2.2.1 AWS Variables Section"},{"location":"lambda/setup/#222-splunk-variables-section","text":"function_version is an optional parameter, normally left commented out. It enables you to override the version prompt during the Terraform deployment access_token is the token you wish to use to authenticate with the Splunk Monitoring backend realm specifies which Realm your Splunk Monitoring backend is deployed in collector_image specifies the contributor version of the otel collector, and the latest version can be found here and will need updating as new versions are released terraform.tfvars - Splunk Variables ### Splunk Variables ### ## Terraform will prompt you to select a version to be deployed, ## \"a\" = apm version, \"b\" = base version. ## Base version is typically deployed, but the apm version can be deployed ## for testing and comparison purposes #function_version = \"b\" access_token = \"xxxxxxxxxx\" realm = \"xxx\" # smart_agent_version = \"5.6.0-1\" # Optional - If left blank, latest will be installed smart_agent_version = \"\" # Optional - If left blank, latest will be installed ## Latest otel collector releases can be found at ## https://github.com/open-telemetry/opentelemetry-collector-contrib/releases collector_image = \"otel/opentelemetry-collector-contrib:0.15.0\"","title":"2.2.2 Splunk Variables Section"},{"location":"lambda/setup/#23-generate-the-unique-ids","text":"A file named quantity.auto.tfvars needs to be created and populated with your specific settings. This file contains the Unique IDs which will be appended to each AWS Resource to identify which workshop participant they are allocated to. There is an example file in the repo called quantity.auto.tfvars.example which needs to be copied and renamed to quantity.auto.tfvars . Run the following command from within the directory where the workshop content was download. crete quantity.auto.tfvars cp quantity.auto.tfvars.example quantity.auto.tfvars Edit quantity.auto.tfvars and populate the list of participants, ensuring each value is unique and has no spaces. Ensure the function_count value equals the total number of names, and that each entry ends with a comma, apart from the last one, as per the example below. quantity.auto.tfvars function_count = \"3\" function_ids = [ \"John\", \"Sarah\", \"Amir\" ]","title":"2.3 Generate the Unique IDs"},{"location":"lambda/setup/#3-initialize-terraform","text":"Once you have finished creating and updating xxx and yyy you now need to initialize terraform, so run the following command: Terraform init teraform init Example output Initializing the backend... Initializing provider plugins... - Finding latest version of terraform-providers/docker... - Finding latest version of hashicorp/null... - Finding latest version of hashicorp/archive... - Finding latest version of hashicorp/aws... - Installing hashicorp/archive v2.0.0... - Installed hashicorp/archive v2.0.0 (signed by HashiCorp) - Installing hashicorp/aws v3.18.0... - Installed hashicorp/aws v3.18.0 (signed by HashiCorp) - Installing terraform-providers/docker v2.7.2... - Installed terraform-providers/docker v2.7.2 (signed by HashiCorp) - Installing hashicorp/null v3.0.0... - Installed hashicorp/null v3.0.0 (signed by HashiCorp) The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, we recommend adding version constraints in a required_providers block in your configuration, with the constraint strings suggested below. * hashicorp/archive: version = \"~> 2.0.0\" * hashicorp/aws: version = \"~> 3.18.0\" * hashicorp/null: version = \"~> 3.0.0\" * terraform-providers/docker: version = \"~> 2.7.2\" Warning: Additional provider information from registry The remote registry returned warnings for registry.terraform.io/terraform-providers/docker: - For users on Terraform 0.13 or greater, this provider has moved to kreuzwerker/docker. Please update your source in required_providers. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"3. Initialize Terraform"},{"location":"lambda/setup/#4-deploy-the-workshop","text":"","title":"4. Deploy the Workshop"},{"location":"lambda/setup/#41-terraform-plan","text":"You can now deploy the workshop using Terraform. It is always best practice to run a plan so you can check what changes Terraform is going to make. When executing Terraform will prompt you to select a version of the workshop (select b for base), and also an AWS Region (choose an appropriate one from the list) Terraform plan terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Example output terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.aws_ami.latest-ubuntu: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create <= read (data resources) Terraform will perform the following actions: # data.archive_file.retailorder_lambda_zip will be read during apply # (config refers to values not yet known) <= data \"archive_file\" \"retailorder_lambda_zip\" { + id = (known after apply) + output_base64sha256 = (known after apply) + output_md5 = (known after apply) + output_path = \"retailorder_lambda.zip\" + output_sha = (known after apply) + output_size = (known after apply) + source_file = \"retailorder_lambda_function.py\" + type = \"zip\" } ........ EXTRA LINES REMOVED ........ Plan: 106 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run.","title":"4.1 Terraform Plan"},{"location":"lambda/setup/#43-terraform-apply","text":"After checking the plan output looks OK, you can now apply the deployment, using the same options as when you ran the plan , and entering yes when prompted: Terraform apply terraform apply var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Plan: 106 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Example output Apply complete! Resources: 106 added, 0 changed, 0 destroyed. Outputs: OTC_Instances = [ \"john_otc, 52.47.138.166\", \"sarah_otc, 15.188.51.119\", \"amir_otc, 35.180.230.215\", ] retaildiscount_invoke_url = [ \"John_RetailOrderDiscount_api_gateway, o91dr4o3c6.execute-api.eu-west-3.amazonaws.com\", \"Sarah_RetailOrderDiscount_api_gateway, leelg1e0y9.execute-api.eu-west-3.amazonaws.com\", \"Amir_RetailOrderDiscount_api_gateway, 0gdlkj8ceb.execute-api.eu-west-3.amazonaws.com\", ] retailorder_arns = [ \"John_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrder\", \"Sarah_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrder\", \"Amir_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrder\", ] retailorder_invoke_url = [ \"John_RetailOrder_api_gateway, https://286kkhj9k1.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrder_api_gateway, https://rgv6lwdf81.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrder_api_gateway, https://5lnuuesqz3.execute-api.eu-west-3.amazonaws.com/default\", ] retailorderdiscount_arns = [ \"John_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderDiscount\", ] retailorderdiscount_path = [ \"John_RetailOrderDiscount, /default/John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, /default/Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, /default/Amir_RetailOrderDiscount\", ] retailorderline_arns = [ \"John_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderLine\", \"Sarah_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderLine\", \"Amir_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderLine\", ] retailorderprice_arns = [ \"John_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderPrice\", \"Sarah_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderPrice\", \"Amir_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderPrice\", ] retailorderprice_invoke_url = [ \"John_RetailOrderPrice_api_gateway, https://gohem8pwib.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrderPrice_api_gateway, https://dv23py0xo0.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrderPrice_api_gateway, https://dv1rn91g83.execute-api.eu-west-3.amazonaws.com/default\", ]","title":"4.3 Terraform Apply"},{"location":"lambda/setup/#43-what-did-we-deploy","text":"Assuming a successful deployment, if you check in your AWS Console you should find the following (with multiple versions where UID matches the user names from quantity.auto.tfvars ): Lambda Functions UID_RetailOrder UID_RetailOrderDiscount UID_RetailOrderLine UID_RetailOrderPrice Lambda Layers request-opentracing_2_0 Instances UID_otc API Gateways UID_RetailOrder_api_gateway UID_RetailOrderDiscount_api_gateway UID_RetailOrderPrice_api_gateway IAM Roles splunk_lambda_role_xxxxxxxxxx IAM Policies lambda_initiate_lambda_policy_xxxxxxxxxxx","title":"4.3 What Did We Deploy?"},{"location":"lambda/springboot-apm-II/","text":"Enable APM for Mobile Shop Springboot App (Cont.) \u00b6 3. Run a case and find both the Service Dashboard and your trace \u00b6 Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this: 3.1 Find your Service Dashboard for the Springboot app in Splunk APM \u00b6 Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Springboot App will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes. 3.1 Look at trace info in splunk APM \u00b6 Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application. Lets start enable APM in our first Lambda function to enrich this view!","title":"Enable APM for Mobile Shop Springboot App (Cont.)"},{"location":"lambda/springboot-apm-II/#enable-apm-for-mobile-shop-springboot-app-cont","text":"","title":"Enable APM for Mobile Shop Springboot App (Cont.)"},{"location":"lambda/springboot-apm-II/#3-run-a-case-and-find-both-the-service-dashboard-and-your-trace","text":"Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this:","title":"3. Run a case and find both the Service Dashboard and your trace"},{"location":"lambda/springboot-apm-II/#31-find-your-service-dashboard-for-the-springboot-app-in-splunk-apm","text":"Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Springboot App will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes.","title":"3.1 Find your Service Dashboard for the Springboot app in Splunk APM"},{"location":"lambda/springboot-apm-II/#31-look-at-trace-info-in-splunk-apm","text":"Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application. Lets start enable APM in our first Lambda function to enrich this view!","title":"3.1 Look at trace info in splunk APM"},{"location":"lambda/springboot-apm/","text":"Enable APM for Mobile Shop Springboot App \u00b6 1. Validate APM Environment \u00b6 The first activity we are going to do is validate that we have access to the Splunk APM environment and establish our starting point. To do this please login to Splunk Infrastructure & APM and select APM . This will bring you to the APM monitoring page, depending how many services you are currently monitoring with Splunk APM, you may or may not see a list of services. Once you have run the applications with APM enabled, you can filter the environment by entering your unique id you have been provided. This should then show just your APM environment. So lets start enabling APM in our environment. 2. Update the settings in the spring boot app \u00b6 To enable APM on the Spring boot application we need to update the FrameWork (pom.xml), update the application settings en add enable some lines in the code. 2.1 Update the FrameWork by updating POM.XML \u00b6 Connect back into your EC2 instance and stop the running Spring boot application by pressing Ctrl + C . Once the application is stopped, open an editor and edit the pom.xml file Shell Command nano pom.xml Scroll down until you find the following section and remove the comment marks lines (<-- & -->) by placing the cursor on the line with the remark and press Ctrl + K Afterwards the section will look like this: Make sure the lines are properly aligned and save the file by pressing Ctrl + O followed by Enter to write the file pom.xml to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line. 2.2 Update the application property file \u00b6 Edit the application property file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/resources/application.properties Remove the comment marks ## on the following 2 lines: ```java ##spring.sleuth.sampler.probability=1.0 ##spring.zipkin.baseUrl=http://localhost:9080 ``` The first line tells the springboot app to send 100% of the traces to Splunk APM and the second line directs the traces and spans to the local local OpenTelemetry Collector that will forward them to splunk APM. Your file should now look like this: application.properties File spring.thymeleaf.cache=false spring.thymeleaf.enabled=true spring.thymeleaf.prefix=classpath:/templates/ spring.thymeleaf.suffix=.html server.port=8080 spring.application.name=ACME-Mobile-Web-Shop-Base # For Sleuth 2.1+ use this property ## Enable this for full fidelity tracing spring.sleuth.sampler.probability=1.0 # The base url with the endpoint (/api/v2/spans) excluded # OpenTelemetry Collector deployed in your TKE namespace ## Enable this to send Traces to SignalFX spring.zipkin.baseUrl=http://localhost:9080 Now save the file by pressing Ctrl + O followed by enter to write the file applications.properties file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line. 2.3 Update the source code by enabling APM \u00b6 Open the main java file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/java/com/sfx/JavaLambda/JavaLambdaController.java Remove the comment marks /* & */ around the following three lines: /* import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; */ And remove the comment marks // in front of the following two lines: //@Autowired Tracer tracer; //@Autowired SpanCustomizer span; So that your JavaLambdaController.java file now looks like this (note only the first 33 lines are shown below for brevity): JavaLambdaController.java package com.sfx.JavaLambda; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import com.fasterxml.jackson.databind.ObjectMapper; import com.fasterxml.jackson.databind.ObjectReader; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.http.*; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.client.RestTemplate; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ModelAttribute; import org.springframework.web.bind.annotation.PostMapping; import java.util.*; import java.io.IOException; import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; @Controller public class JavaLambdaController { // set up AutoWired sleuth for APM @Autowired Tracer tracer; @Autowired SpanCustomizer span; Now save the file by pressing Ctrl + O followed by enter to write the file Java file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line. 2.4 Run the App \u00b6 Run the application by issuing the following command again: Shell Command mvn spring-boot:run When you have updated the files correctly you should see the SpringBoot logo again with no errors . We are now ready to test the app and send our first Trace. Info If you get errors: Please make sure that all the files are properly aligned and replace any leading spaces with Tabs Continue to the next section.","title":"Enable APM for Mobile Shop Springboot App"},{"location":"lambda/springboot-apm/#enable-apm-for-mobile-shop-springboot-app","text":"","title":"Enable APM for Mobile Shop Springboot App"},{"location":"lambda/springboot-apm/#1-validate-apm-environment","text":"The first activity we are going to do is validate that we have access to the Splunk APM environment and establish our starting point. To do this please login to Splunk Infrastructure & APM and select APM . This will bring you to the APM monitoring page, depending how many services you are currently monitoring with Splunk APM, you may or may not see a list of services. Once you have run the applications with APM enabled, you can filter the environment by entering your unique id you have been provided. This should then show just your APM environment. So lets start enabling APM in our environment.","title":"1. Validate APM Environment"},{"location":"lambda/springboot-apm/#2-update-the-settings-in-the-spring-boot-app","text":"To enable APM on the Spring boot application we need to update the FrameWork (pom.xml), update the application settings en add enable some lines in the code.","title":"2. Update the settings in the spring boot app"},{"location":"lambda/springboot-apm/#21-update-the-framework-by-updating-pomxml","text":"Connect back into your EC2 instance and stop the running Spring boot application by pressing Ctrl + C . Once the application is stopped, open an editor and edit the pom.xml file Shell Command nano pom.xml Scroll down until you find the following section and remove the comment marks lines (<-- & -->) by placing the cursor on the line with the remark and press Ctrl + K Afterwards the section will look like this: Make sure the lines are properly aligned and save the file by pressing Ctrl + O followed by Enter to write the file pom.xml to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line.","title":"2.1 Update the FrameWork by updating POM.XML"},{"location":"lambda/springboot-apm/#22-update-the-application-property-file","text":"Edit the application property file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/resources/application.properties Remove the comment marks ## on the following 2 lines: ```java ##spring.sleuth.sampler.probability=1.0 ##spring.zipkin.baseUrl=http://localhost:9080 ``` The first line tells the springboot app to send 100% of the traces to Splunk APM and the second line directs the traces and spans to the local local OpenTelemetry Collector that will forward them to splunk APM. Your file should now look like this: application.properties File spring.thymeleaf.cache=false spring.thymeleaf.enabled=true spring.thymeleaf.prefix=classpath:/templates/ spring.thymeleaf.suffix=.html server.port=8080 spring.application.name=ACME-Mobile-Web-Shop-Base # For Sleuth 2.1+ use this property ## Enable this for full fidelity tracing spring.sleuth.sampler.probability=1.0 # The base url with the endpoint (/api/v2/spans) excluded # OpenTelemetry Collector deployed in your TKE namespace ## Enable this to send Traces to SignalFX spring.zipkin.baseUrl=http://localhost:9080 Now save the file by pressing Ctrl + O followed by enter to write the file applications.properties file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line.","title":"2.2 Update the application property file"},{"location":"lambda/springboot-apm/#23-update-the-source-code-by-enabling-apm","text":"Open the main java file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/java/com/sfx/JavaLambda/JavaLambdaController.java Remove the comment marks /* & */ around the following three lines: /* import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; */ And remove the comment marks // in front of the following two lines: //@Autowired Tracer tracer; //@Autowired SpanCustomizer span; So that your JavaLambdaController.java file now looks like this (note only the first 33 lines are shown below for brevity): JavaLambdaController.java package com.sfx.JavaLambda; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import com.fasterxml.jackson.databind.ObjectMapper; import com.fasterxml.jackson.databind.ObjectReader; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.http.*; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.client.RestTemplate; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ModelAttribute; import org.springframework.web.bind.annotation.PostMapping; import java.util.*; import java.io.IOException; import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; @Controller public class JavaLambdaController { // set up AutoWired sleuth for APM @Autowired Tracer tracer; @Autowired SpanCustomizer span; Now save the file by pressing Ctrl + O followed by enter to write the file Java file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line.","title":"2.3 Update the source code by enabling APM"},{"location":"lambda/springboot-apm/#24-run-the-app","text":"Run the application by issuing the following command again: Shell Command mvn spring-boot:run When you have updated the files correctly you should see the SpringBoot logo again with no errors . We are now ready to test the app and send our first Trace. Info If you get errors: Please make sure that all the files are properly aligned and replace any leading spaces with Tabs Continue to the next section.","title":"2.4 Run the App"},{"location":"lambda/wip/","text":"This section is still under construction \u00b6","title":"This section is still under construction"},{"location":"lambda/wip/#this-section-is-still-under-construction","text":"","title":"This section is still under construction"},{"location":"module-support/vm/","text":"Lab Summary \u00b6 Deploy SignalFx Smart Agent via install script on a VM Confirm the Smart Agent is working and sending data Use multipass to create a vanilla Ubuntu VM and shell into it. You can also use a Linux-based VM with your cloud provider of choice. Replace [INITIALS] with your actual initials. Shell Command multipass launch [INITIALS]-vm multipass shell [INITIALs]-vm 1. Deploy SignalFx Smart Agent via install script on a VM \u00b6 You will need to obtain your Access Token from the Splunk UI. You can find your Access Token by clicking on your profile icon on the top right of the Splunk UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Later in the lab you can come back here and click the Copy button which will copy it to your clipboard so you can paste it when you need to provide an access token in the lab. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select 'My Profile'. The Ream can be found in the middle of the page within the Organizations section. In this example it is us1 . SignalFx maintains a shell script to install on supported distributions. Copy the script below and replace $REALM and $ACCESS_TOKEN with the values found in previous screen: Shell Command curl -sSL https://dl.signalfx.com/signalfx-agent.sh > /tmp/signalfx-agent.sh sudo sh /tmp/signalfx-agent.sh --realm $REALM -- $ACCESS_TOKEN Once the installation is complete check the status of the agent. Shell Command signalfx-agent status Output SignalFx Agent version: 5.1.2 Agent uptime: 4s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 16 Bad Monitor Config: None Global Dimensions: {host: as-k3s} GlobalSpanTags: map[] Datapoints sent (last minute): 0 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 0 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything Important Make a note of the value displayed for host in the Global Dimensions section of the output, as you need this later! 2. Confirm the Smart Agent is working and sending data \u00b6 To see the Metrics that the Smart Agent is sending to SignalFx, please goto the Splunk UI, and select Infrastructure \u2192 Hosts to see the lists of hosts. Here you see a list of the Nodes that have an Smart Agent installed and are reporting into SignalFx. Make sure you see your Multipass or AWS/EC2 instance in the list of hosts. (The hostname from the previous section) You can also set a filter for just your instance by selecting the host: attribute, followed by picking the name of your host from the drop down list. Click on the link to your host from the list, this wil take you to the overview page of your host. Make sure you have the SYSTEM METRIC tab selected. Here you can see various charts that relate to the health of your host, like CPU & Memory Used%, Disk I/O and many more. You can also see the list of services running on your host by selecting the PROCESSES tab. Take a moment to explore the various charts and the Processes list.","title":"Lab Summary"},{"location":"module-support/vm/#lab-summary","text":"Deploy SignalFx Smart Agent via install script on a VM Confirm the Smart Agent is working and sending data Use multipass to create a vanilla Ubuntu VM and shell into it. You can also use a Linux-based VM with your cloud provider of choice. Replace [INITIALS] with your actual initials. Shell Command multipass launch [INITIALS]-vm multipass shell [INITIALs]-vm","title":"Lab Summary"},{"location":"module-support/vm/#1-deploy-signalfx-smart-agent-via-install-script-on-a-vm","text":"You will need to obtain your Access Token from the Splunk UI. You can find your Access Token by clicking on your profile icon on the top right of the Splunk UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Later in the lab you can come back here and click the Copy button which will copy it to your clipboard so you can paste it when you need to provide an access token in the lab. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select 'My Profile'. The Ream can be found in the middle of the page within the Organizations section. In this example it is us1 . SignalFx maintains a shell script to install on supported distributions. Copy the script below and replace $REALM and $ACCESS_TOKEN with the values found in previous screen: Shell Command curl -sSL https://dl.signalfx.com/signalfx-agent.sh > /tmp/signalfx-agent.sh sudo sh /tmp/signalfx-agent.sh --realm $REALM -- $ACCESS_TOKEN Once the installation is complete check the status of the agent. Shell Command signalfx-agent status Output SignalFx Agent version: 5.1.2 Agent uptime: 4s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 16 Bad Monitor Config: None Global Dimensions: {host: as-k3s} GlobalSpanTags: map[] Datapoints sent (last minute): 0 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 0 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything Important Make a note of the value displayed for host in the Global Dimensions section of the output, as you need this later!","title":"1. Deploy SignalFx Smart Agent via install script on a VM"},{"location":"module-support/vm/#2-confirm-the-smart-agent-is-working-and-sending-data","text":"To see the Metrics that the Smart Agent is sending to SignalFx, please goto the Splunk UI, and select Infrastructure \u2192 Hosts to see the lists of hosts. Here you see a list of the Nodes that have an Smart Agent installed and are reporting into SignalFx. Make sure you see your Multipass or AWS/EC2 instance in the list of hosts. (The hostname from the previous section) You can also set a filter for just your instance by selecting the host: attribute, followed by picking the name of your host from the drop down list. Click on the link to your host from the list, this wil take you to the overview page of your host. Make sure you have the SYSTEM METRIC tab selected. Here you can see various charts that relate to the health of your host, like CPU & Memory Used%, Disk I/O and many more. You can also see the list of services running on your host by selecting the PROCESSES tab. Take a moment to explore the various charts and the Processes list.","title":"2. Confirm the Smart Agent is working and sending data"},{"location":"monitoring-as-code/","text":"Monitoring as Code - Lab Summary \u00b6 Use Terraform 1 to manage Observability Cloud Dashboards and Detectors Initialize the Terraform Splunk Provider 2 . Run Terraform to create detectors and dashboards from code using the Splunk Terraform Provider. See how Terraform can also delete detectors and dashboards. 1. Initial setup \u00b6 Monitoring as code adopts the same approach as infrastructure as code. You can manage monitoring the same way you do applications, servers, or other infrastructure components. You can monitoring as code to build out your visualisations, what to monitor, and when to alert, among other things. This means your monitoring setup, processes, and rules can be versioned, shared, and reused. Full documentation for the Splunk Terraform Provider is available here . Remaining in your AWS/EC2 instance, change into the signalfx-jumpstart directory Shell Command cd ~/signalfx-jumpstart The environment variables needed should already be set from Installation using Helm . If not, create the following environment variables to use in the Terraform steps below Shell Command export ACCESS_TOKEN=<replace_with_default_access_token> export REALM=<replace_with_splunk_realm> Initialize Terraform and upgrade to the latest version of the Splunk Terraform Provider Upgrading the SignalFx Terraform Provider You will need to run the command below each time a new version of the Splunk Terraform Provider is released. You can track the releases on GitHub . Shell Command terraform init -upgrade Output Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kafka in modules/kafka - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Finding latest version of splunk-terraform/signalfx... - Installing splunk-terraform/signalfx v6.7.3... - Installed splunk-terraform/signalfx v6.7.3 (signed by a HashiCorp partner, key ID 8B5755E223754FC9) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. 2. Create execution plan \u00b6 The terraform plan command creates an execution plan. By default, creating a plan consists of: Reading the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date. Comparing the current configuration to the prior state and noting any differences. Proposing a set of change actions that should, if applied, make the remote objects match the configuration. The plan command alone will not actually carry out the proposed changes, and so you can use this command to check whether the proposed changes match what you expected before you apply the changes Shell Command terraform plan -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Example Output Plan: 92 to add, 0 to change, 0 to destroy. If the plan executes successfully, we can go ahead and apply: 3. Apply execution plan \u00b6 The terraform apply command executes the actions proposed in the Terraform plan above. The most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to provide the Access Token, Realm (the prefix defaults to Splunk ) and approve the plan, before taking the indicated actions. Due to this being a workshop it is required that the prefix is to be unique so you need to run the terraform apply below. Shell Command terraform apply -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Example Output Apply complete! Resources: 92 added, 0 changed, 0 destroyed. Once the apply has completed, validate that the detectors were created, under the Alerts \u2192 Detectors . They will be prefixed by the hostname of your instance. To check the prefix value run: Shell Command echo $(hostname) You will see a list of the new detectors and you can search for the prefix that was output from above. 3. Destroy all your hard work \u00b6 The terraform destroy command is a convenient way to destroy all remote objects managed by your Terraform configuration. While you will typically not want to destroy long-lived objects in a production environment, Terraform is sometimes used to manage ephemeral infrastructure for development purposes, in which case you can use terraform destroy to conveniently clean up all of those temporary objects once you are finished with your work. Now go and destroy all the Detectors and Dashboards that were previously applied! Shell Command terraform destroy -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" Example Output Destroy complete! Resources: 92 destroyed. Validate all the detectors have been removed by navigating to Alerts \u2192 Detectors Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. \u21a9 A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. Splunk, Terraform Cloud, DNSimple, Cloudflare). \u21a9","title":"Plan, Apply and Destroy"},{"location":"monitoring-as-code/#monitoring-as-code-lab-summary","text":"Use Terraform 1 to manage Observability Cloud Dashboards and Detectors Initialize the Terraform Splunk Provider 2 . Run Terraform to create detectors and dashboards from code using the Splunk Terraform Provider. See how Terraform can also delete detectors and dashboards.","title":"Monitoring as Code - Lab Summary"},{"location":"monitoring-as-code/#1-initial-setup","text":"Monitoring as code adopts the same approach as infrastructure as code. You can manage monitoring the same way you do applications, servers, or other infrastructure components. You can monitoring as code to build out your visualisations, what to monitor, and when to alert, among other things. This means your monitoring setup, processes, and rules can be versioned, shared, and reused. Full documentation for the Splunk Terraform Provider is available here . Remaining in your AWS/EC2 instance, change into the signalfx-jumpstart directory Shell Command cd ~/signalfx-jumpstart The environment variables needed should already be set from Installation using Helm . If not, create the following environment variables to use in the Terraform steps below Shell Command export ACCESS_TOKEN=<replace_with_default_access_token> export REALM=<replace_with_splunk_realm> Initialize Terraform and upgrade to the latest version of the Splunk Terraform Provider Upgrading the SignalFx Terraform Provider You will need to run the command below each time a new version of the Splunk Terraform Provider is released. You can track the releases on GitHub . Shell Command terraform init -upgrade Output Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kafka in modules/kafka - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Finding latest version of splunk-terraform/signalfx... - Installing splunk-terraform/signalfx v6.7.3... - Installed splunk-terraform/signalfx v6.7.3 (signed by a HashiCorp partner, key ID 8B5755E223754FC9) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"1. Initial setup"},{"location":"monitoring-as-code/#2-create-execution-plan","text":"The terraform plan command creates an execution plan. By default, creating a plan consists of: Reading the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date. Comparing the current configuration to the prior state and noting any differences. Proposing a set of change actions that should, if applied, make the remote objects match the configuration. The plan command alone will not actually carry out the proposed changes, and so you can use this command to check whether the proposed changes match what you expected before you apply the changes Shell Command terraform plan -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Example Output Plan: 92 to add, 0 to change, 0 to destroy. If the plan executes successfully, we can go ahead and apply:","title":"2. Create execution plan"},{"location":"monitoring-as-code/#3-apply-execution-plan","text":"The terraform apply command executes the actions proposed in the Terraform plan above. The most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to provide the Access Token, Realm (the prefix defaults to Splunk ) and approve the plan, before taking the indicated actions. Due to this being a workshop it is required that the prefix is to be unique so you need to run the terraform apply below. Shell Command terraform apply -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Example Output Apply complete! Resources: 92 added, 0 changed, 0 destroyed. Once the apply has completed, validate that the detectors were created, under the Alerts \u2192 Detectors . They will be prefixed by the hostname of your instance. To check the prefix value run: Shell Command echo $(hostname) You will see a list of the new detectors and you can search for the prefix that was output from above.","title":"3. Apply execution plan"},{"location":"monitoring-as-code/#3-destroy-all-your-hard-work","text":"The terraform destroy command is a convenient way to destroy all remote objects managed by your Terraform configuration. While you will typically not want to destroy long-lived objects in a production environment, Terraform is sometimes used to manage ephemeral infrastructure for development purposes, in which case you can use terraform destroy to conveniently clean up all of those temporary objects once you are finished with your work. Now go and destroy all the Detectors and Dashboards that were previously applied! Shell Command terraform destroy -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" Example Output Destroy complete! Resources: 92 destroyed. Validate all the detectors have been removed by navigating to Alerts \u2192 Detectors Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. \u21a9 A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. Splunk, Terraform Cloud, DNSimple, Cloudflare). \u21a9","title":"3. Destroy all your hard work"},{"location":"oncall/getting_started/","text":"Initial Setup \u00b6 Aim \u00b6 This module is simply to ensure you have access to the Splunk On-Call UI (formerly known as VictorOps), Splunk Infrastructure Monitoring UI (formerly known as SignalFx) and the EC2 Instance which has been allocated to you. Once you have access to each platform, keep them open for the duration of the workshop as you will be switching between them and the workshop instructions. 1. Activate your Splunk On-Call Login \u00b6 You should have received an invitation to Activate your Splunk On-Call account via e-mail, if you have not already done so, click the Activate Account link and follow the prompts. If you did not receive an invitation it is probably because you already have a Splunk On-Call login, linked to a different organisation. If so login to to that Org, then use the organisation dropdown next to your username in the top left to switch to the Observability Workshop Org. Note If you do not see the Organisation dropdown menu item next to your name with Observability Workshop EMEA that is OK, it simply means you only have access to a single Org so that menu is not visible to you. If you have forgotten your password go to the sign-in page and use the forgotten password link to reset your password. 2. Activate your Splunk Infrastructure Monitoring Login \u00b6 You should have received an invitation to join the Splunk Infrastructure Monitoring - Observability Workshop. If you have not already done so click the JOIN NOW button and follow the prompts to set a password and activate your login. 3. Access your EC2 Instance \u00b6 Splunk has provided you with a dedicated EC2 Instance which you can use during this workshop for triggering Incidents the same way the instructor did during the introductory demo. This VM has Splunk Infrastructure Monitoring deployed and has an associated Detector configured. The Detector will pass Alerts to Splunk On-Call which will then create Incidents and page the on-call user. The welcome e-mail you received providing you all the details for this Workshop contain the instructions for accessing your allocated EC2 Instance. SSH (Mac OS/Linux) \u00b6 Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in your welcome e-mail). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the welcome e-mail. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue with the workshop when instructed to do so by the instructor Putty (Windows users only) \u00b6 If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find the downloads here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and in the Host Name (or IP address) field enter the IP address provided in the welcome e-mail. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your EC2 instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu using the password provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue with the workshop when instructed to do so by the instructor Web Browser (All) \u00b6 If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the welcome e-mail). Once connected, login in as ubuntu and the password is the one provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below: Copy & Paste in browser \u00b6 Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop asks you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue with the workshop when instructed to do so by the instructor Download Putty \u21a9","title":"Initial Setup"},{"location":"oncall/getting_started/#initial-setup","text":"","title":"Initial Setup"},{"location":"oncall/getting_started/#aim","text":"This module is simply to ensure you have access to the Splunk On-Call UI (formerly known as VictorOps), Splunk Infrastructure Monitoring UI (formerly known as SignalFx) and the EC2 Instance which has been allocated to you. Once you have access to each platform, keep them open for the duration of the workshop as you will be switching between them and the workshop instructions.","title":"Aim"},{"location":"oncall/getting_started/#1-activate-your-splunk-on-call-login","text":"You should have received an invitation to Activate your Splunk On-Call account via e-mail, if you have not already done so, click the Activate Account link and follow the prompts. If you did not receive an invitation it is probably because you already have a Splunk On-Call login, linked to a different organisation. If so login to to that Org, then use the organisation dropdown next to your username in the top left to switch to the Observability Workshop Org. Note If you do not see the Organisation dropdown menu item next to your name with Observability Workshop EMEA that is OK, it simply means you only have access to a single Org so that menu is not visible to you. If you have forgotten your password go to the sign-in page and use the forgotten password link to reset your password.","title":"1. Activate your Splunk On-Call Login"},{"location":"oncall/getting_started/#2-activate-your-splunk-infrastructure-monitoring-login","text":"You should have received an invitation to join the Splunk Infrastructure Monitoring - Observability Workshop. If you have not already done so click the JOIN NOW button and follow the prompts to set a password and activate your login.","title":"2. Activate your Splunk Infrastructure Monitoring Login"},{"location":"oncall/getting_started/#3-access-your-ec2-instance","text":"Splunk has provided you with a dedicated EC2 Instance which you can use during this workshop for triggering Incidents the same way the instructor did during the introductory demo. This VM has Splunk Infrastructure Monitoring deployed and has an associated Detector configured. The Detector will pass Alerts to Splunk On-Call which will then create Incidents and page the on-call user. The welcome e-mail you received providing you all the details for this Workshop contain the instructions for accessing your allocated EC2 Instance.","title":"3. Access your EC2 Instance"},{"location":"oncall/getting_started/#ssh-mac-oslinux","text":"Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in your welcome e-mail). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the welcome e-mail. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue with the workshop when instructed to do so by the instructor","title":"SSH (Mac OS/Linux)"},{"location":"oncall/getting_started/#putty-windows-users-only","text":"If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find the downloads here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and in the Host Name (or IP address) field enter the IP address provided in the welcome e-mail. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your EC2 instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu using the password provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue with the workshop when instructed to do so by the instructor","title":"Putty (Windows users only)"},{"location":"oncall/getting_started/#web-browser-all","text":"If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the welcome e-mail). Once connected, login in as ubuntu and the password is the one provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below:","title":"Web Browser (All)"},{"location":"oncall/getting_started/#copy-paste-in-browser","text":"Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop asks you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue with the workshop when instructed to do so by the instructor Download Putty \u21a9","title":"Copy &amp; Paste in browser"},{"location":"oncall/getting_started/escalation/","text":"Configure Escalation Policies \u00b6 Aim \u00b6 Escalation policies determine who is actually on-call for a given team and are the link to utilizing any rotations that have been created. The aim of this module is for you to create three different Escalation Policies to demonstrate a number of different features and operating models. The instructor will start by explaining the concepts before you proceed with the configuration. Navigate to the Escalation Polices tab on the Teams sub menu, you should have no existing Polices so we need to create some. We are going to create the following Polices to cover off three typical use cases. 1. 24/7 Policy \u00b6 Click Add Escalation Policy Policy Name: 24/7 Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Senior SRE Escalation Click Save 2. Primary Policy \u00b6 Click Add Escalation Policy Policy Name: Primary Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Follow the Sun Support - Business Hours Click Add Step Step 2 If still unacked after 15 minutes Notify the next user(s) in the current on-duty shift \u2192 Follow the Sun Support - Business Hours Click Add Step Step 3 If still unacked after 15 more minutes Execute Policy \u2192 [Your Team Name] : 24/7 Click Save 3. Waiting Room Policy \u00b6 Click Add Escalation Policy Policy Name: Waiting Room Step 1 If still unacked after 10 more minutes Execute Policy \u2192 [Your Team Name] : Primary Click Save You should now have the following three escalation polices: You may have noticed that when we created each policy there was the following warning message: Warning There are no routing keys for this policy - it will only receive incidents via manual reroute or when on another escalation policy This is because there are no Routing Keys linked to these Escalation Polices, so now that we have these polices configured we can create the Routing Keys and link them to our Polices.. Continue and also complete the Creating Routing Keys module.","title":"Configure Escalation Policies"},{"location":"oncall/getting_started/escalation/#configure-escalation-policies","text":"","title":"Configure Escalation Policies"},{"location":"oncall/getting_started/escalation/#aim","text":"Escalation policies determine who is actually on-call for a given team and are the link to utilizing any rotations that have been created. The aim of this module is for you to create three different Escalation Policies to demonstrate a number of different features and operating models. The instructor will start by explaining the concepts before you proceed with the configuration. Navigate to the Escalation Polices tab on the Teams sub menu, you should have no existing Polices so we need to create some. We are going to create the following Polices to cover off three typical use cases.","title":"Aim"},{"location":"oncall/getting_started/escalation/#1-247-policy","text":"Click Add Escalation Policy Policy Name: 24/7 Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Senior SRE Escalation Click Save","title":"1. 24/7 Policy"},{"location":"oncall/getting_started/escalation/#2-primary-policy","text":"Click Add Escalation Policy Policy Name: Primary Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Follow the Sun Support - Business Hours Click Add Step Step 2 If still unacked after 15 minutes Notify the next user(s) in the current on-duty shift \u2192 Follow the Sun Support - Business Hours Click Add Step Step 3 If still unacked after 15 more minutes Execute Policy \u2192 [Your Team Name] : 24/7 Click Save","title":"2. Primary Policy"},{"location":"oncall/getting_started/escalation/#3-waiting-room-policy","text":"Click Add Escalation Policy Policy Name: Waiting Room Step 1 If still unacked after 10 more minutes Execute Policy \u2192 [Your Team Name] : Primary Click Save You should now have the following three escalation polices: You may have noticed that when we created each policy there was the following warning message: Warning There are no routing keys for this policy - it will only receive incidents via manual reroute or when on another escalation policy This is because there are no Routing Keys linked to these Escalation Polices, so now that we have these polices configured we can create the Routing Keys and link them to our Polices.. Continue and also complete the Creating Routing Keys module.","title":"3. Waiting Room Policy"},{"location":"oncall/getting_started/rotations/","text":"Configure Rotations \u00b6 Aim \u00b6 A rotation is a recurring schedule, that consists of one or more shifts, with members who rotate through a shift. The aim of this module is for you to configure two example Rotations, and assign Team Members to the Rotations. Navigate to the Rotations tab on the Teams sub menu, you should have no existing Rotations so we need to create some. The 1st Rotation you will create is for a follow the sun support pattern where the members of each shift provide cover during their normal working hours within their time zone. The 2nd will be a Rotation used to provide escalation support by more experienced senior members of the team, based on a 24/7, 1 week shift pattern. 1. Follow the Sun Support - Business Hours \u00b6 Click Add Rotation Enter a name of \" Follow the Sun Support - Business Hours \" and Select Partial day from the three available shift templates. Enter a Shift name of \" Asia \" Time Zone set to \" Asia/Tokyo \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Rotation You will now be prompted to add Members to this shift; add the Asia members who are jimhalpert, lydia and marie , but only if you're using the Splunk provided environment for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add an 2nd shift for Europe by again clicking +Add a shift \u2192 Partial Day Enter a Shift name of \" Europe \" Time Zone set to \" Europe/London \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the Europe members who are duanechow, gomez and heisenberg , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add a 3rd shift for West Coast USA by again clicking +Add a shift - Partial Day Enter a Shift name of \" West Coast \" Time Zone set to \" US/Pacific \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the West Coast members who are maximo, michaelscott and tuco , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. The first user added will be the 'current' user for that shift. You can re-order the shifts by simply dragging the users up and down, and you can change the current user by clicking Set Current on an alternate user You will now have three different Shift patterns, that provide cover 24hr hours, Mon - Fri, but with no cover at weekends. We will now add another Rotation for our Senior SRE Escalation cover. 2. Senior SRE Escalation \u00b6 Click Add Rotation Enter a name of \" Senior SRE Escalation \" Select 24/7 from the three available shift templates Enter a Shift name of \" Senior SRE Escalation \" Time Zone set to \" Asia/Tokyo \" Handoff happens every \" 7 days at 9.00am \" The next handoff happens [select the next Monday from the date picker] Click Save Rotation You will again be prompted to add Members to this shift; add the 24/7 members who are jackwelker, hank and pambeesly , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Please wait for the instructor before proceeding to the Configuring Escalation Policies module.","title":"Configure Rotations"},{"location":"oncall/getting_started/rotations/#configure-rotations","text":"","title":"Configure Rotations"},{"location":"oncall/getting_started/rotations/#aim","text":"A rotation is a recurring schedule, that consists of one or more shifts, with members who rotate through a shift. The aim of this module is for you to configure two example Rotations, and assign Team Members to the Rotations. Navigate to the Rotations tab on the Teams sub menu, you should have no existing Rotations so we need to create some. The 1st Rotation you will create is for a follow the sun support pattern where the members of each shift provide cover during their normal working hours within their time zone. The 2nd will be a Rotation used to provide escalation support by more experienced senior members of the team, based on a 24/7, 1 week shift pattern.","title":"Aim"},{"location":"oncall/getting_started/rotations/#1-follow-the-sun-support-business-hours","text":"Click Add Rotation Enter a name of \" Follow the Sun Support - Business Hours \" and Select Partial day from the three available shift templates. Enter a Shift name of \" Asia \" Time Zone set to \" Asia/Tokyo \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Rotation You will now be prompted to add Members to this shift; add the Asia members who are jimhalpert, lydia and marie , but only if you're using the Splunk provided environment for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add an 2nd shift for Europe by again clicking +Add a shift \u2192 Partial Day Enter a Shift name of \" Europe \" Time Zone set to \" Europe/London \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the Europe members who are duanechow, gomez and heisenberg , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add a 3rd shift for West Coast USA by again clicking +Add a shift - Partial Day Enter a Shift name of \" West Coast \" Time Zone set to \" US/Pacific \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the West Coast members who are maximo, michaelscott and tuco , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. The first user added will be the 'current' user for that shift. You can re-order the shifts by simply dragging the users up and down, and you can change the current user by clicking Set Current on an alternate user You will now have three different Shift patterns, that provide cover 24hr hours, Mon - Fri, but with no cover at weekends. We will now add another Rotation for our Senior SRE Escalation cover.","title":"1. Follow the Sun Support - Business Hours"},{"location":"oncall/getting_started/rotations/#2-senior-sre-escalation","text":"Click Add Rotation Enter a name of \" Senior SRE Escalation \" Select 24/7 from the three available shift templates Enter a Shift name of \" Senior SRE Escalation \" Time Zone set to \" Asia/Tokyo \" Handoff happens every \" 7 days at 9.00am \" The next handoff happens [select the next Monday from the date picker] Click Save Rotation You will again be prompted to add Members to this shift; add the 24/7 members who are jackwelker, hank and pambeesly , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Please wait for the instructor before proceeding to the Configuring Escalation Policies module.","title":"2. Senior SRE Escalation"},{"location":"oncall/getting_started/routing/","text":"Create Routing Keys \u00b6 Aim \u00b6 Routing Keys map the incoming alert messages from your monitoring system to an Escalation Policy which in turn sends the notifications to the appropriate team. Note that routing keys are case insensitive and should only be composed of letters, numbers, hyphens, and underscores. The aim of this module is for you to create some routing keys and then link them to your Escalation Policies you have created in the previous exercise. 1. Instance ID \u00b6 Each participant requires a unique Routing Key so we use the Hostname of the EC2 Instance you were allocated. We are only doing this to ensure your Routing Key is unique and we know all Hostnames are unique. In a production deployment the Routing Key would typically reflect the name of a System or Service being monitored, or a Team such as 1st Line Support etc. Your welcome e-mail informed you of the details of your EC2 Instance that has been provided for you to use during this workshop and you should have logged into this as part of the 1st exercise. The e-mail also contained the Hostname of the Instance, but you can also obtain it from the Instance directly. To get your Hostname from within the shell session connected to your Instance run the following command: Shell Command echo ${HOSTNAME} Example Output zevn It is very important that when creating the Routing Keys you use the 4 letter hostname allocated to you as a Detector has been configured within Splunk Infrastructure Monitoring using this hostname, so any deviation will cause future exercises to fail. 2 Create Routing Keys \u00b6 Navigate to Settings on the main menu bar, you should now be at the Routing Keys page. You are going to create the following two Routing Keys using the naming conventions listed in the following table, but replacing HOSTNAME with the value from above and replace TEAM_NAME with the team you were allocated or created earlier. Routing Key Escalation Policies HOSTNAME _PRI TEAM_NAME : Primary HOSTNAME _WR TEAM_NAME : Waiting Room There will probably already be a number of Routing Keys configured, but to add a new one simply scroll to the bottom of the page and then click Add Key In the left hand box, enter the name for the key as per the table above. In the Routing Key column, select your Teams Primary policy from the drop down in the Escalation Polices column. You can start typing your Team Name to filter the results. Note If there are a large number of participants on the workshop, resulting in an unusually large number of Escalation Policies sometimes the search filter does not list all the Policies under your Team Name. If this happens instead of using the search feature, simply scroll down to your team name, all the policies will then be listed. Repeat the above steps for both Keys, xxxx_PRI and xxxx_WR , mapping them to your Teams Primary and Waiting Room policies. You should now have two Routing Keys configured, similar to the following: Tip You can assign a Routing Key to multiple Escalation Policies if required by simply selecting more from the list If you now navigate back to Teams \u2192 [Your Team Name] \u2192 Escalation Policies and look at the settings for your Primary and Waiting Room polices you will see that these now have Routes assigned to them. The 24/7 policy does not have a Route assigned as this will only be triggered via an Execute Policy escalation from the Primary policy. Please wait for the instructor before proceeding to the Incident Lifecycle/Overview module.","title":"Create Routing Keys"},{"location":"oncall/getting_started/routing/#create-routing-keys","text":"","title":"Create Routing Keys"},{"location":"oncall/getting_started/routing/#aim","text":"Routing Keys map the incoming alert messages from your monitoring system to an Escalation Policy which in turn sends the notifications to the appropriate team. Note that routing keys are case insensitive and should only be composed of letters, numbers, hyphens, and underscores. The aim of this module is for you to create some routing keys and then link them to your Escalation Policies you have created in the previous exercise.","title":"Aim"},{"location":"oncall/getting_started/routing/#1-instance-id","text":"Each participant requires a unique Routing Key so we use the Hostname of the EC2 Instance you were allocated. We are only doing this to ensure your Routing Key is unique and we know all Hostnames are unique. In a production deployment the Routing Key would typically reflect the name of a System or Service being monitored, or a Team such as 1st Line Support etc. Your welcome e-mail informed you of the details of your EC2 Instance that has been provided for you to use during this workshop and you should have logged into this as part of the 1st exercise. The e-mail also contained the Hostname of the Instance, but you can also obtain it from the Instance directly. To get your Hostname from within the shell session connected to your Instance run the following command: Shell Command echo ${HOSTNAME} Example Output zevn It is very important that when creating the Routing Keys you use the 4 letter hostname allocated to you as a Detector has been configured within Splunk Infrastructure Monitoring using this hostname, so any deviation will cause future exercises to fail.","title":"1. Instance ID"},{"location":"oncall/getting_started/routing/#2-create-routing-keys","text":"Navigate to Settings on the main menu bar, you should now be at the Routing Keys page. You are going to create the following two Routing Keys using the naming conventions listed in the following table, but replacing HOSTNAME with the value from above and replace TEAM_NAME with the team you were allocated or created earlier. Routing Key Escalation Policies HOSTNAME _PRI TEAM_NAME : Primary HOSTNAME _WR TEAM_NAME : Waiting Room There will probably already be a number of Routing Keys configured, but to add a new one simply scroll to the bottom of the page and then click Add Key In the left hand box, enter the name for the key as per the table above. In the Routing Key column, select your Teams Primary policy from the drop down in the Escalation Polices column. You can start typing your Team Name to filter the results. Note If there are a large number of participants on the workshop, resulting in an unusually large number of Escalation Policies sometimes the search filter does not list all the Policies under your Team Name. If this happens instead of using the search feature, simply scroll down to your team name, all the policies will then be listed. Repeat the above steps for both Keys, xxxx_PRI and xxxx_WR , mapping them to your Teams Primary and Waiting Room policies. You should now have two Routing Keys configured, similar to the following: Tip You can assign a Routing Key to multiple Escalation Policies if required by simply selecting more from the list If you now navigate back to Teams \u2192 [Your Team Name] \u2192 Escalation Policies and look at the settings for your Primary and Waiting Room polices you will see that these now have Routes assigned to them. The 24/7 policy does not have a Route assigned as this will only be triggered via an Execute Policy escalation from the Primary policy. Please wait for the instructor before proceeding to the Incident Lifecycle/Overview module.","title":"2 Create Routing Keys"},{"location":"oncall/getting_started/team/","text":"Teams \u00b6 Aim \u00b6 The aim of this module is for you to complete the first step of Team configuration by adding users to your Team. 1. Find your Team \u00b6 Navigate to the Teams tab on the main toolbar, you should find you that a Team has been created for you as part of the workshop pre-setup and you would have been informed of your Team Name via e-mail. If you have found your pre-configured Team, skip Step 2. and proceed to Step 3. Configure Your Team However if you cannot find you allocated Team, you will need to create a new one, so proceed with Step 2. Create Team 2. Create Team \u00b6 Only complete this step if you cannot find your pre-allocated Team as detailed in your workshop e-mail. Select Add Team , then enter your allocated team name, this will typically be in the format of \"AttendeeID Workshop\" and then save by clicking the Add Team button. 3. Configure Your Team \u00b6 You now need to add other users to your team. If you are running this workshop using the Splunk provided environment, the following accounts are available for testing. If you are running this lab in your own environment, you will have been provided a list of usernames you can use in place of the table below. These users are dummy accounts who will not receive notifications when they are on call. Name Username Shift Duane Chow duanechow Europe Steven Gomez gomez Europe Walter White heisenberg Europe Jim Halpert jimhalpert Asia Lydia Rodarte-Quayle lydia Asia Marie Schrader marie Asia Maximo Arciniega maximo West Coast Michael Scott michaelscott West Coast Tuco Salamanca tuco West Coast Jack Welker jackwelker 24/7 Hank Schrader hank 24/7 Pam Beesly pambeesly 24/7 Add the users to your team, using either the above list or the alternate one provided to you. The value in the Shift column can be ignored for now, but will be required for a later step. Click the Invite User button on the right hand side, then either start typing the usernames (this will filter the list), or copy and paste them into the dialogue box. Once all users are added to the list click the Add User button. To make a team member a Team Admin, simply click the icon in the right hand column, pick any user and make them an Admin. Tip For large team management you can use the APIs to streamline this process Continue and also complete the Configure Rotations module.","title":"Teams"},{"location":"oncall/getting_started/team/#teams","text":"","title":"Teams"},{"location":"oncall/getting_started/team/#aim","text":"The aim of this module is for you to complete the first step of Team configuration by adding users to your Team.","title":"Aim"},{"location":"oncall/getting_started/team/#1-find-your-team","text":"Navigate to the Teams tab on the main toolbar, you should find you that a Team has been created for you as part of the workshop pre-setup and you would have been informed of your Team Name via e-mail. If you have found your pre-configured Team, skip Step 2. and proceed to Step 3. Configure Your Team However if you cannot find you allocated Team, you will need to create a new one, so proceed with Step 2. Create Team","title":"1. Find your Team"},{"location":"oncall/getting_started/team/#2-create-team","text":"Only complete this step if you cannot find your pre-allocated Team as detailed in your workshop e-mail. Select Add Team , then enter your allocated team name, this will typically be in the format of \"AttendeeID Workshop\" and then save by clicking the Add Team button.","title":"2. Create Team"},{"location":"oncall/getting_started/team/#3-configure-your-team","text":"You now need to add other users to your team. If you are running this workshop using the Splunk provided environment, the following accounts are available for testing. If you are running this lab in your own environment, you will have been provided a list of usernames you can use in place of the table below. These users are dummy accounts who will not receive notifications when they are on call. Name Username Shift Duane Chow duanechow Europe Steven Gomez gomez Europe Walter White heisenberg Europe Jim Halpert jimhalpert Asia Lydia Rodarte-Quayle lydia Asia Marie Schrader marie Asia Maximo Arciniega maximo West Coast Michael Scott michaelscott West Coast Tuco Salamanca tuco West Coast Jack Welker jackwelker 24/7 Hank Schrader hank 24/7 Pam Beesly pambeesly 24/7 Add the users to your team, using either the above list or the alternate one provided to you. The value in the Shift column can be ignored for now, but will be required for a later step. Click the Invite User button on the right hand side, then either start typing the usernames (this will filter the list), or copy and paste them into the dialogue box. Once all users are added to the list click the Add User button. To make a team member a Team Admin, simply click the icon in the right hand column, pick any user and make them an Admin. Tip For large team management you can use the APIs to streamline this process Continue and also complete the Configure Rotations module.","title":"3. Configure Your Team"},{"location":"oncall/getting_started/user_profile/","text":"User Profile \u00b6 Aim \u00b6 The aim of this module is for you to configure your personal profile which controls how you will be notified by Splunk On-Call whenever you get paged. 1. Contact Methods \u00b6 Switch to the Splunk On-Call UI and click on your login name in the top right hand corner and chose Profile from the drop down. Confirm your contact methods are listed correctly and add any additional phone numbers and e-mail address you wish to use. 2. Mobile Devices \u00b6 To install the Splunk On-Call app for your smartphone search your phones App Store for Splunk On-Call to find the appropriate version of the app. The publisher should be listed as VictorOps Inc. Configuration help guides are available: Apple Android Install the App and login, then refresh the Profile page and your device should now be listed under the devices section. Click the Test push notification button and confirm you receive the test message. 3. Personal Calendar \u00b6 This link will enable you to sync your on-call schedule with your calendar, however as you do not have any allocated shifts yet this will currently be empty. You can add it to your calendar by copying the link into your preferred application and setting it up as a new subscription. 4. Paging Policies \u00b6 Paging Polices specify how you will be contacted when on-call. The Primary Paging Policy will have defaulted to sending you an SMS assuming you added your phone number when activating your account. We will now configure this policy into a three tier multi-stage policy similar to the image below. Step 1: Send a push notification \u00b6 Click the edit policy button in the top right corner for the Primary Paging Policy. Send a push notification to all my devices Execute the next step if I have not responded within 5 minutes Click Add a Step Step 2: Send an e-mail \u00b6 Send an e-mail to [your email address] Execute the next step if I have not responded within 5 minutes Click Add a Step Step 3: Call your number \u00b6 Every 5 minutes until we have reached you Make a phone call to [your phone number] Click Save to save the policy. When you are on-call or in the escalation path of an incident, you will receive notifications in this order following these time delays. To cease the paging you must acknowledge the incident. Acknowledgements can occur in one of the following ways: Expanding the Push Notification on your device and selecting Acknowledge Responding to the SMS with the 5 digit code included Pressing 4 during the Phone Call Slack Button For more information on Notification Types, see here . 5. Custom Paging Policies \u00b6 Custom paging polices enable you to override the primary policy based on the time and day of the week. A good example would be get the system to immediately phone you whenever you get a page during the evening or weekends as this is more likely to get your attention than a push notification. Create a new Custom Policy by clicking Add a Policy and configure with the following settings: 5.1 Custom evening policy \u00b6 Policy Name: Evening Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: All 7 Days Timezone Between 7pm and 9am Click Save to save the policy then add one more. 5.2 Custom weekend policy \u00b6 Policy Name: Weekend Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: Sat & Sun Timezone Between 9am and 7pm Click Save to save the policy. These custom paging policies will be used during the specified times in place of the Primary Policy. However admins do have the ability to ignore these custom policies, and we will highlight how this is achieved in a later module. The final option here is the setting for Recovery Notifications. As these are typically low priority simply sending you an email or a push notification are the typical settings used. Your profile is now fully configured using these example configurations. Organizations will have different views on how profiles should be configured and will typically issue guidelines for paging policies and times between escalations etc. Please wait for the instructor before proceeding to the Teams module.","title":"User Profile"},{"location":"oncall/getting_started/user_profile/#user-profile","text":"","title":"User Profile"},{"location":"oncall/getting_started/user_profile/#aim","text":"The aim of this module is for you to configure your personal profile which controls how you will be notified by Splunk On-Call whenever you get paged.","title":"Aim"},{"location":"oncall/getting_started/user_profile/#1-contact-methods","text":"Switch to the Splunk On-Call UI and click on your login name in the top right hand corner and chose Profile from the drop down. Confirm your contact methods are listed correctly and add any additional phone numbers and e-mail address you wish to use.","title":"1. Contact Methods"},{"location":"oncall/getting_started/user_profile/#2-mobile-devices","text":"To install the Splunk On-Call app for your smartphone search your phones App Store for Splunk On-Call to find the appropriate version of the app. The publisher should be listed as VictorOps Inc. Configuration help guides are available: Apple Android Install the App and login, then refresh the Profile page and your device should now be listed under the devices section. Click the Test push notification button and confirm you receive the test message.","title":"2. Mobile Devices"},{"location":"oncall/getting_started/user_profile/#3-personal-calendar","text":"This link will enable you to sync your on-call schedule with your calendar, however as you do not have any allocated shifts yet this will currently be empty. You can add it to your calendar by copying the link into your preferred application and setting it up as a new subscription.","title":"3. Personal Calendar"},{"location":"oncall/getting_started/user_profile/#4-paging-policies","text":"Paging Polices specify how you will be contacted when on-call. The Primary Paging Policy will have defaulted to sending you an SMS assuming you added your phone number when activating your account. We will now configure this policy into a three tier multi-stage policy similar to the image below.","title":"4. Paging Policies"},{"location":"oncall/getting_started/user_profile/#step-1-send-a-push-notification","text":"Click the edit policy button in the top right corner for the Primary Paging Policy. Send a push notification to all my devices Execute the next step if I have not responded within 5 minutes Click Add a Step","title":"Step 1: Send a push notification"},{"location":"oncall/getting_started/user_profile/#step-2-send-an-e-mail","text":"Send an e-mail to [your email address] Execute the next step if I have not responded within 5 minutes Click Add a Step","title":"Step 2: Send an e-mail"},{"location":"oncall/getting_started/user_profile/#step-3-call-your-number","text":"Every 5 minutes until we have reached you Make a phone call to [your phone number] Click Save to save the policy. When you are on-call or in the escalation path of an incident, you will receive notifications in this order following these time delays. To cease the paging you must acknowledge the incident. Acknowledgements can occur in one of the following ways: Expanding the Push Notification on your device and selecting Acknowledge Responding to the SMS with the 5 digit code included Pressing 4 during the Phone Call Slack Button For more information on Notification Types, see here .","title":"Step 3: Call your number"},{"location":"oncall/getting_started/user_profile/#5-custom-paging-policies","text":"Custom paging polices enable you to override the primary policy based on the time and day of the week. A good example would be get the system to immediately phone you whenever you get a page during the evening or weekends as this is more likely to get your attention than a push notification. Create a new Custom Policy by clicking Add a Policy and configure with the following settings:","title":"5. Custom Paging Policies"},{"location":"oncall/getting_started/user_profile/#51-custom-evening-policy","text":"Policy Name: Evening Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: All 7 Days Timezone Between 7pm and 9am Click Save to save the policy then add one more.","title":"5.1 Custom evening policy"},{"location":"oncall/getting_started/user_profile/#52-custom-weekend-policy","text":"Policy Name: Weekend Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: Sat & Sun Timezone Between 9am and 7pm Click Save to save the policy. These custom paging policies will be used during the specified times in place of the Primary Policy. However admins do have the ability to ignore these custom policies, and we will highlight how this is achieved in a later module. The final option here is the setting for Recovery Notifications. As these are typically low priority simply sending you an email or a push notification are the typical settings used. Your profile is now fully configured using these example configurations. Organizations will have different views on how profiles should be configured and will typically issue guidelines for paging policies and times between escalations etc. Please wait for the instructor before proceeding to the Teams module.","title":"5.2 Custom weekend policy"},{"location":"oncall/incident_lifecycle/","text":"UI Overview \u00b6 Aim \u00b6 The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features. 1. Timeline \u00b6 The aim of Splunk On-Call is to \"Make On Call Suck Less\" , and it does this by getting the critical data, to the right people, at the right time. The key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting. Login to the Splunk On-Call UI and select the Timeline tab on the main menu bar, you should have a screen similar to the following image: 2. People \u00b6 On the left we have the People section with the Teams and Users sub tabs. On the Teams tab, click on All Teams then expand [Your Teamname] . Users with the Splunk On-Call Logo against their name are currently on call. Here you can see who is on call within a particular Team, or across all Teams via Users \u2192 On-Call . If you click into one of the currently on call users, you can see their status. It shows which Rotation they are on call for, when their current Shift ends and their next Shift starts (times are displayed in your timezone), what contact methods they have and which Teams they belong to (dummy users such as Hank do not have Contact Methods configured). 3. Timeline \u00b6 In the centre Timeline section you get a realtime view of what is happening within your environment with the newest messages at the top. Here you can quickly post update messages to make your colleagues aware of important developments etc. You can filter the view using the buttons on the top toolbar showing only update messages, GitHub integrations, or apply more advanced filters. Lets change the Filters settings to streamline your view. Click the Filters button then within the Routing Keys tab change the Show setting from all routing keys to selected routing keys . Change the My Keys value to all and the Other Keys value to selected and deselect all keys under the Other Keys section. Click anywhere outside of the dialogue box to close it. You will probably now have a much simpler view as you will not currently have Incidents created using your Routing Keys, so you are left with the other types of messages that the Timeline can display. Click on Filters again, but this time switch to the Message Types tab. Here you control the types of messages that are displayed. For example, deselect On-call Changes and Escalations , this will reduce the amount of messages displayed. 4. Incidents \u00b6 On the right we have the Incidents section. Here we get a list of all the incidents within the platform, or we can view a more specific list such as incidents you are specifically assigned to, or for any of the Teams you are a member of. Select the Team Incidents tab you should find that the Triggered , Acknowledged & Resolved tabs are currently all empty as you have had no incidents logged. Let's change that by generating your first incident! Continue with the Create Incidents module.","title":"UI Overview"},{"location":"oncall/incident_lifecycle/#ui-overview","text":"","title":"UI Overview"},{"location":"oncall/incident_lifecycle/#aim","text":"The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features.","title":"Aim"},{"location":"oncall/incident_lifecycle/#1-timeline","text":"The aim of Splunk On-Call is to \"Make On Call Suck Less\" , and it does this by getting the critical data, to the right people, at the right time. The key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting. Login to the Splunk On-Call UI and select the Timeline tab on the main menu bar, you should have a screen similar to the following image:","title":"1. Timeline"},{"location":"oncall/incident_lifecycle/#2-people","text":"On the left we have the People section with the Teams and Users sub tabs. On the Teams tab, click on All Teams then expand [Your Teamname] . Users with the Splunk On-Call Logo against their name are currently on call. Here you can see who is on call within a particular Team, or across all Teams via Users \u2192 On-Call . If you click into one of the currently on call users, you can see their status. It shows which Rotation they are on call for, when their current Shift ends and their next Shift starts (times are displayed in your timezone), what contact methods they have and which Teams they belong to (dummy users such as Hank do not have Contact Methods configured).","title":"2. People"},{"location":"oncall/incident_lifecycle/#3-timeline","text":"In the centre Timeline section you get a realtime view of what is happening within your environment with the newest messages at the top. Here you can quickly post update messages to make your colleagues aware of important developments etc. You can filter the view using the buttons on the top toolbar showing only update messages, GitHub integrations, or apply more advanced filters. Lets change the Filters settings to streamline your view. Click the Filters button then within the Routing Keys tab change the Show setting from all routing keys to selected routing keys . Change the My Keys value to all and the Other Keys value to selected and deselect all keys under the Other Keys section. Click anywhere outside of the dialogue box to close it. You will probably now have a much simpler view as you will not currently have Incidents created using your Routing Keys, so you are left with the other types of messages that the Timeline can display. Click on Filters again, but this time switch to the Message Types tab. Here you control the types of messages that are displayed. For example, deselect On-call Changes and Escalations , this will reduce the amount of messages displayed.","title":"3. Timeline"},{"location":"oncall/incident_lifecycle/#4-incidents","text":"On the right we have the Incidents section. Here we get a list of all the incidents within the platform, or we can view a more specific list such as incidents you are specifically assigned to, or for any of the Teams you are a member of. Select the Team Incidents tab you should find that the Triggered , Acknowledged & Resolved tabs are currently all empty as you have had no incidents logged. Let's change that by generating your first incident! Continue with the Create Incidents module.","title":"4. Incidents"},{"location":"oncall/incident_lifecycle/create_incidents/","text":"Create Incidents \u00b6 Aim \u00b6 The aim of this module is for you to place yourself 'On-Call' then generate an Incident using the supplied EC2 Instance so you can then work through the lifecycle of an Incident. 1. On-Call \u00b6 Before generating any incidents you should assign yourself to the current Shift within your Follow the Sun Support - Business Hours Rotation and also place yourself On-Call . Click on the Schedule link within your Team in the People section on the left, or navigate to Teams \u2192 [Your Team] \u2192 Rotations Expand the Follow the Sun Support - Business Hours Rotation Click on the Manage members icon (the figures) for the current active shift depending on your timezone Use the Select a user to add... dropdown to add yourself to the shift Then click on Set Current next to your name to make yourself the current on-call user within the shift You should now get a Push Notification to your phone informing you that You Are Now On-Call 2. Trigger Alert \u00b6 Switch back to your shell session connected to your EC2 Instance; all of the following commands will be executed from your Instance. Force the CPU to spike to 100% by running the following command: Shell Command openssl speed -multi $(grep -ci processor /proc/cpuinfo) Output Forked child 0 +DT:md4:3:16 +R:19357020:md4:3.000000 +DT:md4:3:64 +R:14706608:md4:3.010000 +DT:md4:3:256 +R:8262960:md4:3.000000 +DT:md4:3:1024 This will result in an Alert being generated by Splunk Infrastructure Monitoring which in turn will generate an Incident within Splunk On-Call within a maximum of 10 seconds. This is the default polling time for the SignalFx Agent installed on your Instance (note it can be reduced to 1 second). Continue with the Manage Incidents module.","title":"Create Incidents"},{"location":"oncall/incident_lifecycle/create_incidents/#create-incidents","text":"","title":"Create Incidents"},{"location":"oncall/incident_lifecycle/create_incidents/#aim","text":"The aim of this module is for you to place yourself 'On-Call' then generate an Incident using the supplied EC2 Instance so you can then work through the lifecycle of an Incident.","title":"Aim"},{"location":"oncall/incident_lifecycle/create_incidents/#1-on-call","text":"Before generating any incidents you should assign yourself to the current Shift within your Follow the Sun Support - Business Hours Rotation and also place yourself On-Call . Click on the Schedule link within your Team in the People section on the left, or navigate to Teams \u2192 [Your Team] \u2192 Rotations Expand the Follow the Sun Support - Business Hours Rotation Click on the Manage members icon (the figures) for the current active shift depending on your timezone Use the Select a user to add... dropdown to add yourself to the shift Then click on Set Current next to your name to make yourself the current on-call user within the shift You should now get a Push Notification to your phone informing you that You Are Now On-Call","title":"1. On-Call"},{"location":"oncall/incident_lifecycle/create_incidents/#2-trigger-alert","text":"Switch back to your shell session connected to your EC2 Instance; all of the following commands will be executed from your Instance. Force the CPU to spike to 100% by running the following command: Shell Command openssl speed -multi $(grep -ci processor /proc/cpuinfo) Output Forked child 0 +DT:md4:3:16 +R:19357020:md4:3.000000 +DT:md4:3:64 +R:14706608:md4:3.010000 +DT:md4:3:256 +R:8262960:md4:3.000000 +DT:md4:3:1024 This will result in an Alert being generated by Splunk Infrastructure Monitoring which in turn will generate an Incident within Splunk On-Call within a maximum of 10 seconds. This is the default polling time for the SignalFx Agent installed on your Instance (note it can be reduced to 1 second). Continue with the Manage Incidents module.","title":"2. Trigger Alert"},{"location":"oncall/incident_lifecycle/manage_incidents/","text":"Managing Incidents \u00b6 1. Acknowledge \u00b6 Use your Splunk On-Call App on your phone to acknowledge the Incident by clicking on the push notification ... ...to open the alert in the Splunk On-Call mobile app, then clicking on either the single tick in the top right hand corner, or the Acknowledge link to acknowledge the incident and stop the escalation process. The will then transform into a , and the status will change from TRIGGERED to ACKNOWLEDGED . Triggered Incident Acknowledge Incident 2. Details and Annotations \u00b6 Still on your phone, select the Alert Details tab. Then on the Web UI, navigate back to Timeline , select Team Incidents on the right, then select Acknowledged and click into the new Incident , this will open up the War Room Dashboard view of the Incident. You should now have the Details tab displayed on both your Phone and the Web UI. Notice how they both show the exact same information. Now select the Annotations tab on both the Phone and the Web UI, you should have a Graph displayed in the UI which is generated by Splunk Infrastructure Monitoring. On your phone you should get the same image displayed (sometimes it's a simple hyperlink depending on the image size) Splunk On-Call is a 'Mobile First' platform meaning the phone app is full functionality and you can manage an incident directly from your phone. For the remainder of this module we will focus on the Web UI however please spend some time later exploring the phone app features. 3. Link to Alerting System \u00b6 Sticking with the Web UI, click the 2. Alert Details in SignalFx link. This will open a new browser tab and take you directly to the Alert within Splunk Infrastructure Monitoring where you could then progress your troubleshooting using the powerful tools built into its UI. However, we are focussing on Splunk On-Call so close this tab and return to the Splunk On-Call UI. 4. Similar Incidents \u00b6 What if Splunk On-Call could identify previous incidents within the system which may give you a clue to the best way to tackle this incident. The Similar Incidents tab does exactly that, surfacing previous incidents allowing you to look at them and see what actions were taken to resolve them, actions which could be easily repeated for this incident. 5 Timeline \u00b6 On right we have a Time Line view where you can add messages and see the history of previous alerts and interactions. 6 Add Responders \u00b6 On the far left you have the option of allocating additional resources to this incident by clicking on the Add Responders link. This allows you build a virtual team specific to this incident by adding other Teams or individual Users, and also share details of a Conference Bridge where you can all get together and collaborate. Once the system has built up some incident data history, it will use Machine Learning to suggest Teams and Users who have historically worked on similar incidents, as they may be best placed to help resolve this incident quickly. You can select different Teams and/or Users and also choose from a pre-configured conference bridge, or populate the details of a new bridge from your preferred provider. We do not need to add any Responders in this exercise so close the Add Responders dialogue by clicking Cancel . 7 Reroute \u00b6 If it's decided that maybe the incident could be better dealt with by a different Team, the call can be Rerouted by clicking the Reroute Button at the top of the left hand panel. In a similar method to that used in the Add Responders dialogue, you can select Teams or Users to Reroute the Incident to. We do not need to actually Reroute in this exercise so close the Reroute Incident dialogue by clicking Cancel . 8 Snooze \u00b6 You can also snooze this incident by clicking on the alarm clock Button at the top of the left hand panel. You can enter an amount of time upto 24 hours to snooze the incident. This action will be tracked in the Timeline, and when the time expires the paging will restart. This is useful for low priority incidents, enabling you to put them on a back burner for a few hours, but it ensures they do not get forgotten thanks to the paging process starting again. We do not need to actually Snooze in this exercise so close the Snooze Incident dialogue by clicking Cancel . 9 Action Tracking \u00b6 Now lets fix this issue and update the Incident with what we did. Add a new message at the top of the right hand panel such as Discovered rogue process, terminated it . All the actions related to the Incident will be recorded here, and can then be summarized is a Post Incident Review Report available from the Reports tab 10 Resolution \u00b6 Now kill off the process we started in the VM to max out the CPU by switching back the Shell session for the VM and pressing ctrl+c Within no more than 10 seconds SignalFx should detect the new CPU value, clear the alert state in SignalFx, then automatically update the Incident in VictorOps marking it as Resolved . As we have two way integration between Splunk Infrastructure Monitoring and Splunk On-Call we could have also marked the incident as Resolved in Splunk On-Call, and this would have resulted in the alert in Splunk Infrastructure Monitoring being resolved as well. That completes this introduction to Splunk On-Call, but feel free to checkout the more advanced modules which will be published in the coming weeks in the Optional Modules section. These will cover topics such as: Reporting Using the API Webhooks Alert Rules Engine Maintenance Mode","title":"Managing Incidents"},{"location":"oncall/incident_lifecycle/manage_incidents/#managing-incidents","text":"","title":"Managing Incidents"},{"location":"oncall/incident_lifecycle/manage_incidents/#1-acknowledge","text":"Use your Splunk On-Call App on your phone to acknowledge the Incident by clicking on the push notification ... ...to open the alert in the Splunk On-Call mobile app, then clicking on either the single tick in the top right hand corner, or the Acknowledge link to acknowledge the incident and stop the escalation process. The will then transform into a , and the status will change from TRIGGERED to ACKNOWLEDGED . Triggered Incident Acknowledge Incident","title":"1. Acknowledge"},{"location":"oncall/incident_lifecycle/manage_incidents/#2-details-and-annotations","text":"Still on your phone, select the Alert Details tab. Then on the Web UI, navigate back to Timeline , select Team Incidents on the right, then select Acknowledged and click into the new Incident , this will open up the War Room Dashboard view of the Incident. You should now have the Details tab displayed on both your Phone and the Web UI. Notice how they both show the exact same information. Now select the Annotations tab on both the Phone and the Web UI, you should have a Graph displayed in the UI which is generated by Splunk Infrastructure Monitoring. On your phone you should get the same image displayed (sometimes it's a simple hyperlink depending on the image size) Splunk On-Call is a 'Mobile First' platform meaning the phone app is full functionality and you can manage an incident directly from your phone. For the remainder of this module we will focus on the Web UI however please spend some time later exploring the phone app features.","title":"2. Details and Annotations"},{"location":"oncall/incident_lifecycle/manage_incidents/#3-link-to-alerting-system","text":"Sticking with the Web UI, click the 2. Alert Details in SignalFx link. This will open a new browser tab and take you directly to the Alert within Splunk Infrastructure Monitoring where you could then progress your troubleshooting using the powerful tools built into its UI. However, we are focussing on Splunk On-Call so close this tab and return to the Splunk On-Call UI.","title":"3. Link to Alerting System"},{"location":"oncall/incident_lifecycle/manage_incidents/#4-similar-incidents","text":"What if Splunk On-Call could identify previous incidents within the system which may give you a clue to the best way to tackle this incident. The Similar Incidents tab does exactly that, surfacing previous incidents allowing you to look at them and see what actions were taken to resolve them, actions which could be easily repeated for this incident.","title":"4. Similar Incidents"},{"location":"oncall/incident_lifecycle/manage_incidents/#5-timeline","text":"On right we have a Time Line view where you can add messages and see the history of previous alerts and interactions.","title":"5 Timeline"},{"location":"oncall/incident_lifecycle/manage_incidents/#6-add-responders","text":"On the far left you have the option of allocating additional resources to this incident by clicking on the Add Responders link. This allows you build a virtual team specific to this incident by adding other Teams or individual Users, and also share details of a Conference Bridge where you can all get together and collaborate. Once the system has built up some incident data history, it will use Machine Learning to suggest Teams and Users who have historically worked on similar incidents, as they may be best placed to help resolve this incident quickly. You can select different Teams and/or Users and also choose from a pre-configured conference bridge, or populate the details of a new bridge from your preferred provider. We do not need to add any Responders in this exercise so close the Add Responders dialogue by clicking Cancel .","title":"6 Add Responders"},{"location":"oncall/incident_lifecycle/manage_incidents/#7-reroute","text":"If it's decided that maybe the incident could be better dealt with by a different Team, the call can be Rerouted by clicking the Reroute Button at the top of the left hand panel. In a similar method to that used in the Add Responders dialogue, you can select Teams or Users to Reroute the Incident to. We do not need to actually Reroute in this exercise so close the Reroute Incident dialogue by clicking Cancel .","title":"7 Reroute"},{"location":"oncall/incident_lifecycle/manage_incidents/#8-snooze","text":"You can also snooze this incident by clicking on the alarm clock Button at the top of the left hand panel. You can enter an amount of time upto 24 hours to snooze the incident. This action will be tracked in the Timeline, and when the time expires the paging will restart. This is useful for low priority incidents, enabling you to put them on a back burner for a few hours, but it ensures they do not get forgotten thanks to the paging process starting again. We do not need to actually Snooze in this exercise so close the Snooze Incident dialogue by clicking Cancel .","title":"8 Snooze"},{"location":"oncall/incident_lifecycle/manage_incidents/#9-action-tracking","text":"Now lets fix this issue and update the Incident with what we did. Add a new message at the top of the right hand panel such as Discovered rogue process, terminated it . All the actions related to the Incident will be recorded here, and can then be summarized is a Post Incident Review Report available from the Reports tab","title":"9 Action Tracking"},{"location":"oncall/incident_lifecycle/manage_incidents/#10-resolution","text":"Now kill off the process we started in the VM to max out the CPU by switching back the Shell session for the VM and pressing ctrl+c Within no more than 10 seconds SignalFx should detect the new CPU value, clear the alert state in SignalFx, then automatically update the Incident in VictorOps marking it as Resolved . As we have two way integration between Splunk Infrastructure Monitoring and Splunk On-Call we could have also marked the incident as Resolved in Splunk On-Call, and this would have resulted in the alert in Splunk Infrastructure Monitoring being resolved as well. That completes this introduction to Splunk On-Call, but feel free to checkout the more advanced modules which will be published in the coming weeks in the Optional Modules section. These will cover topics such as: Reporting Using the API Webhooks Alert Rules Engine Maintenance Mode","title":"10 Resolution"},{"location":"oncall/optional/detector/","text":"Create a SignalFx Detector \u00b6 Aim \u00b6 We need to create a new Detector within SignalFx which will use VictorOps as the target to send alerts to. We will use Terraform installed within the VM to create the Detector, but first we need to obtain some values required for Terraform to run. 1. Preparation \u00b6 The presenter will typically share these values with you at the start of the module to save time, but the following instructions explain how to get them for yourself. 1.1 Create a variables document \u00b6 We suggest you create a variables document using your preferred text editor as you will be gathering three different values in the next few steps which you need to use in the last step of this module. Add the following lines to your variables document, then as you gather the values you can add them to the appropriate lines: variables.txt export ACCESS_TOKEN= export REALM= export SFXVOPSID= 1.2 Obtain SignalFx Access Token \u00b6 In the Splunk UI you can find your Access Token by clicking on the Settings icon on the top right of the Splunk UI, select Organization Settings \u2192 Access Tokens , expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy it you your clipboard, then paste it into the ACCESS_TOKEN line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= export SFXVOPSID= 1.3 Obtain SignalFx Realm \u00b6 Still in the Splunk UI, click on the Settings icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us1 , but yours may be eu0 or one of the many other SignalFx Realms. Copy it to the REALM line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID= 1.4. Obtain VictorOps Integration ID \u00b6 In Splunk UI navigate to Integrations and use the search feature to find the VictorOps Integration. Expand the VictorOps-xxxx configuration; if there are more than one you will be informed which one to copy by the presenter. Copy it to the SFXVOPSID line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID= xxxx 2. Create environment variables \u00b6 2.1 Copy variables to VM \u00b6 With all the required values now safely copied into your variables document you can use them to compile the commands which we will run in your VM in the next step. Example export SFXVOPSID=EYierbGA4AA export ACCESS_TOKEN=by78voyt7b..... export REALM=us1 Switch back to your shell session connected to the VM you created in the Getting Started/Create a Test Environment module, all of the following commands will be executed within this instance: Past the three commands from your variables document into the shell session of your VM. 3. Initialize and apply Terraform \u00b6 Still within your VM, switch to the victorops folder where the Terraform config files are located (you should still be logged in as Ubuntu and should not have elevated to root) Change Directory cd ~/workshop/victorops Now we can initialize Terraform: Shell Command terraform init -upgrade Example Output Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.21.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.21\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. You can now copy and the paste the following code block to run Terraform using the Variables you created in the VM. Check the plan output for errors before typing yes to commit the apply. Shell Command terraform apply \\ -var=\"access_token=$ACCESS_TOKEN\" \\ -var=\"realm=$REALM\" \\ -var=\"sfx_prefix=${HOSTNAME}\" \\ -var=\"sfx_vo_id=$SFXVOPSID\" \\ -var=\"routing_key=${HOSTNAME}_PRI\" Example Output An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # signalfx_detector.cpu_greater_90 will be created + resource \"signalfx_detector\" \"cpu_greater_90\" { + description = \"Alerts when CPU usage is greater than 90%\" + id = (known after apply) + max_delay = 0 + name = \"vmpe CPU greater than 90%\" + program_text = <<~EOT from signalfx.detectors.against_recent import against_recent A = data('cpu.utilization', filter=filter('host', 'vmpe*')).publish(label='A') detect(when(A > threshold(90))).publish('CPU utilization is greater than 90%') EOT + show_data_markers = true + time_range = 3600 + url = (known after apply) + rule { + detect_label = \"CPU utilization is greater than 90%\" + disabled = false + notifications = [ + \"VictorOps,xxx,vmpe_pri\", ] + parameterized_body = <<~EOT {{#if anomalous}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" triggered at {{timestamp}}. {{else}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" cleared at {{timestamp}}. {{/if}} {{#if anomalous}} Triggering condition: {{{readableRule}}} {{/if}} {{#if anomalous}} Signal value: {{inputs.A.value}} {{else}} Current signal value: {{inputs.A.value}} {{/if}} {{#notEmpty dimensions}} Signal details: {{{dimensions}}} {{/notEmpty}} {{#if anomalous}} {{#if runbookUrl}} Runbook: {{{runbookUrl}}} {{/if}} {{#if tip}} Tip: {{{tip}}} {{/if}} {{/if}} EOT + parameterized_subject = \"{{ruleSeverity}} Alert: {{{ruleName}}} ({{{detectorName}}})\" + severity = \"Critical\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions in workspace \"Workshop\"? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes signalfx_detector.cpu_greater_90: Creating... signalfx_detector.cpu_greater_90: Creation complete after 2s [id=EWHU-YAAAAA] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. 4. Summary \u00b6 By running Terraform within the VM you have just created a new Detector within SignalFx which will send alerts to VictorOps if the CPU utilization of your specific VM goes above 90%. In the Splunk UI go to Alerts \u2192 Detectors to show all the Detectors and find the one matching your INSTANCE value (the first four letters of the name of your VM). Optionally - Click on CPU Utilization is greater than 90% to open the Alert Rule Editor to view its settings. A filter has been used to specifically monitor your VM using the 1st 4 characters of its name, which were randomly assigned when you created the VM. A Recipient has been configured using the VictorOps Integration and your Routing Key has been specified. This is how a monitoring system such as SignalFx knows to route Alerts into VictorOps, and ensure they get routed to the correct team. You have now configured the Integrations between VictorOps and SignalFx! The final part of this module is to test the flow of alerts from SignalFx into VictorOps and see how you can manage the incident with both the VictorOps UI and Mobile App.","title":"Create a SignalFx Detector"},{"location":"oncall/optional/detector/#create-a-signalfx-detector","text":"","title":"Create a SignalFx Detector"},{"location":"oncall/optional/detector/#aim","text":"We need to create a new Detector within SignalFx which will use VictorOps as the target to send alerts to. We will use Terraform installed within the VM to create the Detector, but first we need to obtain some values required for Terraform to run.","title":"Aim"},{"location":"oncall/optional/detector/#1-preparation","text":"The presenter will typically share these values with you at the start of the module to save time, but the following instructions explain how to get them for yourself.","title":"1. Preparation"},{"location":"oncall/optional/detector/#11-create-a-variables-document","text":"We suggest you create a variables document using your preferred text editor as you will be gathering three different values in the next few steps which you need to use in the last step of this module. Add the following lines to your variables document, then as you gather the values you can add them to the appropriate lines: variables.txt export ACCESS_TOKEN= export REALM= export SFXVOPSID=","title":"1.1 Create a variables document"},{"location":"oncall/optional/detector/#12-obtain-signalfx-access-token","text":"In the Splunk UI you can find your Access Token by clicking on the Settings icon on the top right of the Splunk UI, select Organization Settings \u2192 Access Tokens , expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy it you your clipboard, then paste it into the ACCESS_TOKEN line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= export SFXVOPSID=","title":"1.2 Obtain SignalFx Access Token"},{"location":"oncall/optional/detector/#13-obtain-signalfx-realm","text":"Still in the Splunk UI, click on the Settings icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us1 , but yours may be eu0 or one of the many other SignalFx Realms. Copy it to the REALM line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID=","title":"1.3 Obtain SignalFx Realm"},{"location":"oncall/optional/detector/#14-obtain-victorops-integration-id","text":"In Splunk UI navigate to Integrations and use the search feature to find the VictorOps Integration. Expand the VictorOps-xxxx configuration; if there are more than one you will be informed which one to copy by the presenter. Copy it to the SFXVOPSID line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID= xxxx","title":"1.4. Obtain VictorOps Integration ID"},{"location":"oncall/optional/detector/#2-create-environment-variables","text":"","title":"2. Create environment variables"},{"location":"oncall/optional/detector/#21-copy-variables-to-vm","text":"With all the required values now safely copied into your variables document you can use them to compile the commands which we will run in your VM in the next step. Example export SFXVOPSID=EYierbGA4AA export ACCESS_TOKEN=by78voyt7b..... export REALM=us1 Switch back to your shell session connected to the VM you created in the Getting Started/Create a Test Environment module, all of the following commands will be executed within this instance: Past the three commands from your variables document into the shell session of your VM.","title":"2.1 Copy variables to VM"},{"location":"oncall/optional/detector/#3-initialize-and-apply-terraform","text":"Still within your VM, switch to the victorops folder where the Terraform config files are located (you should still be logged in as Ubuntu and should not have elevated to root) Change Directory cd ~/workshop/victorops Now we can initialize Terraform: Shell Command terraform init -upgrade Example Output Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.21.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.21\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. You can now copy and the paste the following code block to run Terraform using the Variables you created in the VM. Check the plan output for errors before typing yes to commit the apply. Shell Command terraform apply \\ -var=\"access_token=$ACCESS_TOKEN\" \\ -var=\"realm=$REALM\" \\ -var=\"sfx_prefix=${HOSTNAME}\" \\ -var=\"sfx_vo_id=$SFXVOPSID\" \\ -var=\"routing_key=${HOSTNAME}_PRI\" Example Output An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # signalfx_detector.cpu_greater_90 will be created + resource \"signalfx_detector\" \"cpu_greater_90\" { + description = \"Alerts when CPU usage is greater than 90%\" + id = (known after apply) + max_delay = 0 + name = \"vmpe CPU greater than 90%\" + program_text = <<~EOT from signalfx.detectors.against_recent import against_recent A = data('cpu.utilization', filter=filter('host', 'vmpe*')).publish(label='A') detect(when(A > threshold(90))).publish('CPU utilization is greater than 90%') EOT + show_data_markers = true + time_range = 3600 + url = (known after apply) + rule { + detect_label = \"CPU utilization is greater than 90%\" + disabled = false + notifications = [ + \"VictorOps,xxx,vmpe_pri\", ] + parameterized_body = <<~EOT {{#if anomalous}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" triggered at {{timestamp}}. {{else}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" cleared at {{timestamp}}. {{/if}} {{#if anomalous}} Triggering condition: {{{readableRule}}} {{/if}} {{#if anomalous}} Signal value: {{inputs.A.value}} {{else}} Current signal value: {{inputs.A.value}} {{/if}} {{#notEmpty dimensions}} Signal details: {{{dimensions}}} {{/notEmpty}} {{#if anomalous}} {{#if runbookUrl}} Runbook: {{{runbookUrl}}} {{/if}} {{#if tip}} Tip: {{{tip}}} {{/if}} {{/if}} EOT + parameterized_subject = \"{{ruleSeverity}} Alert: {{{ruleName}}} ({{{detectorName}}})\" + severity = \"Critical\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions in workspace \"Workshop\"? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes signalfx_detector.cpu_greater_90: Creating... signalfx_detector.cpu_greater_90: Creation complete after 2s [id=EWHU-YAAAAA] Apply complete! Resources: 1 added, 0 changed, 0 destroyed.","title":"3. Initialize and apply Terraform"},{"location":"oncall/optional/detector/#4-summary","text":"By running Terraform within the VM you have just created a new Detector within SignalFx which will send alerts to VictorOps if the CPU utilization of your specific VM goes above 90%. In the Splunk UI go to Alerts \u2192 Detectors to show all the Detectors and find the one matching your INSTANCE value (the first four letters of the name of your VM). Optionally - Click on CPU Utilization is greater than 90% to open the Alert Rule Editor to view its settings. A filter has been used to specifically monitor your VM using the 1st 4 characters of its name, which were randomly assigned when you created the VM. A Recipient has been configured using the VictorOps Integration and your Routing Key has been specified. This is how a monitoring system such as SignalFx knows to route Alerts into VictorOps, and ensure they get routed to the correct team. You have now configured the Integrations between VictorOps and SignalFx! The final part of this module is to test the flow of alerts from SignalFx into VictorOps and see how you can manage the incident with both the VictorOps UI and Mobile App.","title":"4. Summary"},{"location":"oncall/optional/multipass/","text":"Creating a Test VM Using Multipass \u00b6 Aim \u00b6 The aim of this module is to guide you through the process of creating a VM locally using Multipass. Once the configuration of VictorOps is complete you will use this VM to trigger an Alert from SignalFx which in turn will create an Incident within VictorOps, resulting in you getting paged. 1. Install Multipass \u00b6 If you do not already have Multipass installed you can download the installer from here . Users running macOS can install it using Homebrew by running: Shell Command brew cask install multipass 2. Create VM using Multipass \u00b6 2.1 Cloud-init \u00b6 The first step is to pull down the cloud-init file to launch a pre-configured VM. Shell Command WSVERSION=2.26 curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/victorops.yaml \\ -o victorops.yaml 2.2 Launch VM \u00b6 Remaining in the same directory where you downloaded victorops.yaml , run the following commands to create your VM. The first command will generate a random unique 4 character string. This will prevent clashes in the Splunk UI. Shell Command export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) multipass launch \\ --name ${INSTANCE} \\ --cloud-init victorops.yaml Example Output Launched: zevn Make a note of your VMs Hostname as you will need it in later steps. 2.3 Connect to VM \u00b6 Once the VM has deployed successfully, in a new shell session connect to the VM using the following command. Shell Command multipass shell ${INSTANCE} Example Input multipass shell zevn Example Output Last login: Tue Jun 9 15:10:19 2020 from 192.168.64.1 ubuntu@zevn:~$ 3. Install SignalFx Agent \u00b6 An easy way to install the SignalFx Agent into your VM is to copy the install commands from the Splunk UI, then run them directly within your VM. 3.1 Splunk UI \u00b6 Navigate to the Integrations tab within the Splunk UI, where you will find the SignalFx SmartAgent tile on the top row. Click on the SmartAgent tile to open it... ...then select the Setup tab... ...then scroll down to 'Step 1' where you will find the commands for installing the agent for both Linux and Windows. You need to copy the commands for Linux, so click the top copy button to place these commands on your clipboard ready for the next step. 3.2 Install Agent \u00b6 Now paste the linux install commands into your VM Shell, the SignalFx Agent will install and after approx 1 min you should have the following result. Example Output The SignalFx Agent has been successfully installed. Make sure that your system's time is relatively accurate or else datapoints may not be accepted. The agent's main configuration file is located at /etc/signalfx/agent.yaml. 4. Check SignalFx Agent \u00b6 4.1 Agent Status \u00b6 Once the agent has completed installing run the following command to check the status Shell Command sudo signalfx-agent status Example Output SignalFx Agent version: 5.3.0 Agent uptime: 2m7s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 6 Bad Monitor Config: None Global Dimensions: {host: zevn} GlobalSpanTags: map[] Datapoints sent (last minute): 237 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 18 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything 4.2 Check the Splunk UI \u00b6 Navigate to the Splunk UI and click on the Infrastructure tab. The click on Hosts (Smart Agent / collectd) under the Hosts section. Find your VM and confirm it is reporting in correctly; allow a few minutes for it to appear. If it fails to appear after 3 mins, please let the Splunk Team know so they can help troubleshoot. Because you are using a Multipass VM instead of a Splunk Provided EC2 Instance, you will also need to complete the optional module \"Create a Detector\". This will ensure you receive incident notifications for this VM.","title":"Creating a Test VM Using Multipass"},{"location":"oncall/optional/multipass/#creating-a-test-vm-using-multipass","text":"","title":"Creating a Test VM Using Multipass"},{"location":"oncall/optional/multipass/#aim","text":"The aim of this module is to guide you through the process of creating a VM locally using Multipass. Once the configuration of VictorOps is complete you will use this VM to trigger an Alert from SignalFx which in turn will create an Incident within VictorOps, resulting in you getting paged.","title":"Aim"},{"location":"oncall/optional/multipass/#1-install-multipass","text":"If you do not already have Multipass installed you can download the installer from here . Users running macOS can install it using Homebrew by running: Shell Command brew cask install multipass","title":"1. Install Multipass"},{"location":"oncall/optional/multipass/#2-create-vm-using-multipass","text":"","title":"2. Create VM using Multipass"},{"location":"oncall/optional/multipass/#21-cloud-init","text":"The first step is to pull down the cloud-init file to launch a pre-configured VM. Shell Command WSVERSION=2.26 curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/victorops.yaml \\ -o victorops.yaml","title":"2.1 Cloud-init"},{"location":"oncall/optional/multipass/#22-launch-vm","text":"Remaining in the same directory where you downloaded victorops.yaml , run the following commands to create your VM. The first command will generate a random unique 4 character string. This will prevent clashes in the Splunk UI. Shell Command export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) multipass launch \\ --name ${INSTANCE} \\ --cloud-init victorops.yaml Example Output Launched: zevn Make a note of your VMs Hostname as you will need it in later steps.","title":"2.2 Launch VM"},{"location":"oncall/optional/multipass/#23-connect-to-vm","text":"Once the VM has deployed successfully, in a new shell session connect to the VM using the following command. Shell Command multipass shell ${INSTANCE} Example Input multipass shell zevn Example Output Last login: Tue Jun 9 15:10:19 2020 from 192.168.64.1 ubuntu@zevn:~$","title":"2.3 Connect to VM"},{"location":"oncall/optional/multipass/#3-install-signalfx-agent","text":"An easy way to install the SignalFx Agent into your VM is to copy the install commands from the Splunk UI, then run them directly within your VM.","title":"3. Install SignalFx Agent"},{"location":"oncall/optional/multipass/#31-splunk-ui","text":"Navigate to the Integrations tab within the Splunk UI, where you will find the SignalFx SmartAgent tile on the top row. Click on the SmartAgent tile to open it... ...then select the Setup tab... ...then scroll down to 'Step 1' where you will find the commands for installing the agent for both Linux and Windows. You need to copy the commands for Linux, so click the top copy button to place these commands on your clipboard ready for the next step.","title":"3.1 Splunk UI"},{"location":"oncall/optional/multipass/#32-install-agent","text":"Now paste the linux install commands into your VM Shell, the SignalFx Agent will install and after approx 1 min you should have the following result. Example Output The SignalFx Agent has been successfully installed. Make sure that your system's time is relatively accurate or else datapoints may not be accepted. The agent's main configuration file is located at /etc/signalfx/agent.yaml.","title":"3.2 Install Agent"},{"location":"oncall/optional/multipass/#4-check-signalfx-agent","text":"","title":"4. Check SignalFx Agent"},{"location":"oncall/optional/multipass/#41-agent-status","text":"Once the agent has completed installing run the following command to check the status Shell Command sudo signalfx-agent status Example Output SignalFx Agent version: 5.3.0 Agent uptime: 2m7s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 6 Bad Monitor Config: None Global Dimensions: {host: zevn} GlobalSpanTags: map[] Datapoints sent (last minute): 237 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 18 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything","title":"4.1 Agent Status"},{"location":"oncall/optional/multipass/#42-check-the-splunk-ui","text":"Navigate to the Splunk UI and click on the Infrastructure tab. The click on Hosts (Smart Agent / collectd) under the Hosts section. Find your VM and confirm it is reporting in correctly; allow a few minutes for it to appear. If it fails to appear after 3 mins, please let the Splunk Team know so they can help troubleshoot. Because you are using a Multipass VM instead of a Splunk Provided EC2 Instance, you will also need to complete the optional module \"Create a Detector\". This will ensure you receive incident notifications for this VM.","title":"4.2 Check the Splunk UI"},{"location":"oncall/optional/sfx_integrations/","text":"VictorOps Integrations - Lab Summary \u00b6 This module covers configuring the Integrations between SignalFx and VictorOps. Whilst the detailed steps below walk you through the process, you will find that the intergrations are already active within the Splunk systems being used for this workhop so Steps 1 & 2 are for info only. You only need to complete Step 3. Copy ID 1. VictorOps Service API Endpoint \u00b6 Warning The SignalFx Integration only needs to be enabled once per VictorOps instance, so you will probably find it has already been enabled, please DO NOT disable an already active integration when completing this lab. This is for info only as the Integration has already been enabled In order to integrate SignalFx with VictorOps we need to first obtain the Service API Endpoint for VictorOps. Within the VictorOps UI navigate to Integrations main tab and then use the search feature to find the SignalFx Integration. If it is not already enabled, click the Enable Integration button to activate it. This would be used when configuring the VictorOps Integration within the Splunk UI if it had not already been enabled. 2. Enable VictorOps Integration within SignalFx \u00b6 In the Splunk UI navigate to Integrations and use the search feature to find the VictorOps integration. Do not create a new integration! Please do not create additional VictorOps integrations if one already exists, it will not break anything but simply creates extra clean up work after the workshop has completed. The aim of this part of the lab was to show you how you would go about configuring the Integration if it was not already enabled. Assuming you are using the AppDev EMEA instance of VictorOps you will find the VictorOps Integration has already been configured so there is no need to create a new one. However the process of creating a new Integration is simply to click on Create New Integration like in the image below, or if there are existing integrations and you want to add another one you would click New Integration . Enter a descriptive Name then paste the Service_API_Endpoint value you copied in the previous step into the Post URL field, then save it. Handling multiple VictorOps integrations SignalFx can integrate with multiple VictorOps accounts so it is important when creating one to use a descriptive name and to not simply call it VictorOps. This name will be used within the Splunk UI when selecting this integration, so ensure it is unambiguous 3. Copy ID \u00b6 In Splunk UI navigate to Integrations and use the search feature to find the VictorOps Integration. Copy the ID field and save it for use in the next steps. We suggest you create a notepad document or similar as you will be gathering some additional values in the next steps.","title":"VictorOps Integrations - Lab Summary"},{"location":"oncall/optional/sfx_integrations/#victorops-integrations-lab-summary","text":"This module covers configuring the Integrations between SignalFx and VictorOps. Whilst the detailed steps below walk you through the process, you will find that the intergrations are already active within the Splunk systems being used for this workhop so Steps 1 & 2 are for info only. You only need to complete Step 3. Copy ID","title":"VictorOps Integrations - Lab Summary"},{"location":"oncall/optional/sfx_integrations/#1-victorops-service-api-endpoint","text":"Warning The SignalFx Integration only needs to be enabled once per VictorOps instance, so you will probably find it has already been enabled, please DO NOT disable an already active integration when completing this lab. This is for info only as the Integration has already been enabled In order to integrate SignalFx with VictorOps we need to first obtain the Service API Endpoint for VictorOps. Within the VictorOps UI navigate to Integrations main tab and then use the search feature to find the SignalFx Integration. If it is not already enabled, click the Enable Integration button to activate it. This would be used when configuring the VictorOps Integration within the Splunk UI if it had not already been enabled.","title":"1. VictorOps Service API Endpoint"},{"location":"oncall/optional/sfx_integrations/#2-enable-victorops-integration-within-signalfx","text":"In the Splunk UI navigate to Integrations and use the search feature to find the VictorOps integration. Do not create a new integration! Please do not create additional VictorOps integrations if one already exists, it will not break anything but simply creates extra clean up work after the workshop has completed. The aim of this part of the lab was to show you how you would go about configuring the Integration if it was not already enabled. Assuming you are using the AppDev EMEA instance of VictorOps you will find the VictorOps Integration has already been configured so there is no need to create a new one. However the process of creating a new Integration is simply to click on Create New Integration like in the image below, or if there are existing integrations and you want to add another one you would click New Integration . Enter a descriptive Name then paste the Service_API_Endpoint value you copied in the previous step into the Post URL field, then save it. Handling multiple VictorOps integrations SignalFx can integrate with multiple VictorOps accounts so it is important when creating one to use a descriptive name and to not simply call it VictorOps. This name will be used within the Splunk UI when selecting this integration, so ensure it is unambiguous","title":"2. Enable VictorOps Integration within SignalFx"},{"location":"oncall/optional/sfx_integrations/#3-copy-id","text":"In Splunk UI navigate to Integrations and use the search feature to find the VictorOps Integration. Copy the ID field and save it for use in the next steps. We suggest you create a notepad document or similar as you will be gathering some additional values in the next steps.","title":"3. Copy ID"},{"location":"otel/","text":"Introduction \u00b6 During this technical Splunk Observability Cloud Workshop you will build out an environment based on a Amazon EKS Kubernetes cluster. In order to simplify the Workshop modules, a pre-configured AWS/EC2 instance is provided. The instance is pre-configured with all the software required to deploy the OpenTelemetery Collector 2 in Kubernetes, deploy a NGINX 3 ReplicaSet 4 . Finally, the Workshop also introduces you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code 5 and the Service Bureau 5 By the end of this technical workshop you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud. Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. \u21a9 The OpenTelemetry Collector offers a vendor-agnostic implementation on how to receive, process and export telemetry data. In addition, it removes the need to run, operate and maintain multiple agents/collectors in order to support open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) sending to multiple open-source or commercial back-ends. \u21a9 NGINX is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. \u21a9 Kubernetes ReplicaSet \u21a9 Monitoring as Code and Service Bureau \u21a9 \u21a9","title":"Workshop Introduction"},{"location":"otel/#introduction","text":"During this technical Splunk Observability Cloud Workshop you will build out an environment based on a Amazon EKS Kubernetes cluster. In order to simplify the Workshop modules, a pre-configured AWS/EC2 instance is provided. The instance is pre-configured with all the software required to deploy the OpenTelemetery Collector 2 in Kubernetes, deploy a NGINX 3 ReplicaSet 4 . Finally, the Workshop also introduces you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code 5 and the Service Bureau 5 By the end of this technical workshop you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud. Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. \u21a9 The OpenTelemetry Collector offers a vendor-agnostic implementation on how to receive, process and export telemetry data. In addition, it removes the need to run, operate and maintain multiple agents/collectors in order to support open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) sending to multiple open-source or commercial back-ends. \u21a9 NGINX is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. \u21a9 Kubernetes ReplicaSet \u21a9 Monitoring as Code and Service Bureau \u21a9 \u21a9","title":"Introduction"},{"location":"otel/connect-info/","text":"How to connect to your workshop environment \u00b6 How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty 1 or your web browser. Verify your connection to your AWS/EC2 cloud instance. 1. AWS/EC2 IP Address \u00b6 In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2. To get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader. Search for your AWS/EC2 instance by looking for your first and last name name, as provided during registration for this workshop. You find the IP address, the SSH command (for Mac OS, Linux and the latest Windows versions) and the password to use to connect to the workshop instance. It also has the Browser Access URL that you can use in case you cannot connect via ssh or putty - see EC2 access via Web browser Important Please use SSH or Putty to gain acess to your EC2 instance if possible and make a note of the IP address as you will need this during the workshop. 2. SSH (Mac OS/Linux) \u00b6 Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the Google Sheet from Step #1. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue and start the workshop 3. Putty (Windows) \u00b6 If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and enter the in Host Name (or IP address) field the IP address provided in the Google Sheet. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue and start the workshop 4. Web Browser (All) \u00b6 If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http:/x.x.x.x:6501 (where X.X.X.X is the IP address from the Google Sheet). Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop ask you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue and start the workshop . 5. Multipass (All) \u00b6 If you are unable to access AWS, but you can install software locally, follow the instructions for using Multipass . Download Putty \u21a9","title":"How to connect to your workshop environment"},{"location":"otel/connect-info/#how-to-connect-to-your-workshop-environment","text":"How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty 1 or your web browser. Verify your connection to your AWS/EC2 cloud instance.","title":"How to connect to your workshop environment"},{"location":"otel/connect-info/#1-awsec2-ip-address","text":"In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2. To get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader. Search for your AWS/EC2 instance by looking for your first and last name name, as provided during registration for this workshop. You find the IP address, the SSH command (for Mac OS, Linux and the latest Windows versions) and the password to use to connect to the workshop instance. It also has the Browser Access URL that you can use in case you cannot connect via ssh or putty - see EC2 access via Web browser Important Please use SSH or Putty to gain acess to your EC2 instance if possible and make a note of the IP address as you will need this during the workshop.","title":"1. AWS/EC2 IP Address"},{"location":"otel/connect-info/#2-ssh-mac-oslinux","text":"Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the Google Sheet from Step #1. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue and start the workshop","title":"2. SSH (Mac OS/Linux)"},{"location":"otel/connect-info/#3-putty-windows","text":"If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and enter the in Host Name (or IP address) field the IP address provided in the Google Sheet. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue and start the workshop","title":"3. Putty (Windows)"},{"location":"otel/connect-info/#4-web-browser-all","text":"If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http:/x.x.x.x:6501 (where X.X.X.X is the IP address from the Google Sheet). Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop ask you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue and start the workshop .","title":"4. Web Browser (All)"},{"location":"otel/connect-info/#5-multipass-all","text":"If you are unable to access AWS, but you can install software locally, follow the instructions for using Multipass . Download Putty \u21a9","title":"5. Multipass (All)"},{"location":"otel/k3s/","text":"Deploying the OpenTelemetry Collector in Kubernetes \u00b6 Use the Splunk Helm chart to install the OpenTelemetry Collector in EKS Explore your cluster in the Kubernetes Navigator 1. Obtain Access Token \u00b6 You will need to obtain your Access Token 1 from the Splunk UI once Kubernetes is running. You can find your Access Token from the top left hamburger menu then selecting Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy to clipboard. You will also need to obtain the name of the Realm 2 for your Splunk account. From the hamburger menu, click on your name and select Account Settings . The Realm can be found in the middle of the page within the Organizations section. In this example it is us0 . 2. Installation using Helm \u00b6 Create the ACCESS_TOKEN and REALM environment variables to use in the proceeding Helm install command. For instance, if your realm is us1 , you would type export REALM=us1 and for eu0 type export REALM=eu0 . Shell Command export ACCESS_TOKEN=<replace_with_default_access_token> export REALM=<replace_with_splunk_realm> Install the OpenTelemetry Collector using the Splunk Helm chart. First, add the Splunk Helm chart repository to Helm and update. Shell Command helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart && helm repo update Example Output Using ACCESS_TOKEN=<redacted> Using REALM=us1 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN=<redacted> Using REALM=us1 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. \u2388Happy Helming!\u2388 Install the OpenTelemetry Collector Helm chart with the following commands, do NOT edit this: Shell Command helm install splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$(hostname)-k3s-cluster\" \\ --set=\"splunkObservability.logsEnabled=false\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml Example Output Using ACCESS_TOKEN=<redacted> Using REALM=eu0 NAME: splunk-otel-collector LAST DEPLOYED: Fri May 7 11:19:01 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None You can monitor the progress of the deployment by running kubectl get pods which should typically report a new pod is up and running after about 30 seconds. Ensure the status is reported as Running before continuing. Shell Command kubectl get pods Example Output NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-2sk6k 0/1 Running 0 10s splunk-otel-collector-k8s-cluster-receiver-6956d4446f-gwnd7 0/1 Running 0 10s Ensure there are no errors by tailing the logs from the OpenTelemetry Collector pod. Output should look similar to the log output shown in the Output tab below. Use the label set by the helm install to tail logs (You will need to press Ctrl + C to exit). Or use the installed k9s terminal UI for bonus points! Shell Command kubectl logs -l app=splunk-otel-collector -f --container otel-collector Example Output 2021-03-21T16:11:10.900Z INFO service/service.go:364 Starting receivers... 2021-03-21T16:11:10.900Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/watcher.go:195 Configured Kubernetes MetadataExporter {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\", \"exporter_name\": \"signalfx\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO healthcheck/handler.go:128 Health Check state change {\"component_kind\": \"extension\", \"component_type\": \"health_check\", \"component_name\": \"health_check\", \"status\": \"ready\"} 2021-03-21T16:11:11.009Z INFO service/service.go:267 Everything is ready. Begin running and processing data. 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/receiver.go:59 Starting shared informers and wait for initial cache sync. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.281Z INFO k8sclusterreceiver@v0.21.0/receiver.go:75 Completed syncing shared informer caches. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} Deleting a failed installation If you make an error installing the OpenTelemetry Collector you can start over by deleting the installation using: helm delete splunk-otel-collector 3. Validate metrics in the UI \u00b6 In the Splunk UI, from the hamburger menu top left click on Infrastructure . Under Containers click on Kubernetes to open the Kubernetes Navigator Cluster Map to ensure metrics are being sent in. Validate that your cluster is discovered and reporting by finding your cluster (in the workshop you will see many other clusters). To find your cluster name run the following command and copy the output to your clipboard: Shell Command echo $(hostname)-k3s-cluster Then in the UI, click on the \"Cluster: - \" menu just below the Splunk Logo, and paste the Cluster name you just copied into the search box, click the box to select your cluster, and finally click off the menu into white space to apply the filter. To examine the health of your node, hover over the pale blue background of your cluster, then click on the blue magnifying glass that appears in the top left hand corner. This will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar. Once it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc. Access Tokens (sometimes called Org Tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the Splunk API. \u21a9 A realm is a self-contained deployment of Splunk in which your Organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). This realm name is shown on your profile page in the Splunk UI. If you do not include the realm name when specifying an endpoint, Splunk will interpret it as pointing to the us0 realm. \u21a9","title":"Deploy the OTel Collector"},{"location":"otel/k3s/#deploying-the-opentelemetry-collector-in-kubernetes","text":"Use the Splunk Helm chart to install the OpenTelemetry Collector in EKS Explore your cluster in the Kubernetes Navigator","title":"Deploying the OpenTelemetry Collector in Kubernetes"},{"location":"otel/k3s/#1-obtain-access-token","text":"You will need to obtain your Access Token 1 from the Splunk UI once Kubernetes is running. You can find your Access Token from the top left hamburger menu then selecting Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy to clipboard. You will also need to obtain the name of the Realm 2 for your Splunk account. From the hamburger menu, click on your name and select Account Settings . The Realm can be found in the middle of the page within the Organizations section. In this example it is us0 .","title":"1. Obtain Access Token"},{"location":"otel/k3s/#2-installation-using-helm","text":"Create the ACCESS_TOKEN and REALM environment variables to use in the proceeding Helm install command. For instance, if your realm is us1 , you would type export REALM=us1 and for eu0 type export REALM=eu0 . Shell Command export ACCESS_TOKEN=<replace_with_default_access_token> export REALM=<replace_with_splunk_realm> Install the OpenTelemetry Collector using the Splunk Helm chart. First, add the Splunk Helm chart repository to Helm and update. Shell Command helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart && helm repo update Example Output Using ACCESS_TOKEN=<redacted> Using REALM=us1 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN=<redacted> Using REALM=us1 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. \u2388Happy Helming!\u2388 Install the OpenTelemetry Collector Helm chart with the following commands, do NOT edit this: Shell Command helm install splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$(hostname)-k3s-cluster\" \\ --set=\"splunkObservability.logsEnabled=false\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml Example Output Using ACCESS_TOKEN=<redacted> Using REALM=eu0 NAME: splunk-otel-collector LAST DEPLOYED: Fri May 7 11:19:01 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None You can monitor the progress of the deployment by running kubectl get pods which should typically report a new pod is up and running after about 30 seconds. Ensure the status is reported as Running before continuing. Shell Command kubectl get pods Example Output NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-2sk6k 0/1 Running 0 10s splunk-otel-collector-k8s-cluster-receiver-6956d4446f-gwnd7 0/1 Running 0 10s Ensure there are no errors by tailing the logs from the OpenTelemetry Collector pod. Output should look similar to the log output shown in the Output tab below. Use the label set by the helm install to tail logs (You will need to press Ctrl + C to exit). Or use the installed k9s terminal UI for bonus points! Shell Command kubectl logs -l app=splunk-otel-collector -f --container otel-collector Example Output 2021-03-21T16:11:10.900Z INFO service/service.go:364 Starting receivers... 2021-03-21T16:11:10.900Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/watcher.go:195 Configured Kubernetes MetadataExporter {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\", \"exporter_name\": \"signalfx\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO healthcheck/handler.go:128 Health Check state change {\"component_kind\": \"extension\", \"component_type\": \"health_check\", \"component_name\": \"health_check\", \"status\": \"ready\"} 2021-03-21T16:11:11.009Z INFO service/service.go:267 Everything is ready. Begin running and processing data. 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/receiver.go:59 Starting shared informers and wait for initial cache sync. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.281Z INFO k8sclusterreceiver@v0.21.0/receiver.go:75 Completed syncing shared informer caches. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} Deleting a failed installation If you make an error installing the OpenTelemetry Collector you can start over by deleting the installation using: helm delete splunk-otel-collector","title":"2. Installation using Helm"},{"location":"otel/k3s/#3-validate-metrics-in-the-ui","text":"In the Splunk UI, from the hamburger menu top left click on Infrastructure . Under Containers click on Kubernetes to open the Kubernetes Navigator Cluster Map to ensure metrics are being sent in. Validate that your cluster is discovered and reporting by finding your cluster (in the workshop you will see many other clusters). To find your cluster name run the following command and copy the output to your clipboard: Shell Command echo $(hostname)-k3s-cluster Then in the UI, click on the \"Cluster: - \" menu just below the Splunk Logo, and paste the Cluster name you just copied into the search box, click the box to select your cluster, and finally click off the menu into white space to apply the filter. To examine the health of your node, hover over the pale blue background of your cluster, then click on the blue magnifying glass that appears in the top left hand corner. This will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar. Once it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc. Access Tokens (sometimes called Org Tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the Splunk API. \u21a9 A realm is a self-contained deployment of Splunk in which your Organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). This realm name is shown on your profile page in the Splunk UI. If you do not include the realm name when specifying an endpoint, Splunk will interpret it as pointing to the us0 realm. \u21a9","title":"3. Validate metrics in the UI"},{"location":"otel/multipass/","text":"Launch a Multipass instance \u00b6 1. Pre-requisites \u00b6 Install Multipass 1 for your operating system. Make sure you are using at least version 1.6.0 . On a Mac you can also install via Homebrew e.g. brew install multipass 2. Download cloud-init YAML \u00b6 Linux/Mac OS WSVERSION=2.26 mkdir cloud-init curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/k3s.yaml \\ -o cloud-init/k3s.yaml export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) Windows Info Download the zip by clicking on the following URL https://github.com/signalfx/observability-workshop/archive/v2.26.zip . Once downloaded, unzip the the file and rename it to workshop . Then, from the command prompt change into that directory and run $INSTANCE = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\".tochararray() | sort {Get-Random})[0..3] -join '' 3. Launch Multipass instance \u00b6 In this section you will build and launch the Multipass instance which will run the Kubernetes (K3s) environment that you will use in the workshop. Shell Command multipass launch \\ --name ${INSTANCE} \\ --cloud-init cloud-init/k3s.yaml \\ --cpus 4 \\ --mem 4Gb \\ --disk 32Gb Once the instance has been successfully created (this can take several minutes), shell into it. Shell Command multipass shell ${INSTANCE} Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@vmpe:~$ Once your instance presents you with the Splunk logo, you have completed the preparation for your Multipass instance and can at this point you are ready to continue and start the workshop . Multipass is a lightweight VM manager for Linux, Windows and macOS. It's designed for developers who want a fresh Ubuntu environment with a single command. It uses KVM on Linux, Hyper-V on Windows and HyperKit on macOS to run the VM with minimal overhead. It can also use VirtualBox on Windows and macOS. Multipass will fetch images for you and keep them up to date. \u21a9","title":"Launch a Multipass instance"},{"location":"otel/multipass/#launch-a-multipass-instance","text":"","title":"Launch a Multipass instance"},{"location":"otel/multipass/#1-pre-requisites","text":"Install Multipass 1 for your operating system. Make sure you are using at least version 1.6.0 . On a Mac you can also install via Homebrew e.g. brew install multipass","title":"1. Pre-requisites"},{"location":"otel/multipass/#2-download-cloud-init-yaml","text":"Linux/Mac OS WSVERSION=2.26 mkdir cloud-init curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/k3s.yaml \\ -o cloud-init/k3s.yaml export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) Windows Info Download the zip by clicking on the following URL https://github.com/signalfx/observability-workshop/archive/v2.26.zip . Once downloaded, unzip the the file and rename it to workshop . Then, from the command prompt change into that directory and run $INSTANCE = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\".tochararray() | sort {Get-Random})[0..3] -join ''","title":"2. Download cloud-init YAML"},{"location":"otel/multipass/#3-launch-multipass-instance","text":"In this section you will build and launch the Multipass instance which will run the Kubernetes (K3s) environment that you will use in the workshop. Shell Command multipass launch \\ --name ${INSTANCE} \\ --cloud-init cloud-init/k3s.yaml \\ --cpus 4 \\ --mem 4Gb \\ --disk 32Gb Once the instance has been successfully created (this can take several minutes), shell into it. Shell Command multipass shell ${INSTANCE} Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@vmpe:~$ Once your instance presents you with the Splunk logo, you have completed the preparation for your Multipass instance and can at this point you are ready to continue and start the workshop . Multipass is a lightweight VM manager for Linux, Windows and macOS. It's designed for developers who want a fresh Ubuntu environment with a single command. It uses KVM on Linux, Hyper-V on Windows and HyperKit on macOS to run the VM with minimal overhead. It can also use VirtualBox on Windows and macOS. Multipass will fetch images for you and keep them up to date. \u21a9","title":"3. Launch Multipass instance"},{"location":"otel/nginx/","text":"Deploying NGINX in EKS - Lab Summary \u00b6 Deploy a NGINX ReplicaSet into your EKS cluster and confirm the discovery of your NGINX deployment. Run a load test to create metrics and confirm them streaming into Splunk Observability Cloud! 1. Start your NGINX \u00b6 Verify the number of pods running in the Splunk UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster. Note the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node! Now switch back to the default cluster node view by selecting the MAP tab and select your cluster again. In the Multipass or AWS/EC2 shell session and change into the nginx directory: Shell Command cd ~/workshop/k3s/nginx 2. Create NGINX deployment \u00b6 Create the NGINX configmap 1 using the nginx.conf file: Shell Command kubectl create configmap nginxconfig --from-file=nginx.conf Output configmap/nginxconfig created Then create the deployment: Shell Command kubectl create -f nginx-deployment.yaml Output deployment.apps/nginx created service/nginx created Next we will confirm that NGINX is running as Kubernetes service Shell Command kubectl get svc Validate the deployment has been successful. NGINX will be assigned an external IP. It will take a few minutes for the Elastic Load Balancer to be available. Grab the External-IP of NGINX service and visit the page using browser on your computer. If you have the Splunk UI open you should see new Pods being started and containers being deployed. It should only take around 20 seconds for the pods to transition into a Running state. In the Splunk UI you will have a cluster that looks like below: If you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX: Let's validate this in your shell as well: Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-77784c659c-ttmpk 1/1 Running 0 9m19s splunk-otel-collector-agent-249rd 1/1 Running 0 9m19s svclb-nginx-vtnzg 1/1 Running 0 5m57s nginx-7b95fb6b6b-7sb9x 1/1 Running 0 5m57s nginx-7b95fb6b6b-lnzsq 1/1 Running 0 5m57s nginx-7b95fb6b6b-hlx27 1/1 Running 0 5m57s nginx-7b95fb6b6b-zwns9 1/1 Running 0 5m57s Validate you are seeing metrics in the UI by going to hamburger icon, top let and select Dashboards \u2192 NGINX \u2192 NGINX Servers . Using the Overrides filter on k8s.cluster.name: , find the name of your cluster as returned by echo $(hostname)-eks-cluster in the terminal. A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. \u21a9","title":"Deploy and monitor NGINX"},{"location":"otel/nginx/#deploying-nginx-in-eks-lab-summary","text":"Deploy a NGINX ReplicaSet into your EKS cluster and confirm the discovery of your NGINX deployment. Run a load test to create metrics and confirm them streaming into Splunk Observability Cloud!","title":"Deploying NGINX in EKS - Lab Summary"},{"location":"otel/nginx/#1-start-your-nginx","text":"Verify the number of pods running in the Splunk UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster. Note the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node! Now switch back to the default cluster node view by selecting the MAP tab and select your cluster again. In the Multipass or AWS/EC2 shell session and change into the nginx directory: Shell Command cd ~/workshop/k3s/nginx","title":"1. Start your NGINX"},{"location":"otel/nginx/#2-create-nginx-deployment","text":"Create the NGINX configmap 1 using the nginx.conf file: Shell Command kubectl create configmap nginxconfig --from-file=nginx.conf Output configmap/nginxconfig created Then create the deployment: Shell Command kubectl create -f nginx-deployment.yaml Output deployment.apps/nginx created service/nginx created Next we will confirm that NGINX is running as Kubernetes service Shell Command kubectl get svc Validate the deployment has been successful. NGINX will be assigned an external IP. It will take a few minutes for the Elastic Load Balancer to be available. Grab the External-IP of NGINX service and visit the page using browser on your computer. If you have the Splunk UI open you should see new Pods being started and containers being deployed. It should only take around 20 seconds for the pods to transition into a Running state. In the Splunk UI you will have a cluster that looks like below: If you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX: Let's validate this in your shell as well: Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-77784c659c-ttmpk 1/1 Running 0 9m19s splunk-otel-collector-agent-249rd 1/1 Running 0 9m19s svclb-nginx-vtnzg 1/1 Running 0 5m57s nginx-7b95fb6b6b-7sb9x 1/1 Running 0 5m57s nginx-7b95fb6b6b-lnzsq 1/1 Running 0 5m57s nginx-7b95fb6b6b-hlx27 1/1 Running 0 5m57s nginx-7b95fb6b6b-zwns9 1/1 Running 0 5m57s Validate you are seeing metrics in the UI by going to hamburger icon, top let and select Dashboards \u2192 NGINX \u2192 NGINX Servers . Using the Overrides filter on k8s.cluster.name: , find the name of your cluster as returned by echo $(hostname)-eks-cluster in the terminal. A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. \u21a9","title":"2. Create NGINX deployment"},{"location":"resources/","text":"Additional Splunk for DevOps Resources \u00b6 Below are helpful resources about Splunk and the DevOps use case. Topics covered are Observability Cloud, VictorOps, OpenTelemetry, Observability and Incident Response. Documentation \u00b6 Observability O11y Cloud Docs Observability O11y Cloud Docs On-Call On-Call Platform Docs On-Call On-Call API Docs Blog Posts \u00b6 Observability \"TraceInvaders - an OpenTelemetry Video Game\" Observability \"Observability Microsite\" APM \"How Splunk Does Site Reliability Engineering (SRE)\" OTEL \"OpenTelemetry for Business Continuity\" APM \"In Observability RED is the new Black\" On-Call \"Alerts to Incident Response in Three Easy Steps\" APM \"Application Performance Redefined: Meet the New Splunk's Microservices APM\" Splunk \"Splunk is Lambda Ready: Announcing a New Partnership with AWS\" APM \"Supporting APM for .NET Applications\" OTEL \"OpenTelemetry Consolidates Data for Observability\" Observability \"Using Observability as a Proxy for User Happiness\" Webinars & Podcast \u00b6 APM \"Future of Microservices and APM\" DevOps \"DevOps from the Top Podcast\" Observability \"Oh my! SRE, AIOps and Observe-a-what?\" Observability \"On-Demand Observability Demo\" DevOps \"Dissecting DevOps PlayList\" DevOps \"DevOps Open Source Innovation and Insights On-Demand Virtual Event\" DevOps \"Observability to Enable Your Digital Initiatives On-Demand Virtual Event\"","title":"Links"},{"location":"resources/#additional-splunk-for-devops-resources","text":"Below are helpful resources about Splunk and the DevOps use case. Topics covered are Observability Cloud, VictorOps, OpenTelemetry, Observability and Incident Response.","title":"Additional Splunk for DevOps Resources"},{"location":"resources/#documentation","text":"Observability O11y Cloud Docs Observability O11y Cloud Docs On-Call On-Call Platform Docs On-Call On-Call API Docs","title":"Documentation"},{"location":"resources/#blog-posts","text":"Observability \"TraceInvaders - an OpenTelemetry Video Game\" Observability \"Observability Microsite\" APM \"How Splunk Does Site Reliability Engineering (SRE)\" OTEL \"OpenTelemetry for Business Continuity\" APM \"In Observability RED is the new Black\" On-Call \"Alerts to Incident Response in Three Easy Steps\" APM \"Application Performance Redefined: Meet the New Splunk's Microservices APM\" Splunk \"Splunk is Lambda Ready: Announcing a New Partnership with AWS\" APM \"Supporting APM for .NET Applications\" OTEL \"OpenTelemetry Consolidates Data for Observability\" Observability \"Using Observability as a Proxy for User Happiness\"","title":"Blog Posts"},{"location":"resources/#webinars-podcast","text":"APM \"Future of Microservices and APM\" DevOps \"DevOps from the Top Podcast\" Observability \"Oh my! SRE, AIOps and Observe-a-what?\" Observability \"On-Demand Observability Demo\" DevOps \"Dissecting DevOps PlayList\" DevOps \"DevOps Open Source Innovation and Insights On-Demand Virtual Event\" DevOps \"Observability to Enable Your Digital Initiatives On-Demand Virtual Event\"","title":"Webinars &amp; Podcast"},{"location":"resources/faq/","text":"Frequently Asked Questions \u00b6 A collection of the common questions and their answers associated with Observability, DevOps, Incident Response and Splunk On-Call. Q: Alerts v. Incident Response v. Incident Management \u00b6 A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process. Monitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents. Those incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident. Incidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved. All these components are necessary for successful incident response and management practices. On-Call Q: Is Observability Monitoring \u00b6 A: The key difference between Monitoring and Observability is the difference between \u201cknown knowns\u201d and \u201cunknown knowns\u201d respectively. In monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed. Observability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited. Observability is a set of practices and technology, which include traditional monitoring metrics. These practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static. Observability Q: What are Traces and Spans \u00b6 A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together. Because microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures. APM Q: What is the Sidecar Pattern \u00b6 A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents associated with the management plan along with the application service they support. In Observability the sidecar services are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle. Observability","title":"FAQ"},{"location":"resources/faq/#frequently-asked-questions","text":"A collection of the common questions and their answers associated with Observability, DevOps, Incident Response and Splunk On-Call.","title":"Frequently Asked Questions"},{"location":"resources/faq/#q-alerts-v-incident-response-v-incident-management","text":"A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process. Monitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents. Those incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident. Incidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved. All these components are necessary for successful incident response and management practices. On-Call","title":"Q: Alerts v. Incident Response v. Incident Management"},{"location":"resources/faq/#q-is-observability-monitoring","text":"A: The key difference between Monitoring and Observability is the difference between \u201cknown knowns\u201d and \u201cunknown knowns\u201d respectively. In monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed. Observability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited. Observability is a set of practices and technology, which include traditional monitoring metrics. These practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static. Observability","title":"Q: Is Observability Monitoring"},{"location":"resources/faq/#q-what-are-traces-and-spans","text":"A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together. Because microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures. APM","title":"Q: What are Traces and Spans"},{"location":"resources/faq/#q-what-is-the-sidecar-pattern","text":"A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents associated with the management plan along with the application service they support. In Observability the sidecar services are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle. Observability","title":"Q: What is the Sidecar Pattern"},{"location":"servicebureau/billing-and-usage/","text":"Service Bureau - Lab Summary \u00b6 How to keep track of the usage of Observability Cloud in your organization Learn how to keep track of spend by exploring the Billing and Usage interface Creating Teams Adding notification rules to Teams Controlling usage 1. Understanding engagement \u00b6 To fully understand Observability Cloud engagement inside your organization, click on the hamburger top left and select the Organizations Settings \u2192 Organization Overview , this will provide you with the following dashboard that shows you how your Observability Cloud organization is being used: On the left hand menu you will see a list of members, and on the right, various charts that show you the number of users, teams, charts, dashboards, and dashboard groups created, as well as various growth trends. The workshop organization you're using now may have less data to work with as this is cleared down after each workshop. Take a minute to explore the various charts in the Organization Overview of this workshop instance. 2. Usage and Billing \u00b6 If you want to see what your usage is against your contract you can select the Organizations Settings \u2192 Billing and Usage . Or the faster way is to select the Billing and Usage item from the left hand pane. This screen may take a few seconds to load whilst it calculates and pulls in the usage. 3. Understanding usage \u00b6 You will see a screen similar like the one below that will give you an overview of the current usage, the average usage and your entitlement per category : Hosts, Containers, Custom Metrics and High Resolution Metrics. For more information about about these categories please refer to Billing and Usage information . 4. Examine usage in detail \u00b6 The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below). Also, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart). In this example you can see that there are 25 Hosts, 0 Containers and 100 Custom Metrics and 0 High Resolution Metrics. In the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart). The blue line marked Average Usage indicates what Observability Cloud will use to calculate your average usage for the current billing period. Info As you can see from the screenshot, Observability Cloud does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges. To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the the Billing Period with the drop down on the right. Please take a minute to explore the different time periods & categories and their views. Finally, the pane on the right shows you information about your Subscription.","title":"Billing and Usage"},{"location":"servicebureau/billing-and-usage/#service-bureau-lab-summary","text":"How to keep track of the usage of Observability Cloud in your organization Learn how to keep track of spend by exploring the Billing and Usage interface Creating Teams Adding notification rules to Teams Controlling usage","title":"Service Bureau - Lab Summary"},{"location":"servicebureau/billing-and-usage/#1-understanding-engagement","text":"To fully understand Observability Cloud engagement inside your organization, click on the hamburger top left and select the Organizations Settings \u2192 Organization Overview , this will provide you with the following dashboard that shows you how your Observability Cloud organization is being used: On the left hand menu you will see a list of members, and on the right, various charts that show you the number of users, teams, charts, dashboards, and dashboard groups created, as well as various growth trends. The workshop organization you're using now may have less data to work with as this is cleared down after each workshop. Take a minute to explore the various charts in the Organization Overview of this workshop instance.","title":"1. Understanding engagement"},{"location":"servicebureau/billing-and-usage/#2-usage-and-billing","text":"If you want to see what your usage is against your contract you can select the Organizations Settings \u2192 Billing and Usage . Or the faster way is to select the Billing and Usage item from the left hand pane. This screen may take a few seconds to load whilst it calculates and pulls in the usage.","title":"2. Usage and Billing"},{"location":"servicebureau/billing-and-usage/#3-understanding-usage","text":"You will see a screen similar like the one below that will give you an overview of the current usage, the average usage and your entitlement per category : Hosts, Containers, Custom Metrics and High Resolution Metrics. For more information about about these categories please refer to Billing and Usage information .","title":"3. Understanding usage"},{"location":"servicebureau/billing-and-usage/#4-examine-usage-in-detail","text":"The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below). Also, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart). In this example you can see that there are 25 Hosts, 0 Containers and 100 Custom Metrics and 0 High Resolution Metrics. In the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart). The blue line marked Average Usage indicates what Observability Cloud will use to calculate your average usage for the current billing period. Info As you can see from the screenshot, Observability Cloud does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges. To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the the Billing Period with the drop down on the right. Please take a minute to explore the different time periods & categories and their views. Finally, the pane on the right shows you information about your Subscription.","title":"4. Examine usage in detail"},{"location":"servicebureau/teams/","text":"Teams - Lab Summary \u00b6 Introduction to Teams Create a Team and add members to Team 1. Introduction to Teams \u00b6 To make sure that users see the dashboards and alerts that are relevant to them when using Observability Cloud, most organizations will use Observability Cloud's Teams feature to assign a member to one or more Teams. Ideally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in Observability Cloud. When a user logs into Observability Cloud, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role. In the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team. This Dashboard has specific Dashboard Groups for NGINX, Infra and K8s assigned but any Dashboard Group can be linked to a Teams Dashboard. They can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL Dashboards using the adjacent link. Alerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert. The Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown). 2. Creating a new Team \u00b6 To work with to Splunk's Team UI click on the hamburger icon top left and select the Organizations Settings \u2192 Teams . When the Team UI is selected you will be presented with the list of current Teams. To add a new Team click on the Create New Team button. This will present you with the Create New Team dialog. Create your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below: You can remove selected users by pressing Remove or the small x . Make sure you have your group created with your initials and with yourself added as a member, then click Done This will bring you back to the Teams list that will now show your Team and the one's created by others. Note The Teams(s) you are a member of have a grey Member icon in front of it. If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself. This is the same dialog you get when pressing the 3 dots ... at the end of the line with your Team and selecting Edit Team The ... menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member). 3. Adding Notification Rules \u00b6 You can set up specific Notification rules per team, click on the Notification Policy tab, this will open the notification edit menu. By default the system offers you the ability to set up a general notification rule for your team. Note The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type. 3.1 Adding recipients \u00b6 You can add other recipients, by clicking Add Recipient . These recipients do not need to be Observability Cloud users. However if you click on the link Configure separate notification tiers for different severity alerts you can configure every alert level independently. Different alert rules for the different alert levels can be configured, as shown in the above image. Critical and Major are using Splunk's VictorOps Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email. 3.2 Notification Integrations \u00b6 In addition to sending alert notifications via email, you can configure Observability Cloud to send alert notifications to the services shown below. Take a moment to create some notification rules for you Team.","title":"Teams"},{"location":"servicebureau/teams/#teams-lab-summary","text":"Introduction to Teams Create a Team and add members to Team","title":"Teams - Lab Summary"},{"location":"servicebureau/teams/#1-introduction-to-teams","text":"To make sure that users see the dashboards and alerts that are relevant to them when using Observability Cloud, most organizations will use Observability Cloud's Teams feature to assign a member to one or more Teams. Ideally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in Observability Cloud. When a user logs into Observability Cloud, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role. In the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team. This Dashboard has specific Dashboard Groups for NGINX, Infra and K8s assigned but any Dashboard Group can be linked to a Teams Dashboard. They can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL Dashboards using the adjacent link. Alerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert. The Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown).","title":"1. Introduction to Teams"},{"location":"servicebureau/teams/#2-creating-a-new-team","text":"To work with to Splunk's Team UI click on the hamburger icon top left and select the Organizations Settings \u2192 Teams . When the Team UI is selected you will be presented with the list of current Teams. To add a new Team click on the Create New Team button. This will present you with the Create New Team dialog. Create your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below: You can remove selected users by pressing Remove or the small x . Make sure you have your group created with your initials and with yourself added as a member, then click Done This will bring you back to the Teams list that will now show your Team and the one's created by others. Note The Teams(s) you are a member of have a grey Member icon in front of it. If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself. This is the same dialog you get when pressing the 3 dots ... at the end of the line with your Team and selecting Edit Team The ... menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member).","title":"2. Creating a new Team"},{"location":"servicebureau/teams/#3-adding-notification-rules","text":"You can set up specific Notification rules per team, click on the Notification Policy tab, this will open the notification edit menu. By default the system offers you the ability to set up a general notification rule for your team. Note The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type.","title":"3. Adding Notification Rules"},{"location":"servicebureau/teams/#31-adding-recipients","text":"You can add other recipients, by clicking Add Recipient . These recipients do not need to be Observability Cloud users. However if you click on the link Configure separate notification tiers for different severity alerts you can configure every alert level independently. Different alert rules for the different alert levels can be configured, as shown in the above image. Critical and Major are using Splunk's VictorOps Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email.","title":"3.1 Adding recipients"},{"location":"servicebureau/teams/#32-notification-integrations","text":"In addition to sending alert notifications via email, you can configure Observability Cloud to send alert notifications to the services shown below. Take a moment to create some notification rules for you Team.","title":"3.2 Notification Integrations"},{"location":"servicebureau/tokens/","text":"Controlling Usage - Lab Summary \u00b6 Discover how you can restrict usage by creating separate Access Tokens and set limits. 1. Access Tokens \u00b6 If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization. In the UI click on the hamburger top left and select the Organizations Settings \u2192 Access Tokens The Access Tokens Interface provides an overview of your allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured. Each Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume. The Usage Status Column quickly shows if a token is above or below its assigned limits. 2. Creating a new token \u00b6 Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog. Enter the new name of the new Token by using your Initials e.g. RWC-Token and make sure to tick both Ingest Token and API Token checkboxes! After you press OK , you will be taken back to the Access Token UI, here your new token should be present, among the ones created by others. If you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis ( ... ) menu button behind a token limit to open the manage token menu. If you made a typo you can use the Rename Token option to correct the name of your token. 3. Disabling a token \u00b6 If you need to make sure a token cannot be used to send Metrics in you can disable a token. Click on Disable to disable the token, this means the token cannot be used for sending in data to Splunk Observability Cloud. The line with your token should have become greyed out to indicate that is has been disabled as you can see in the screenshot below. Go ahead and click on the ellipsis ( ... ) menu button to Disable and Enable your token. 4. Manage token usage limits \u00b6 Now Lets start limiting usage by clicking on Manage Token Limit in the 3 ... menu. This will show the Manage Token Limit Dialog: In this Dialog you can set the limits per category. Please go ahead and specify the limits as follows for each usage metric: Limit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above. Token limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes. To specify the recipients, click Add Recipient , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended). The severity for token alerts is always Critical. Click on Update to save your Access Tokens limits and The Alert Settings. Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by Observability Cloud. This will make sure you there will be no unexpected cost due to a team sending in data without restriction. Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved. In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to Observability Cloud. This will allow you to fine tune the way you consume your Observability Cloud allotment and prevent overages from happening. Congratulations! You have now have completed the Service Bureau module.","title":"Control Usage"},{"location":"servicebureau/tokens/#controlling-usage-lab-summary","text":"Discover how you can restrict usage by creating separate Access Tokens and set limits.","title":"Controlling Usage - Lab Summary"},{"location":"servicebureau/tokens/#1-access-tokens","text":"If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization. In the UI click on the hamburger top left and select the Organizations Settings \u2192 Access Tokens The Access Tokens Interface provides an overview of your allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured. Each Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume. The Usage Status Column quickly shows if a token is above or below its assigned limits.","title":"1. Access Tokens"},{"location":"servicebureau/tokens/#2-creating-a-new-token","text":"Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog. Enter the new name of the new Token by using your Initials e.g. RWC-Token and make sure to tick both Ingest Token and API Token checkboxes! After you press OK , you will be taken back to the Access Token UI, here your new token should be present, among the ones created by others. If you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis ( ... ) menu button behind a token limit to open the manage token menu. If you made a typo you can use the Rename Token option to correct the name of your token.","title":"2. Creating a new token"},{"location":"servicebureau/tokens/#3-disabling-a-token","text":"If you need to make sure a token cannot be used to send Metrics in you can disable a token. Click on Disable to disable the token, this means the token cannot be used for sending in data to Splunk Observability Cloud. The line with your token should have become greyed out to indicate that is has been disabled as you can see in the screenshot below. Go ahead and click on the ellipsis ( ... ) menu button to Disable and Enable your token.","title":"3. Disabling a token"},{"location":"servicebureau/tokens/#4-manage-token-usage-limits","text":"Now Lets start limiting usage by clicking on Manage Token Limit in the 3 ... menu. This will show the Manage Token Limit Dialog: In this Dialog you can set the limits per category. Please go ahead and specify the limits as follows for each usage metric: Limit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above. Token limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes. To specify the recipients, click Add Recipient , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended). The severity for token alerts is always Critical. Click on Update to save your Access Tokens limits and The Alert Settings. Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by Observability Cloud. This will make sure you there will be no unexpected cost due to a team sending in data without restriction. Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved. In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to Observability Cloud. This will allow you to fine tune the way you consume your Observability Cloud allotment and prevent overages from happening. Congratulations! You have now have completed the Service Bureau module.","title":"4. Manage token usage limits"},{"location":"splunk/","text":"Step 1: Sign-up for a free trial account \u00b6 Sign up for a free trial account. Step 2: Access your trial account \u00b6 After you are logged into Splunk.com click on Free Splunk Then click Access Free 15-day trial Step 3: Login to your Splunk Cloud Environment \u00b6 Check your email for getting URL and credentials to your Splunk Cloud Environment. Make sure to check spam folder or search for 'Splunk Cloud' if you do not see the email. Login to your Splunk Cloud. You will be asked to change the password the first time you login.","title":"Signing Up"},{"location":"splunk/#step-1-sign-up-for-a-free-trial-account","text":"Sign up for a free trial account.","title":"Step 1: Sign-up for a free trial account"},{"location":"splunk/#step-2-access-your-trial-account","text":"After you are logged into Splunk.com click on Free Splunk Then click Access Free 15-day trial","title":"Step 2: Access your trial account"},{"location":"splunk/#step-3-login-to-your-splunk-cloud-environment","text":"Check your email for getting URL and credentials to your Splunk Cloud Environment. Make sure to check spam folder or search for 'Splunk Cloud' if you do not see the email. Login to your Splunk Cloud. You will be asked to change the password the first time you login.","title":"Step 3: Login to your Splunk Cloud Environment"},{"location":"splunk/datalinks/","text":"Integration with Splunk \u00b6 1. Introduction to Data links \u00b6 Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards. 2. Configuring an integration with Splunk \u00b6 Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Examine in Splunk. For the Link to use the dropdown and select Custom URL . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The Custom URL value will be the your Splunk instance. In this workshop, we will configure two Data Links - 1. for mapping pod specific data, and 2. for workload logs and events While configuring use Trigger > Any Value Of > kubernetes_pod_name as shown in the picture above. For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" object.involvedObject.name={{properties.kubernetes_pod_name}} Similarly to set up workload data link select the properties as shown below For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" \"{{properties.kubernetes_workload_name}}\" 3. Using Data Links \u00b6 Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . For linking pod logs and events For linking workload logs and events","title":"Setting up Data Links for Contextual Logging"},{"location":"splunk/datalinks/#integration-with-splunk","text":"","title":"Integration with Splunk"},{"location":"splunk/datalinks/#1-introduction-to-data-links","text":"Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards.","title":"1. Introduction to Data links"},{"location":"splunk/datalinks/#2-configuring-an-integration-with-splunk","text":"Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Examine in Splunk. For the Link to use the dropdown and select Custom URL . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The Custom URL value will be the your Splunk instance. In this workshop, we will configure two Data Links - 1. for mapping pod specific data, and 2. for workload logs and events While configuring use Trigger > Any Value Of > kubernetes_pod_name as shown in the picture above. For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" object.involvedObject.name={{properties.kubernetes_pod_name}} Similarly to set up workload data link select the properties as shown below For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" \"{{properties.kubernetes_workload_name}}\"","title":"2. Configuring an integration with Splunk"},{"location":"splunk/datalinks/#3-using-data-links","text":"Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . For linking pod logs and events For linking workload logs and events","title":"3. Using Data Links"},{"location":"splunk/hectoken/","text":"Create Splunk Http Event Collector (HEC) \u00b6 The HTTP Event Collector (HEC) is a fast and efficient way to send data over HTTP (or HTTPS) directly to Splunk Cloud from your application, Amazon EKS Kubernetes cluster in our case. Click Settings > Data Inputs in Splunk Cloud Click on \"Add New\" next to HTTP Event Collector Pick appropriate name such as k8s-hec, leave other fields as default Pick the clusters we created in the step \u2013 em_meta and em_events Review and Submit Copy Token Value","title":"Creating HEC Token"},{"location":"splunk/hectoken/#create-splunk-http-event-collector-hec","text":"The HTTP Event Collector (HEC) is a fast and efficient way to send data over HTTP (or HTTPS) directly to Splunk Cloud from your application, Amazon EKS Kubernetes cluster in our case. Click Settings > Data Inputs in Splunk Cloud Click on \"Add New\" next to HTTP Event Collector Pick appropriate name such as k8s-hec, leave other fields as default Pick the clusters we created in the step \u2013 em_meta and em_events Review and Submit Copy Token Value","title":"Create Splunk Http Event Collector (HEC)"},{"location":"splunk/sck/","text":"Setting up Kubernetes Logging by Deploying Splunk Connect for Kubernetes \u00b6 Shell Command cd splunk export HEC_TOKEN=# paste your hec token value export HEC_HOST=# paste Splunk Cloud host e.g. prd-p-ox1qg.splunkcloud.com Deploy SCK as a DaemonSet using Helm in Kubernetes namespace splunk Shell Command kubectl create ns splunk Deploy SCK Helm Chart Shell Command helm install \\ splunk --set global.splunk.hec.token=$HEC_TOKEN \\ --set global.splunk.hec.host=$HEC_HOST \\ --namespace splunk -f values.yaml https://github.com/splunk/splunk-connect-for-kubernetes/releases/download/1.4.1/splunk-connect-for-kubernetes-1.4.1.tgz OUTPUT NAME: splunk LAST DEPLOYED: Sat Jul 25 21:12:34 2020 NAMESPACE: splunk STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557\u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d Listen to your data. Splunk Connect for Kubernetes is spinning up in your cluster. After a few minutes, you should see data being indexed in your Splunk. If you get stuck, we're here to help. Look for answers here: http://docs.splunk.com","title":"Deploying Splunk Connect for Kubernetes"},{"location":"splunk/sck/#setting-up-kubernetes-logging-by-deploying-splunk-connect-for-kubernetes","text":"Shell Command cd splunk export HEC_TOKEN=# paste your hec token value export HEC_HOST=# paste Splunk Cloud host e.g. prd-p-ox1qg.splunkcloud.com Deploy SCK as a DaemonSet using Helm in Kubernetes namespace splunk Shell Command kubectl create ns splunk Deploy SCK Helm Chart Shell Command helm install \\ splunk --set global.splunk.hec.token=$HEC_TOKEN \\ --set global.splunk.hec.host=$HEC_HOST \\ --namespace splunk -f values.yaml https://github.com/splunk/splunk-connect-for-kubernetes/releases/download/1.4.1/splunk-connect-for-kubernetes-1.4.1.tgz OUTPUT NAME: splunk LAST DEPLOYED: Sat Jul 25 21:12:34 2020 NAMESPACE: splunk STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557\u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d Listen to your data. Splunk Connect for Kubernetes is spinning up in your cluster. After a few minutes, you should see data being indexed in your Splunk. If you get stuck, we're here to help. Look for answers here: http://docs.splunk.com","title":"Setting up Kubernetes Logging by Deploying Splunk Connect for Kubernetes"},{"location":"splunk/solution/","text":"Exercises and solutions \u00b6 Your mission, should you choose to accept it is to solve the mystery why ImageSample and Hungry-job are stuck in Pending status. Hint: Use monitoring and troubleshooting workflow from SignalFx to Splunk Cloud. Contextual logs will guide you towards the solutions. Fixes are available in /fixes directory. Hungry Cron Job is requesting too many resources. 5000 CPU millicores (5 whole CPUs) and 32GB of memory! That is why Kubernetes is not scheduling the pod even though it is being requested every minute by the cronjob. Let's fix these resource requests to a more reasonable number Shell Command kubectl patch cronjob hungry-job --patch=\"$(cat apps/fixes/hungry-fix.yaml)\" ImageSample pod is pending because it was trying to use an image which does not exist in the container registry provided in the pod spec. Shell Command kubectl patch deployment imagesample --patch=\"$(cat apps/fixes/imagesample-fix.yaml)\" After a few seconds check Kubernetes Navigator to make sure your patches have been applied successfully.","title":"Solution"},{"location":"splunk/solution/#exercises-and-solutions","text":"Your mission, should you choose to accept it is to solve the mystery why ImageSample and Hungry-job are stuck in Pending status. Hint: Use monitoring and troubleshooting workflow from SignalFx to Splunk Cloud. Contextual logs will guide you towards the solutions. Fixes are available in /fixes directory. Hungry Cron Job is requesting too many resources. 5000 CPU millicores (5 whole CPUs) and 32GB of memory! That is why Kubernetes is not scheduling the pod even though it is being requested every minute by the cronjob. Let's fix these resource requests to a more reasonable number Shell Command kubectl patch cronjob hungry-job --patch=\"$(cat apps/fixes/hungry-fix.yaml)\" ImageSample pod is pending because it was trying to use an image which does not exist in the container registry provided in the pod spec. Shell Command kubectl patch deployment imagesample --patch=\"$(cat apps/fixes/imagesample-fix.yaml)\" After a few seconds check Kubernetes Navigator to make sure your patches have been applied successfully.","title":"Exercises and solutions"},{"location":"splunk/splunkindex/","text":"Creating Splunk Index \u00b6 The index is the repository for Splunk Enterprise data. Splunk Cloud transforms incoming data into events, which it stores in indexes. From the home page, Click on Settings and from expanded menu click Indexes Click New Index with Index Name em_meta . Leave Index Data Type selected as Events. Put 0 in Max raw data size and Searchable time (days) 15. Click Save to create the index. Following similar steps create another Index with name em_events","title":"Creating Index"},{"location":"splunk/splunkindex/#creating-splunk-index","text":"The index is the repository for Splunk Enterprise data. Splunk Cloud transforms incoming data into events, which it stores in indexes. From the home page, Click on Settings and from expanded menu click Indexes Click New Index with Index Name em_meta . Leave Index Data Type selected as Events. Put 0 in Max raw data size and Searchable time (days) 15. Click Save to create the index. Following similar steps create another Index with name em_events","title":"Creating Splunk Index"}]}